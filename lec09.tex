\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{needspace}
\usepackage{tabls}
\usepackage{wasysym}
\usepackage{easytable}
\usepackage{pythonhighlight}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Friday, October 08, 2021 11:50:55}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%TCIDATA{ComputeDefs=
%$A=\left(
%\begin{array}
%[c]{ccc}%
%0 & 1 & 1\\
%0 & 0 & 0\\
%0 & 0 & 0
%\end{array}
%\right)  $
%$S=\left(
%\begin{array}
%[c]{ccc}%
%0 & 1 & 0\\
%1 & 0 & 0\\
%-1 & 0 & 1
%\end{array}
%\right)  $
%}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\usetikzlibrary{arrows.meta}
\usetikzlibrary{chains}
\newcounter{exer}
\newcounter{exera}
\numberwithin{exer}{subsection}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[subsection]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{convention}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{quest}[theo]{Question}
\newenvironment{question}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{warning}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\newtheorem{exmp}[exer]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\newenvironment{statement}{\begin{quote}}{\end{quote}}
\newenvironment{fineprint}{\medskip \begin{small}}{\end{small} \medskip}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{question}[1][Question]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{warning}[1][Warning]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{teachingnote}[1][Teaching note]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\setlength\tablinesep{3pt}
\setlength\arraylinesep{3pt}
\setlength\extrarulesep{3pt}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newcommand\arxiv[1]{\href{http://www.arxiv.org/abs/#1}{\texttt{arXiv:#1}}}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\newenvironment{teachingnote}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\excludecomment{teachingnote}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\rev}{\operatorname{rev}}
\newcommand{\powset}[2][]{\ifthenelse{\equal{#2}{}}{\mathcal{P}\left(#1\right)}{\mathcal{P}_{#1}\left(#2\right)}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\tup}[1]{\left( #1 \right)}
\newcommand{\ive}[1]{\left[ #1 \right]}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\lf}[2]{#1^{\underline{#2}}}
\newcommand{\underbrack}[2]{\underbrace{#1}_{\substack{#2}}}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\are}{\ar@{-}}
\newcommand{\nnn}{\nonumber\\}
\newcommand{\sslash}{\mathbin{/\mkern-6mu/}}
\newcommand{\numboxed}[2]{\underbrace{\boxed{#1}}_{\text{box } #2}}
\newcommand{\ig}[2]{\includegraphics[scale=#1]{#2.png}}
\newcommand{\UNFINISHED}{\begin{center} \Huge{\textbf{Unfinished material begins here.}} \end{center} }
\iffalse
\NOEXPAND{\today}{\today}
\NOEXPAND{\sslash}{\sslash}
\NOEXPAND{\numboxed}[2]{\numboxed}
\NOEXPAND{\UNFINISHED}{\UNFINISHED}
\fi
\ihead{Math 504 notes}
\ohead{page \thepage}
\cfoot{\today}
\begin{document}

\title{Math 504: Advanced Linear Algebra}
\author{Hugo Woerdeman, with edits by Darij Grinberg\thanks{Drexel University, Korman
Center, 15 S 33rd Street, Philadelphia PA, 19104, USA}}
\date{\today\ (unfinished!)}
\maketitle
\tableofcontents

\section*{Math 504 Lecture 9}

\section{Jordan canonical (aka normal) form (cont'd)}

\subsection{Step 3: Strictly upper-triangular matrices}

Last time, we have proved the uniqueness of the JCF (Jordan canonical form) of
a matrix $A\in\mathbb{C}^{n\times n}$.

We also started proving its existence. The first 2 steps we made were:

\begin{itemize}
\item We brought $A$ to an upper-triangular form with diagonal entries in
contiguous blocks. (This was just careful Schur triangularization.)

\item We cleaned out the space \textquotedblleft between\textquotedblright%
\ distinct diagonal entries: $\left(
\begin{array}
[c]{ccc}%
1 & \ast & \ast\\
& 1 & \ast\\
&  & 2
\end{array}
\right)  \rightarrow\left(
\begin{array}
[c]{ccc}%
1 & \ast & \\
& 1 & \\
&  & 2
\end{array}
\right)  $.
\end{itemize}

Thus, after these steps, our matrix has become a block-diagonal matrix
$\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right)  $, where each $A_{i}$ is an upper-triangular matrix with all its
diagonal entries equal.

Now it remains to decompose each $A_{i}$ into Jordan cells. To be precise, we
want to show that each $A_{i}$ is similar to a Jordan matrix. (Indeed, this
will easily cause the total matrix to be similar to a Jordan matrix.)

For each $i\in\left[  k\right]  $, the matrix $A_{i}$ has all its diagonal
entries equal. Let's say they all equal $\mu_{i}$. Thus, $A_{i}-\mu_{i}I$ is a
strictly upper-triangular matrix. (Recall: a strictly
\textbf{upper-triangular} matrix is an upper-triangular matrix whose diagonal
entries are $0$.)

So we need to find a way to decompose a strictly upper-triangular matrix into
Jordan cells.

Call the upper-triangular matrix $A$ instead of $A_{i}-\mu_{i}I$. Forget about
the big matrix.

\bigskip

So we have a strictly upper-triangular matrix $A\in\mathbb{C}^{n\times n}$. We
want to prove that $A$ is similar to a Jordan matrix.

We begin by trying some examples:\newline

\begin{example}
Let $n=2$. Then, $A=\left(
\begin{array}
[c]{cc}%
0 & a\\
0 & 0
\end{array}
\right)  $ for some $a\in\mathbb{C}$.

We are looking for an invertible matrix $S\in\mathbb{C}^{2\times2}$ such that
$S^{-1}AS$ is a Jordan matrix.

If $a=0$, then this is obvious (just take $S=I_{2}$), since $A=\left(
\begin{array}
[c]{cc}%
J_{1}\left(  0\right)   & \\
& J_{1}\left(  0\right)
\end{array}
\right)  $ is already a Jordan matrix.

Now assume $a\neq0$.

Consider our unknown invertible matrix $S$. Let $s_{1}$ and $s_{2}$ be its
columns. Then, $s_{1}$ and $s_{2}$ are linearly independent (since $S$ is
invertible). Moreover, we want $S^{-1}AS=\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $. In other words, we want $AS=S\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $. However, $S=\left(
\begin{array}
[c]{cc}%
s_{1} & s_{2}%
\end{array}
\right)  $ (in block-matrix notation), so $S\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
0 & s_{1}%
\end{array}
\right)  $. Thus our equation $AS=S\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $ is equivalent to%
\[
\left(
\begin{array}
[c]{cc}%
As_{1} & As_{2}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
0 & s_{1}%
\end{array}
\right)  .
\]
In other words, $As_{1}=0$ and $As_{2}=s_{1}$.

So we are looking for two linearly independent vectors $s_{1},s_{2}%
\in\mathbb{C}^{2}$ such that $As_{1}=0$ and $As_{2}=s_{1}$.

One way to do so is to pick some nonzero vector $s_{1}\in\operatorname*{Ker}%
A$, and then define $s_{2}$ to be some preimage of $s_{1}$ under $A$. (It can
be shown that such preimage exists.) This way, however, does not generalize to
higher $n$.

Another (better) way is to start by picking $s_{2}\in\mathbb{C}^{2}%
\setminus\operatorname*{Ker}A$ and then setting $s_{1}=As_{2}$. We claim that
$s_{1}$ and $s_{2}$ are linearly independent, and that $As_{1}=0$.

To show that $As_{1}=0$, we just observe that $As_{1}=\underbrace{AA}%
_{=A^{2}=0}s_{2}=0$.

To show that $s_{1}$ and $s_{2}$ are linearly independent, we argue as
follows: Let $\lambda_{1},\lambda_{2}\in\mathbb{C}$ be such that $\lambda
_{1}s_{1}+\lambda_{2}s_{2}=0$. Applying $A$ to this, we obtain $A\cdot\left(
\lambda_{1}s_{1}+\lambda_{2}s_{2}\right)  =A\cdot0=0$. However,%
\[
A\cdot\left(  \lambda_{1}s_{1}+\lambda_{2}s_{2}\right)  =\lambda
_{1}\underbrace{As_{1}}_{=0}+\lambda_{2}\underbrace{As_{2}}_{=s_{1}}%
=\lambda_{2}s_{1},
\]
so this becomes $\lambda_{2}s_{1}=0$. However, $s_{1}\neq0$ (because
$s_{1}=As_{2}$ but $s_{2}\notin\operatorname*{Ker}A$). Hence, $\lambda_{2}=0$.
Now, $\lambda_{1}s_{1}+\lambda_{2}s_{2}=0$ becomes $\lambda_{1}s_{1}=0$. Since
$s_{1}\neq0$, this yields $\lambda_{1}=0$. Now both $\lambda_{i}$s are $0$, qed.
\end{example}

\begin{example}
Let $n=3$ and $A=\left(
\begin{array}
[c]{ccc}
& 1 & 1\\
&  & 0\\
&  &
\end{array}
\right)  $.

Our first method above doesn't work, because most vectors in
$\operatorname*{Ker}A$ do not have preimages under $A$.

However, our second method can be made to work:

We pick a vector $s_{3}\notin\operatorname*{Ker}A$. To wit, we pick
$s_{3}=e_{3}=\left(
\begin{array}
[c]{c}%
0\\
0\\
1
\end{array}
\right)  $. Then, $As_{3}=e_{1}$. Set $s_{2}=As_{3}=e_{1}$. Note that
$s_{2}\in\operatorname*{Ker}A$. Let $s_{1}$ be another nonzero vector in
$\operatorname*{Ker}A$, namely $e_{2}-e_{3}$.{} These three vectors
$s_{1},s_{2},s_{3}$ are linearly independent and satisfy $As_{1}=0$ and
$As_{2}=0$ and $As_{3}=s_{2}$.

So $S=\left(
\begin{array}
[c]{ccc}%
0 & 1 & 0\\
1 & 0 & 0\\
-1 & 0 & 1
\end{array}
\right)  $. And indeed, $S^{-1}AS=\allowbreak\left(
\begin{array}
[c]{ccc}%
0 & 0 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{array}
\right)  $ is a Jordan matrix.
\end{example}

So what is the general algorithm here? Can we always find $n$ linearly
independent vectors $s_{1},s_{2},\ldots,s_{n}$ such that each $As_{i}$ is
either $0$ or $s_{i-1}$ ?

\bigskip

To approach this question, we recall a theorem from basic linear algebra:

\begin{theorem}
Let $V$ be a finite-dimensional vector space. Let $\left(  v_{1},v_{2}%
,\ldots,v_{k}\right)  $ be any linearly independent tuple of vectors in $V$.
Then, this tuple can be extended to a basis of $V$. In other words, we can
define further vectors $v_{k+1},v_{k+2},\ldots,v_{m}$ (where $m=\dim V$) such
that $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ is a basis of $V$.
\end{theorem}

Now, we are considering a strictly upper-triangular matrix $A\in
\mathbb{C}^{n\times n}$.

The \textquotedblleft entries ballooning upwards\textquotedblright\ argument
shows that $A^{n}=0$. Thus,%
\[
0=\operatorname*{Ker}\left(  A^{0}\right)  \subseteq\operatorname*{Ker}\left(
A^{1}\right)  \subseteq\operatorname*{Ker}\left(  A^{2}\right)  \subseteq
\cdots\subseteq\operatorname*{Ker}\left(  A^{n-1}\right)  \subseteq
\operatorname*{Ker}\left(  A^{n}\right)  =\mathbb{C}^{n}.
\]
This is a chain of subspaces of $\mathbb{C}^{n}$ (although some of the
$\subseteq$ inclusions can be equalities).

Now, we begin by picking a basis $\left(  v_{1},v_{2},v_{3},\ldots\right)  $
of $\operatorname*{Ker}\left(  A^{n-1}\right)  $ (this list is actually
finite; we just don't want to give a name to its last entry), and extending it
to a basis $\left(  v_{1},v_{2},v_{3},\ldots,s_{n,1},s_{n,2},s_{n,3}%
,\ldots\right)  $ of $\operatorname*{Ker}\left(  A^{n}\right)  $ (by the
theorem above, since it is a linearly independent list of vectors in
$\operatorname*{Ker}\left(  A^{n}\right)  $). Now we throw away the
$v_{1},v_{2},v_{3},\ldots$ and only keep the $s_{n,1},s_{n,2},s_{n,3},\ldots$.

Then, $As_{n,1},As_{n,2},As_{n,3},\ldots$ belong to $\operatorname*{Ker}%
\left(  A^{n-1}\right)  $ (indeed, more generally, if $w\in\operatorname*{Ker}%
\left(  A^{k}\right)  $, then $Aw\in\operatorname*{Ker}\left(  A^{k-1}\right)
$), and are linearly independent (to be proved later). Extend the list
$\left(  As_{n,1},As_{n,2},As_{n,3},\ldots\right)  $ to 

==%
%TCIMACRO{\TEXTsymbol{>} }%
%BeginExpansion
$>$
%EndExpansion
TO\ BE\ CONTINUED\ NEXT\ WEDNESDAY

\bigskip

Now to something completely different...

\subsection{The Cauchy--Schwarz inequality}

Recall the inner product $\left\langle u,v\right\rangle =v^{\ast}%
u=u_{1}\overline{v_{1}}+u_{2}\overline{v_{2}}+\cdots+u_{n}\overline{v_{n}}$ of
two vectors $u,v\in\mathbb{C}^{n}$.

\begin{theorem}
[Cauchy--Schwarz inequality]Let $x\in\mathbb{C}^{n}$ and $y\in\mathbb{C}^{n}$
be two vectors. Then:

\textbf{(a)} The inequality%
\[
\left\vert \left\vert x\right\vert \right\vert \cdot\left\vert \left\vert
y\right\vert \right\vert \geq\left\vert \left\langle x,y\right\rangle
\right\vert \ \ \ \ \ \ \ \ \ \ \text{holds.}%
\]


\textbf{(b)} This inequality becomes an equality if and only if the pair
$\left(  x,y\right)  $ is linearly dependent.
\end{theorem}

\begin{proof}
If $x=0$, then this is obvious. So WLOG assume that $x\neq0$. Thus,
$\left\langle x,x\right\rangle >0$.

Let
\[
a=\left\langle x,x\right\rangle =\left\vert \left\vert x\right\vert
\right\vert ^{2}\in\mathbb{R}\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ b=\left\langle y,x\right\rangle \in\mathbb{C}.
\]


Now, recall that $\left\langle u,u\right\rangle \geq0$ for any $u\in
\mathbb{C}^{n}$. Thus,%
\[
\left\langle bx-ay,\ bx-ay\right\rangle \geq0.
\]
Since%
\begin{align*}
\left\langle bx-ay,\ bx-ay\right\rangle  & =\left\langle
bx-ay,\ bx\right\rangle -\left\langle bx-ay,\ ay\right\rangle \\
& =\left\langle bx,\ bx\right\rangle -\left\langle ay,\ bx\right\rangle
-\left\langle bx,\ ay\right\rangle +\left\langle ay,\ ay\right\rangle \\
& =b\overline{b}\left\langle x,x\right\rangle -a\overline{b}\left\langle
y,x\right\rangle -b\overline{a}\left\langle x,y\right\rangle +a\overline
{a}\left\langle y,y\right\rangle \\
& =b\overline{b}\underbrace{\left\langle x,x\right\rangle }_{=a}-a\overline
{b}\underbrace{\left\langle y,x\right\rangle }_{=b}-ba\underbrace{\left\langle
x,y\right\rangle }_{\substack{=\overline{b}\\\text{(since }\left\langle
x,y\right\rangle =\overline{\left\langle y,x\right\rangle }\text{)}%
}}+aa\left\langle y,y\right\rangle \\
& \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a\in\mathbb{R}\text{ and thus
}\overline{a}=a\right)  \\
& =b\overline{b}a-a\overline{b}b-ba\overline{b}+aa\left\langle
y,y\right\rangle =a\left(  a\left\langle y,y\right\rangle -b\overline
{b}\right)  ,
\end{align*}
this rewrites as
\[
a\left(  a\left\langle y,y\right\rangle -b\overline{b}\right)  \geq0.
\]
We can divide by $a$ (since $a=\left\vert \left\vert x\right\vert \right\vert
^{2}>0$), and obtain $a\left\langle y,y\right\rangle -b\overline{b}\geq0$. In
other words,%
\[
a\left\langle y,y\right\rangle \geq b\overline{b}=\left\vert b\right\vert
^{2}=\left\vert \left\langle x,y\right\rangle \right\vert ^{2}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }b=\left\langle y,x\right\rangle
=\overline{\left\langle x,y\right\rangle }\right)  .
\]
Since $a=\left\langle x,x\right\rangle =\left\vert \left\vert x\right\vert
\right\vert ^{2}$ and $\left\langle y,y\right\rangle =\left\vert \left\vert
y\right\vert \right\vert ^{2}$, this rewrites as
\[
\left\vert \left\vert x\right\vert \right\vert ^{2}\cdot\left\vert \left\vert
y\right\vert \right\vert ^{2}\geq\left\vert \left\langle x,y\right\rangle
\right\vert ^{2}.
\]
Now take square roots and be done with \textbf{(a)}.

\textbf{(b)} Take a look at the above proof of \textbf{(a)} and think about
it. See notes for details (\S 1.1).
\end{proof}

\begin{corollary}
[triangle inequality]For any $x,y\in\mathbb{C}^{n}$, we have $\left\vert
\left\vert x\right\vert \right\vert +\left\vert \left\vert y\right\vert
\right\vert \geq\left\vert \left\vert x+y\right\vert \right\vert $.
\end{corollary}

\begin{proof}
Exercise 1.1.2.
\end{proof}


\end{document}