\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{needspace}
\usepackage{tabls}
\usepackage{wasysym}
\usepackage{easytable}
\usepackage{pythonhighlight}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Monday, October 04, 2021 11:50:57}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\usetikzlibrary{arrows.meta}
\usetikzlibrary{chains}
\newcounter{exer}
\newcounter{exera}
\numberwithin{exer}{subsection}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[subsection]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{convention}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{quest}[theo]{Question}
\newenvironment{question}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{warning}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\newtheorem{exmp}[exer]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\newenvironment{statement}{\begin{quote}}{\end{quote}}
\newenvironment{fineprint}{\medskip \begin{small}}{\end{small} \medskip}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{question}[1][Question]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{warning}[1][Warning]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{teachingnote}[1][Teaching note]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\setlength\tablinesep{3pt}
\setlength\arraylinesep{3pt}
\setlength\extrarulesep{3pt}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newcommand\arxiv[1]{\href{http://www.arxiv.org/abs/#1}{\texttt{arXiv:#1}}}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\newenvironment{teachingnote}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\excludecomment{teachingnote}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\rev}{\operatorname{rev}}
\newcommand{\powset}[2][]{\ifthenelse{\equal{#2}{}}{\mathcal{P}\left(#1\right)}{\mathcal{P}_{#1}\left(#2\right)}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\tup}[1]{\left( #1 \right)}
\newcommand{\ive}[1]{\left[ #1 \right]}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\lf}[2]{#1^{\underline{#2}}}
\newcommand{\underbrack}[2]{\underbrace{#1}_{\substack{#2}}}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\are}{\ar@{-}}
\newcommand{\nnn}{\nonumber\\}
\newcommand{\sslash}{\mathbin{/\mkern-6mu/}}
\newcommand{\numboxed}[2]{\underbrace{\boxed{#1}}_{\text{box } #2}}
\newcommand{\ig}[2]{\includegraphics[scale=#1]{#2.png}}
\newcommand{\UNFINISHED}{\begin{center} \Huge{\textbf{Unfinished material begins here.}} \end{center} }
\iffalse
\NOEXPAND{\today}{\today}
\NOEXPAND{\sslash}{\sslash}
\NOEXPAND{\numboxed}[2]{\numboxed}
\NOEXPAND{\UNFINISHED}{\UNFINISHED}
\fi
\ihead{Math 504 notes}
\ohead{page \thepage}
\cfoot{\today}
\begin{document}

\title{Math 504: Advanced Linear Algebra}
\author{Hugo Woerdeman, with edits by Darij Grinberg\thanks{Drexel University, Korman
Center, 15 S 33rd Street, Philadelphia PA, 19104, USA}}
\date{\today\ (unfinished!)}
\maketitle
\tableofcontents

\section*{Math 504 Lecture 7}

\section{Schur triangularization (cont'd)}

\subsection{Sylvester's equation (cont'd)}

Last time, we proved the $\mathcal{V}\Longrightarrow\mathcal{U}$ direction of
the following fact:

\begin{theorem}
Let $A$ be an $n\times n$-matrix, and let $B$ be an $m\times m$-matrix (both
with complex entries). Let $C$ be an $n\times m$-matrix. Then, the following
statements are equivalent:

\begin{itemize}
\item $\mathcal{U}$: There is a \textbf{unique} matrix $X\in\mathbb{C}%
^{n\times m}$ such that $AX-XB=C$.

\item $\mathcal{V}$: We have $\sigma\left(  A\right)  \cap\sigma\left(
B\right)  =\varnothing$.
\end{itemize}
\end{theorem}

Here, $\sigma\left(  A\right)  $ denotes the \textbf{spectrum} of a matrix $A$
(that is, the set of all eigenvalues of $A$).

Today, we shall derive a corollary from the above theorem:

\begin{corollary}
Let $A\in\mathbb{C}^{n\times n}$, $B\in\mathbb{C}^{m\times m}$ and
$C\in\mathbb{C}^{n\times m}$ be three matrices such that $\sigma\left(
A\right)  \cap\sigma\left(  B\right)  =\varnothing$. Then, the two $\left(
n+m\right)  \times\left(  n+m\right)  $-matrices%
\[
\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)
\]
(written in block-matrix notation) are similar.
\end{corollary}

\begin{example}
Let $A=\left(
\begin{array}
[c]{cc}%
1 & 3\\
0 & 1
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{c}%
2
\end{array}
\right)  $ and $C=\left(
\begin{array}
[c]{c}%
7\\
9
\end{array}
\right)  $. Then, the corollary says that the matrices%
\[
\left(
\begin{array}
[c]{ccc}%
1 & 3 & 7\\
0 & 1 & 9\\
0 & 0 & 2
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{ccc}%
1 & 3 & 0\\
0 & 1 & 0\\
0 & 0 & 2
\end{array}
\right)
\]
are similar.
\end{example}

\begin{proof}
[Proof of Corollary.] We know from the previous theorem (specifically, from
its $\mathcal{V}\Longrightarrow\mathcal{U}$ direction) that there exists a
matrix $X\in\mathbb{C}^{n\times m}$ such that $AX-XB=C$. Consider this $X$.

Now, let $S=\left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  $. Now I claim that $S$ is invertible and that%
\[
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  =S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  S^{-1}.
\]
Once this claim is proved, the corollary will follow.

To see that $S$ is invertible, we construct an inverse. Namely, we claim that
$\left(
\begin{array}
[c]{cc}%
I_{n} & -X\\
0 & I_{m}%
\end{array}
\right)  $ is an inverse for $S$. To show this, we just check that%
\begin{align*}
S\left(
\begin{array}
[c]{cc}%
I_{n} & -X\\
0 & I_{m}%
\end{array}
\right)    & =\left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
I_{n} & -X\\
0 & I_{m}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
I_{n}I_{n}+X\cdot0 & I_{n}\left(  -X\right)  +XI_{m}\\
0I_{n}+I_{m}0 & 0\left(  -X\right)  +I_{m}I_{m}%
\end{array}
\right)  \\
& =\left(
\begin{array}
[c]{cc}%
I_{n} & 0\\
0 & I_{m}%
\end{array}
\right)  =I_{n+m}%
\end{align*}
and%
\[
\left(
\begin{array}
[c]{cc}%
I_{n} & -X\\
0 & I_{m}%
\end{array}
\right)  S=I_{n+m}\ \ \ \ \ \ \ \ \ \ \left(  \text{similarly}\right)  .
\]


So we have shown that $S$ is invertible. It remains to check that%
\[
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  =S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  S^{-1}.
\]
To do so, we check the equivalent identity%
\[
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  S=S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  .
\]
This we do by computing both sides and comparing:%
\[
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  S=\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A & AX\\
0 & B
\end{array}
\right)
\]
and%
\begin{align*}
S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)    & =\left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A & C+XB\\
0 & B
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A & AX\\
0 & B
\end{array}
\right)  \\
& \ \ \ \ \ \ \ \ \ \ \left(  \text{since }AX-XB=C\text{ entails
}C+XB=AX\right)  .
\end{align*}
We are done.
\end{proof}

\section{Jordan canonical (aka normal) form}

\subsection{Similarity redux}

\begin{definition}
Let $A$ and $B$ be two $n\times n$-matrices. We write $A\sim B$ to say
\textquotedblleft$A$ is similar to $B$\textquotedblright.
\end{definition}

We recall some properties of similar matrices: Similarity is an equivalence
relation. Furthermore:

\begin{proposition}
Let $A$ and $B$ be two $n\times n$-matrices that satisfy $A\sim B$. Then:

\textbf{(a)} $A^{k}\sim B^{k}$ for each $k\in\mathbb{N}$.

\textbf{(b)} $A-\lambda I_{n}\sim B-\lambda I_{n}$ for each $\lambda
\in\mathbb{C}$.

\textbf{(c)} $\left(  A-\lambda I_{n}\right)  ^{k}\sim\left(  B-\lambda
I_{n}\right)  ^{k}$ for each $\lambda\in\mathbb{C}$ and $k\in\mathbb{N}$.

\textbf{(d)} $p_{A}=p_{B}$. (Recall that $p_{M}$ stands for the characteristic
polynomial of $M$.)

\textbf{(e)} $\dim\operatorname*{Ker}A=\dim\operatorname*{Ker}B$.

\textbf{(f)} $\dim\operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{k}\right)  =\dim\operatorname*{Ker}\left(  \left(  B-\lambda I_{n}\right)
^{k}\right)  $ for each $\lambda\in\mathbb{C}$ and $k\in\mathbb{N}$.
\end{proposition}

Recall the concepts of algebraic and geometric multiplicities:

\begin{definition}
Let $A$ be an $n\times n$-matrix, and let $\lambda\in\mathbb{C}$.

\textbf{(a)} The \textbf{algebraic multiplicity} of $\lambda$ as an eigenvalue
of $A$ is defined to be the multiplicity of $\lambda$ as a root of $p_{A}$.
(If $\lambda$ is not an eigenvalue of $A$, then this is $0$.)

\textbf{(b)} The \textbf{geometric multiplicity} of $\lambda$ as an eigenvalue
of $A$ is defined to be $\dim\operatorname*{Ker}\left(  A-\lambda
I_{n}\right)  $. In other words, it is the maximum number of linearly
independent eigenvectors for eigenvalue $\lambda$. (If $\lambda$ is not an
eigenvalue of $A$, then this is $0$.)
\end{definition}

The geometric multiplicity is always $\leq$ to the algebraic multiplicity.
Sometimes, it is strictly smaller. For instance, the matrix%
\[
\left(
\begin{array}
[c]{ccc}%
1 & 0 & 0\\
0 & 1 & 2\\
0 & 0 & 1
\end{array}
\right)
\]
has only $1$ as its eigenvalue, with algebraic multiplicity $3$ and geometric
multiplicity is $2$.

\subsection{Jordan cells}

\begin{definition}
A \textbf{Jordan cell} is a $k\times k$-matrix of the form%
\[
\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 & 0 & \cdots & 0\\
0 & \lambda & 1 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{for some }k>0\text{ and some }\lambda
\in\mathbb{C}.
\]
So its diagonal entries are $\lambda$; its entries directly above the diagonal
are $1$; all its other entries are $0$. In formal terms, it is the $k\times
k$-matrix $A$ whose entries are given by the rule%
\[
A_{i,j}=%
\begin{cases}
\lambda, & \text{if }i=j;\\
1, & \text{if }i=j-1;\\
0, & \text{otherwise.}%
\end{cases}
\]


Specifically, this matrix is called the \textbf{Jordan cell of size }$k$
\textbf{at eigenvalue }$\lambda$. It is denoted by $J_{k}\left(
\lambda\right)  $.
\end{definition}

For example, the Jordan cell of size $3$ at eigenvalue $-5$ is%
\[
\left(
\begin{array}
[c]{ccc}%
-5 & 1 & 0\\
0 & -5 & 1\\
0 & 0 & -5
\end{array}
\right)  .
\]


\begin{proposition}
Let $k>0$ and $\lambda\in\mathbb{C}$. The only eigenvalue of $J_{k}\left(
\lambda\right)  $ is $\lambda$, and it has algebraic multiplicity $k$ and
geometric multiplicity $1$.
\end{proposition}

\begin{proof}
Look at the matrix:%
\[
J_{k}\left(  \lambda\right)  =\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 & 0 & \cdots & 0\\
0 & \lambda & 1 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  .
\]
This matrix is upper-triangular, so its characteristic polynomial is%
\[
p_{J_{k}\left(  \lambda\right)  }=\left(  t-\lambda\right)  \left(
t-\lambda\right)  \cdots\left(  t-\lambda\right)  =\left(  t-\lambda\right)
^{k}.
\]
Thus, its only eigenvalue is $\lambda$, and this eigenvalue has algebraic
multiplicity $k$. Its geometric multiplicity is%
\[
\dim\operatorname*{Ker}\left(  J_{k}\left(  \lambda\right)  -\lambda
I_{k}\right)  =\dim\operatorname*{Ker}\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  =1.
\]

\end{proof}

\begin{proposition}
Let $k>0$ and $\lambda\in\mathbb{C}$. Let $B=J_{k}\left(  \lambda\right)
-\lambda I_{k}$. For any $p\in\mathbb{N}$, we have%
\[
\dim\operatorname*{Ker}\left(  B^{p}\right)  =%
\begin{cases}
p, & \text{if }p\leq k;\\
k, & \text{if }p>k.
\end{cases}
.
\]

\end{proposition}

\begin{proof}
As we have just seen,%
\[
B=\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  .
\]


For example, if $k=4$, then%
\begin{align*}
& \ B=\left(
\begin{array}
[c]{cccc}%
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{array}
\right)  \\
& \Longrightarrow\ B^{2}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  \\
& \Longrightarrow\ B^{3}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  \\
& \Longrightarrow\ B^{4}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  \\
& \Longrightarrow\ B^{p}=0\text{ for all }p\geq4.
\end{align*}
This generalizes to arbitrary $k$. Indeed, it is easy to see (by induction on
$p$) that
\[
\left(  B^{p}\right)  _{i,j}=%
\begin{cases}
1, & \text{if }i=j-p;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j.
\]
Now, the claim about $\dim\operatorname*{Ker}\left(  B^{p}\right)  $ easily follows.
\end{proof}

\subsection{The Jordan canonical form (aka Jordan normal form)}

\begin{definition}
A \textbf{Jordan matrix} is a block-diagonal matrix whose diagonal blocks are
Jordan cells. In other words, it is a matrix of the form%
\[
\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)   & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)   & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  ,
\]
where $n_{1},n_{2},\ldots,n_{k}$ are positive integers and $\lambda
_{1},\lambda_{2},\ldots,\lambda_{k}$ are scalars in $\mathbb{C}$ (not
necessarily distinct, but not necessarily equal either).
\end{definition}

We claim the following:

\begin{theorem}
[existence of the Jordan canonical form]Every $n\times n$-matrix over
$\mathbb{C}$ is similar to a Jordan matrix.
\end{theorem}

This theorem is useful partly (but not only) because it allows to reduce
questions about general square matrices to questions about Jordan matrices.
And the latter can usually further be reduced to questions about Jordan cells,
because a block-diagonal matrix \textquotedblleft behaves like its diagonal
blocks are separate\textquotedblright.

The above existence theorem has a matching uniqueness statement:

\begin{theorem}
[Jordan canonical form theorem]Let $A$ be an $n\times n$-matrix over
$\mathbb{C}$. Then, there exists a Jordan matrix $J$ such that $A\sim J$.
Furthermore, this $J$ is unique up to the order of the diagonal blocks.
\end{theorem}

\begin{definition}
The matrix $J$ in this theorem is called a \textbf{Jordan canonical form} of
$A$ (or a \textbf{Jordan normal form} of $A$).
\end{definition}

\begin{example}
The Jordan canonical form of $\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
0 & 2 & 5\\
0 & 0 & 1
\end{array}
\right)  $ is $\left(
\begin{array}
[c]{ccc}%
2 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  $. Indeed, $\left(
\begin{array}
[c]{ccc}%
2 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  $ is a Jordan matrix (it can be written as $\left(
\begin{array}
[c]{cc}%
J_{1}\left(  2\right)   & 0\\
0 & J_{2}\left(  1\right)
\end{array}
\right)  $) and it can be checked that%
\[
\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
0 & 2 & 5\\
0 & 0 & 1
\end{array}
\right)  \sim\left(
\begin{array}
[c]{ccc}%
2 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  .
\]
We can, of course, swap the two Jordan cells in this Jordan matrix, and obtain
another Jordan canonical form of the same matrix.
\end{example}

\begin{example}
The Jordan canonical form of $I_{n}$ is $I_{n}$ itself. Indeed, each of its
diagonal entries is itself a little Jordan cell $J_{1}\left(  1\right)  $, so
it is already a Jordan matrix.
\end{example}

Now, we shall approach the proof of the Jordan canonical form theorem step by
step, beginning with the uniqueness part.

\begin{example}
Suppose that $A\sim J$ with%
\[
J=\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  1\right)   & 0 & 0\\
0 & J_{3}\left(  1\right)   & 0\\
0 & 0 & J_{2}\left(  2\right)
\end{array}
\right)  =\left(
\begin{array}
[c]{ccccccc}%
1 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 2 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 2
\end{array}
\right)  .
\]
What can we say about $A$ ?

First, since $A\sim J$, we have $\sigma\left(  A\right)  =\sigma\left(
J\right)  =\left\{  1,2\right\}  $ (since $J$ is upper-triangular). Actually,
from $A\sim J$, we also obtain%
\[
p_{A}=p_{J}=\left(  t-1\right)  ^{5}\left(  t-2\right)  ^{2}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }J\text{ and therefore also }tI_{n}-J\\
\text{is upper-triangular}%
\end{array}
\right)  .
\]
Thus, the algebraic multiplicities of the eigenvalues $1$ and $2$ of $A$ are
$5$ and $2$, respectively.

Now, what about the geometric multiplicities? What is $\dim\operatorname*{Ker}%
\left(  A-1I_{n}\right)  $ ?

In general, if $B_{1},B_{2},\ldots,B_{k}$ are square matrices, then
\[
\dim\operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
B_{1} & 0 & \cdots & 0\\
0 & B_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & B_{k}%
\end{array}
\right)  =\dim\operatorname*{Ker}B_{1}+\dim\operatorname*{Ker}B_{2}%
+\cdots+\dim\operatorname*{Ker}B_{k},
\]
since a block-structured vector $\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{k}%
\end{array}
\right)  $ lies in $\operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
B_{1} & 0 & \cdots & 0\\
0 & B_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & B_{k}%
\end{array}
\right)  $ if and only if each of its pieces $v_{i}$ lies in
$\operatorname*{Ker}B_{i}$. Thus,%
\begin{align*}
& \dim\operatorname*{Ker}\left(  A-1I_{n}\right)  \\
& =\dim\operatorname*{Ker}\left(  J-1I_{n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }A\sim J\right)  \\
& =\dim\operatorname*{Ker}\left(  \left(
\begin{array}
[c]{ccc}%
J_{2}\left(  1\right)   & 0 & 0\\
0 & J_{3}\left(  1\right)   & 0\\
0 & 0 & J_{2}\left(  2\right)
\end{array}
\right)  -1I_{n}\right)  \\
& =\dim\operatorname*{Ker}\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  1\right)  -1I_{2} & 0 & 0\\
0 & J_{3}\left(  1\right)  -1I_{3} & 0\\
0 & 0 & J_{2}\left(  2\right)  -1I_{2}%
\end{array}
\right)  \\
& =\underbrace{\dim\operatorname*{Ker}\left(  J_{2}\left(  1\right)
-1I_{2}\right)  }_{\substack{=1\\\text{(by proposition above)}}%
}+\underbrace{\dim\operatorname*{Ker}\left(  J_{3}\left(  1\right)
-1I_{3}\right)  }_{\substack{=1\\\text{(by proposition above)}}%
}+\underbrace{\dim\operatorname*{Ker}\left(  J_{2}\left(  2\right)
-1I_{2}\right)  }_{\substack{=0\\\text{(since }J_{2}\left(  2\right)
-1I_{2}=\left(
\begin{array}
[c]{cc}%
1 & 1\\
0 & 1
\end{array}
\right)  \text{)}}}\\
& =1+1+0=2.
\end{align*}

\end{example}

In general, this reasoning yields the following:

\begin{proposition}
Let $A$ be an $n\times n$-matrix, and let $J=\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)   & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)   & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  $ be its Jordan canonical form. Then:

\textbf{(a)} We have $\sigma\left(  A\right)  =\left\{  \lambda_{1}%
,\lambda_{2},\ldots,\lambda_{k}\right\}  $.

\textbf{(b)} The geometric multiplicity of an eigenvalue $\lambda$ of $A$ is
the number of Jordan cells of $A$ at eigenvalue $\lambda$. In other words, it
is the number of $i\in\left[  k\right]  $ satisfying $\lambda_{i}=\lambda$.

\textbf{(c)} The algebraic multiplicity of an eigenvalue $\lambda$ of $A$ is
the \textbf{sum} of the sizes of all Jordan cells of $A$ at eigenvalue
$\lambda$. In other words, it is $\sum\limits_{\substack{i\in\left[  k\right]
;\\\lambda_{i}=\lambda}}n_{i}$.
\end{proposition}


\end{document}