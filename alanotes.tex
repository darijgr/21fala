\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{needspace}
\usepackage{tabls}
\usepackage{wasysym}
\usepackage{easytable}
\usepackage{pythonhighlight}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Monday, October 25, 2021 10:52:37}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\usetikzlibrary{arrows.meta}
\usetikzlibrary{chains}
\newcounter{exer}
\newcounter{exera}
\numberwithin{exer}{subsection}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[subsection]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{convention}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{quest}[theo]{Question}
\newenvironment{question}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{warning}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\newtheorem{exmp}[exer]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\newenvironment{statement}{\begin{quote}}{\end{quote}}
\newenvironment{fineprint}{\medskip \begin{small}}{\end{small} \medskip}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{question}[1][Question]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{warning}[1][Warning]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\newenvironment{teachingnote}[1][Teaching note]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\setlength\tablinesep{3pt}
\setlength\arraylinesep{3pt}
\setlength\extrarulesep{3pt}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newcommand\arxiv[1]{\href{http://www.arxiv.org/abs/#1}{\texttt{arXiv:#1}}}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\newenvironment{teachingnote}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\excludecomment{teachingnote}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\rev}{\operatorname{rev}}
\newcommand{\powset}[2][]{\ifthenelse{\equal{#2}{}}{\mathcal{P}\left(#1\right)}{\mathcal{P}_{#1}\left(#2\right)}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\tup}[1]{\left( #1 \right)}
\newcommand{\ive}[1]{\left[ #1 \right]}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\lf}[2]{#1^{\underline{#2}}}
\newcommand{\underbrack}[2]{\underbrace{#1}_{\substack{#2}}}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\are}{\ar@{-}}
\newcommand{\nnn}{\nonumber\\}
\newcommand{\sslash}{\mathbin{/\mkern-6mu/}}
\newcommand{\numboxed}[2]{\underbrace{\boxed{#1}}_{\text{box } #2}}
\newcommand{\ig}[2]{\includegraphics[scale=#1]{#2.png}}
\newcommand{\UNFINISHED}{\begin{center} \Huge{\textbf{Unfinished material begins here.}} \end{center} }
\iffalse
\NOEXPAND{\today}{\today}
\NOEXPAND{\sslash}{\sslash}
\NOEXPAND{\numboxed}[2]{\numboxed}
\NOEXPAND{\UNFINISHED}{\UNFINISHED}
\fi
\ihead{Math 504 notes}
\ohead{page \thepage}
\cfoot{\today}
\begin{document}

\title{Math 504: Advanced Linear Algebra}
\author{Hugo Woerdeman, with edits by Darij Grinberg\thanks{Drexel University, Korman
Center, 15 S 33rd Street, Philadelphia PA, 19104, USA}}
\date{\today\ (unfinished!)}
\maketitle
\tableofcontents

\section*{Preface}

These are lecture notes originally written by Hugo Woerdeman and edited by
myself for the Math 504 (Advanced Linear Algebra) class at Drexel University
in Fall 2021. The website of this class can be found at%
\[
\text{\url{http://www.cip.ifi.lmu.de/~grinberg/t/21fala}\ \ .}%
\]


\textbf{This document is a work in progress.}

\textbf{Please report any errors you find to
\texttt{\href{mailto:darijgrinberg@gmail.com}{darijgrinberg@gmail.com}} .}

\subsection*{What is this?}

This is a second course on linear algebra, meant for (mostly graduate)
students that are already familiar with matrices, determinants and vector
spaces. Much of the prerequisites (but also some of our material, and even
some content that goes beyond our course) is covered by textbooks like
\cite{Heffer20}, \cite{LaNaSc16}, \cite{Taylor20}, \cite{Treil15},
\cite{Strick20}, \cite[Part I]{GalQua20}, \cite{Loehr14}, \cite{Woerde16}%
\footnote{This list is nowhere near complete. (It is biased towards freely
available sources, but even in that category it is probably far from
comprehensive.)}. The text we will follow the closest is \cite{HorJoh13}.

We will freely use the basic theory of complex numbers, including the
Fundamental Theorem of Algebra. See \cite[Chapters 2--3]{LaNaSc16} or
\cite[Chapters 9--10]{Korner20} for an introduction to these matters.

\subsection*{Notations}

\begin{itemize}
\item We let $\mathbb{N}:=\left\{  0,1,2,\ldots\right\}  $.

\item For any $n\in\mathbb{N}$, we let $\left[  n\right]  $ denote the
$n$-element set $\left\{  1,2,\ldots,n\right\}  $.

\item If $\mathbb{F}$ is a field, and $n,m\in\mathbb{N}$, then $\mathbb{F}%
^{n\times m}$ denotes the set (actually, an $\mathbb{F}$-vector space) of all
$n\times m$-matrices over $\mathbb{F}$.

\item If $\mathbb{F}$ is a field, and $n\in\mathbb{N}$, then the space
$\mathbb{F}^{n\times1}$ of all $n\times1$-matrices over $\mathbb{F}$ (that is,
column vectors of size $n$) is also denoted by $\mathbb{F}^{n}$.

\item The $n\times n$ identity matrix is denoted by $I_{n}$ or by $I$ if the
$n$ is clear from the context.

\item The transpose of a matrix $A$ is denoted by $A^{T}$.

\item If $A$ is an $n\times m$-matrix, and if $i\in\left[  n\right]  $ and
$j\in\left[  m\right]  $, then:

\begin{itemize}
\item we let $A_{i,j}$ denote the $\left(  i,j\right)  $-th entry of $A$ (that
is, the entry of $A$ in the $i$-th row and the $j$-th column);

\item we let $A_{i,\bullet}$ denote the $i$-th row of $A$;

\item we let $A_{\bullet,j}$ denote the $j$-th column of $A$.
\end{itemize}

\item The letter $i$ usually denotes the complex number $\sqrt{-1}$. Sometimes
(e.g. in the bullet point just above) it also stands for something else
(usually an \textbf{i}ndex that is an \textbf{i}nteger). I'll do my best to
avoid the latter meaning when there is any realistic chance that it be
confused for the former.

\item We use the notation $\operatorname*{diag}\left(  \lambda_{1},\lambda
_{2},\ldots,\lambda_{n}\right)  $ for the diagonal matrix with diagonal
entries $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$.
\end{itemize}

\subsection{Remark on exercises}

Each exercise gives a number of \textquotedblleft experience
points\textquotedblright, which roughly corresponds to its difficulty (with
some adjustment for its relevance). This is the number in the square (like
\fbox{3} or \fbox{5}). The harder or more important the exercise, the larger
is the number in the square. A \fbox{1} is a warm-up question whose solution
you will probably see right after reading; a \fbox{3} typically requires some
thinking or work; a \fbox{5} requires both; higher values tend to involve some
creativity or research.

\subsection{Scribes}

Parts of these notes were scribed by Math 504 students. I thank the following
students for their help:%
\[%
\begin{tabular}
[c]{c|c}%
scribe & sections\\\hline\hline
Hunter Wages & proof of Theorem \ref{thm.schurtri.syl.equivalence}%
\end{tabular}
\]


\newpage

\section{Unitary matrices (\cite[\S 2.1]{HorJoh13})}

In this chapter, $n$ will usually denote a nonnegative integer.

\begin{noncompile}%
\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Old lecture 1 starts here.}\\\hline\hline
\end{tabular}
\ \
\]

\end{noncompile}

%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 1 starts here.}\\\hline\hline
\end{tabular}
\ \
\]


\subsection{Inner products}

We recall a basic definition regarding complex numbers:

\begin{definition}
Let $z\in\mathbb{C}$ be a complex number. Then, the \emph{complex conjugate}
of $z$ means the complex number $a-bi$, where $z$ is written in the form
$z=a+bi$ for some $a,b\in\mathbb{R}$. In other words, the complex conjugate of
$z$ is obtained from $z$ by keeping the real part unchanged but flipping the
sign of the imaginary part.

The complex conjugate of $z$ is denoted by $\overline{z}$.
\end{definition}

Complex conjugation is known to preserve all arithmetic operations: i.e., for
any complex numbers $z$ and $w$, we have%
\begin{align*}
\overline{z+w}  &  =\overline{z}+\overline{w}\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ \overline{z-w}=\overline{z}-\overline{w}%
\ \ \ \ \ \ \ \ \ \ \text{and}\\
\overline{z\cdot w}  &  =\overline{z}\cdot\overline{w}%
\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \overline{z/w}=\overline
{z}/\overline{w}.
\end{align*}
Also, a complex number $z$ satisfies $\overline{z}=z$ if and only if
$z\in\mathbb{R}$. Finally, if $z$ is any complex number, then $z\overline
{z}=\left\vert z\right\vert ^{2}$ is a nonnegative real.

\begin{definition}
\label{def.unitary.innerprod.innerprod}For any two vectors $x=\left(
\begin{array}
[c]{c}%
x_{1}\\
x_{2}\\
\vdots\\
x_{n}%
\end{array}
\right)  \in\mathbb{C}^{n}$ and $y=\left(
\begin{array}
[c]{c}%
y_{1}\\
y_{2}\\
\vdots\\
y_{n}%
\end{array}
\right)  \in\mathbb{C}^{n}$, we define the scalar%
\begin{equation}
\left\langle x,y\right\rangle :=x_{1}\overline{y_{1}}+x_{2}\overline{y_{2}%
}+\cdots+x_{n}\overline{y_{n}}\in\mathbb{C}
\label{eq.def.unitary.innerprod.innerprod.def}%
\end{equation}
(where $\overline{z}$ denotes the complex conjugate of a $z\in\mathbb{C}$).
This scalar $\left\langle x,y\right\rangle $ is called the \emph{inner
product} (or \emph{dot product}) of $x$ and $y$.
\end{definition}

\begin{example}
If $x=\left(
\begin{array}
[c]{c}%
1+i\\
2+3i
\end{array}
\right)  \in\mathbb{C}^{2}$ and $y=\left(
\begin{array}
[c]{c}%
-i\\
4+i
\end{array}
\right)  \in\mathbb{C}^{2}$, then%
\begin{align*}
\left\langle x,y\right\rangle  &  =\left(  1+i\right)  \left(  \overline
{-i}\right)  +\left(  2+3i\right)  \left(  \overline{4+i}\right) \\
&  =\left(  1+i\right)  i+\left(  2+3i\right)  \left(  4-i\right) \\
&  =i-1+8-2i+12i+3=10+11i.
\end{align*}

\end{example}

Some warnings about the literature are in order:

\begin{itemize}
\item Some authors (e.g., Treil in \cite{Treil15}) write $\left(  x,y\right)
$ instead of $\left\langle x,y\right\rangle $ for the inner product of $x$ and
$y$. This can be rather confusing, since $\left(  x,y\right)  $ also means the
pair consisting of $x$ and $y$.

\item The notation $\left\langle x,y\right\rangle $, too, can mean something
different in certain texts (namely, the span of $x$ and $y$); however, it
won't have this second meaning in our course.

\item If I am not mistaken, Definition \ref{def.unitary.innerprod.innerprod}
is also not the only game in town. Some authors follow a competing standard,
which causes their $\left\langle x,y\right\rangle $ to be what we would denote
$\left\langle y,x\right\rangle $.

\item Finally, the word \textquotedblleft dot product\textquotedblright\ often
means the analogue of $\left\langle x,y\right\rangle $ that does not use
complex conjugation (i.e., that replaces
(\ref{eq.def.unitary.innerprod.innerprod.def}) by $\left\langle
x,y\right\rangle :=x_{1}y_{1}+x_{2}y_{2}+\cdots+x_{n}y_{n}$). This convention
is used mostly in abstract algebra, where complex conjugation is not
considered intrinsic to the number system. We will not use this convention.
For vectors with real entries, the distinction disappears, since
$\overline{\lambda}=\lambda$ for any $\lambda\in\mathbb{R}$.
\end{itemize}

\begin{definition}
\label{def.unitary.innerprod.ystar}For any column vector $y=\left(
\begin{array}
[c]{c}%
y_{1}\\
y_{2}\\
\vdots\\
y_{n}%
\end{array}
\right)  \in\mathbb{C}^{n}$, we define the row vector
\[
y^{\ast}:=\left(
\begin{array}
[c]{cccc}%
\overline{y_{1}} & \overline{y_{2}} & \cdots & \overline{y_{n}}%
\end{array}
\right)  \in\mathbb{C}^{1\times n}.
\]

\end{definition}

\begin{proposition}
\label{prop.unitary.innerprod.props}Let $x\in\mathbb{C}^{n}$ and
$y\in\mathbb{C}^{n}$. Then: \medskip

\textbf{(a)} We have $\left\langle x,y\right\rangle =y^{\ast}x$. \medskip

\textbf{(b)} We have $\left\langle x,y\right\rangle =\overline{\left\langle
y,x\right\rangle }$. \medskip

\textbf{(c)} We have $\left\langle x+x^{\prime},y\right\rangle =\left\langle
x,y\right\rangle +\left\langle x^{\prime},y\right\rangle $ for any $x^{\prime
}\in\mathbb{C}^{n}$. \medskip

\textbf{(d)} We have $\left\langle x,y+y^{\prime}\right\rangle =\left\langle
x,y\right\rangle +\left\langle x,y^{\prime}\right\rangle $ for any $y^{\prime
}\in\mathbb{C}^{n}$. \medskip

\textbf{(e)} We have $\left\langle \lambda x,y\right\rangle =\lambda
\left\langle x,y\right\rangle $ for any $\lambda\in\mathbb{C}$. \medskip

\textbf{(f)} We have $\left\langle x,\lambda y\right\rangle =\overline
{\lambda}\left\langle x,y\right\rangle $ for any $\lambda\in\mathbb{C}$.
\medskip

\textbf{(g)} We have $\left\langle x-x^{\prime},y\right\rangle =\left\langle
x,y\right\rangle -\left\langle x^{\prime},y\right\rangle $ for any $x^{\prime
}\in\mathbb{C}^{n}$. \medskip

\textbf{(h)} We have $\left\langle x,y-y^{\prime}\right\rangle =\left\langle
x,y\right\rangle -\left\langle x,y^{\prime}\right\rangle $ for any $y^{\prime
}\in\mathbb{C}^{n}$. \medskip

\textbf{(i)} We have $\left\langle \sum_{i=1}^{k}\lambda_{i}x_{i}%
,y\right\rangle =\sum_{i=1}^{k}\lambda_{i}\left\langle x_{i},y\right\rangle $
for any $k\in\mathbb{N}$, any $x_{1},x_{2},\ldots,x_{k}\in\mathbb{C}^{n}$ and
any $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}\in\mathbb{C}$. \medskip

\textbf{(j)} We have $\left\langle x,\sum_{i=1}^{k}\lambda_{i}y_{i}%
\right\rangle =\sum_{i=1}^{k}\overline{\lambda_{i}}\left\langle x,y_{i}%
\right\rangle $ for any $k\in\mathbb{N}$, any $y_{1},y_{2},\ldots,y_{k}%
\in\mathbb{C}^{n}$ and any $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}%
\in\mathbb{C}$.
\end{proposition}

\begin{proof}
Parts \textbf{(a)} till \textbf{(h)} are straightforward computations using
Definition \ref{def.unitary.innerprod.innerprod}, since

\begin{itemize}
\item the multiplication in $\mathbb{C}$ is commutative;

\item we have $\overline{\overline{z}}=z$ for any $z\in\mathbb{C}$.
\end{itemize}

Parts \textbf{(i)} and \textbf{(j)} follow from parts \textbf{(c)},
\textbf{(d)}, \textbf{(e)} and \textbf{(f)} by induction on $k$.
\end{proof}

\begin{proposition}
\label{prop.unitary.innerprod.pos}Let $x\in\mathbb{C}^{n}$. Then: \medskip

\textbf{(a)} The number $\left\langle x,x\right\rangle $ is a nonnegative
real. \medskip

\textbf{(b)} We have $\left\langle x,x\right\rangle >0$ whenever $x\neq0$.
\end{proposition}

\begin{proof}
Write $x$ as $x=\left(
\begin{array}
[c]{cccc}%
x_{1} & x_{2} & \cdots & x_{n}%
\end{array}
\right)  ^{T}$. Then, the definition of $\left\langle x,x\right\rangle $
yields%
\begin{align}
\left\langle x,x\right\rangle  &  =x_{1}\overline{x_{1}}+x_{2}\overline{x_{2}%
}+\cdots+x_{n}\overline{x_{n}}\nonumber\\
&  =\left\vert x_{1}\right\vert ^{2}+\left\vert x_{2}\right\vert ^{2}%
+\cdots+\left\vert x_{n}\right\vert ^{2},
\label{pf.prop.unitary.innerprod.pos.1}%
\end{align}
since any complex number $z$ satisfies $z\overline{z}=\left\vert z\right\vert
^{2}$. Since all the absolute values $\left\vert x_{1}\right\vert ,\left\vert
x_{2}\right\vert ,\ldots,\left\vert x_{n}\right\vert $ are real, this yields
immediately that $\left\langle x,x\right\rangle $ is a nonnegative real. Thus,
Proposition \ref{prop.unitary.innerprod.pos} \textbf{(a)} is proved. \medskip

\textbf{(b)} Assume that $x\neq0$. Thus, at least one $i\in\left[  n\right]  $
satisfies $x_{i}\neq0$ and therefore $\left\vert x_{i}\right\vert ^{2}>0$.
This entails $\left\langle x,x\right\rangle =\left\vert x_{1}\right\vert
^{2}+\left\vert x_{2}\right\vert ^{2}+\cdots+\left\vert x_{n}\right\vert
^{2}>0$ (because a sum of nonnegative reals that has at least one positive
addend is always $>0$). In view of (\ref{pf.prop.unitary.innerprod.pos.1}),
this rewrites as $\left\langle x,x\right\rangle >0$. This proves Proposition
\ref{prop.unitary.innerprod.pos} \textbf{(b)}.
\end{proof}

\begin{definition}
\label{def.unitary.innerprod.len}Let $x\in\mathbb{C}^{n}$. We define the
\emph{length} of $x$ to be the nonnegative real number
\[
\left\vert \left\vert x\right\vert \right\vert :=\sqrt{\left\langle
x,x\right\rangle }.
\]
This is well-defined, since Proposition \ref{prop.unitary.innerprod.pos}
\textbf{(a)} says that $\left\langle x,x\right\rangle $ is a nonnegative real.
\end{definition}

\begin{example}
If $x=\left(
\begin{array}
[c]{c}%
1+i\\
3-2i
\end{array}
\right)  \in\mathbb{C}^{2}$, then%
\begin{align*}
\left\langle x,x\right\rangle  &  =\left(  1+i\right)  \left(  \overline
{1+i}\right)  +\left(  3-2i\right)  \left(  \overline{3+2i}\right)  =\left(
1+i\right)  \left(  1-i\right)  +\left(  3-2i\right)  \left(  3+2i\right) \\
&  =1+1+9+4=15
\end{align*}
and thus $\left\vert \left\vert x\right\vert \right\vert =\sqrt{\left\langle
x,x\right\rangle }=\sqrt{15}$.
\end{example}

The length $\left\vert \left\vert x\right\vert \right\vert $ of a vector
$x\in\mathbb{C}^{n}$ is sometimes also called the \emph{norm} of $x$ (but
beware that other things are called \textquotedblleft norms\textquotedblright%
\ as well).

\begin{proposition}
For any $\lambda\in\mathbb{C}$ and $x\in\mathbb{C}^{n}$, we have $\left\vert
\left\vert \lambda x\right\vert \right\vert =\left\vert \lambda\right\vert
\cdot\left\vert \left\vert x\right\vert \right\vert $.
\end{proposition}

\begin{proof}
Straightforward.
\end{proof}

\begin{exercise}
\label{exe.unitary.innerprod.x+y}\fbox{3} Let $x\in\mathbb{C}^{n}$ and
$y\in\mathbb{C}^{n}$. Prove that
\[
\left\vert \left\vert x+y\right\vert \right\vert ^{2}-\left\vert \left\vert
x\right\vert \right\vert ^{2}-\left\vert \left\vert y\right\vert \right\vert
^{2}=\left\langle x,y\right\rangle +\left\langle y,x\right\rangle
=2\cdot\operatorname*{Re}\left\langle x,y\right\rangle .
\]
Here, $\operatorname*{Re}z$ denotes the real part of any complex number $z$.
\end{exercise}

One of the most famous properties of the inner product is the
\emph{Cauchy--Schwarz inequality} (see \cite{Steele04} for various applications):

\begin{theorem}
[Cauchy--Schwarz inequality]\label{thm.unitary.innerprod.cs}Let $x\in
\mathbb{C}^{n}$ and $y\in\mathbb{C}^{n}$ be two vectors. Then:

\textbf{(a)} The inequality%
\[
\left\vert \left\vert x\right\vert \right\vert \cdot\left\vert \left\vert
y\right\vert \right\vert \geq\left\vert \left\langle x,y\right\rangle
\right\vert
\]
holds.

\textbf{(b)} This inequality becomes an equality if and only if the pair
$\left(  x,y\right)  $ of vectors is linearly dependent.
\end{theorem}

\begin{proof}
[Proof of Theorem \ref{thm.unitary.innerprod.cs}.]If $x=0$, then Theorem
\ref{thm.unitary.innerprod.cs} is obvious (because the inequality in part
\textbf{(a)} simplifies to $0\geq0$, and since the pair $\left(  0,y\right)  $
of vectors is always linearly dependent). Hence, for the rest of this proof,
we WLOG assume that $x\neq0$.

Thus, Proposition \ref{prop.unitary.innerprod.pos} \textbf{(a)} yields that
$\left\langle x,x\right\rangle $ is a nonnegative real, and Proposition
\ref{prop.unitary.innerprod.pos} \textbf{(b)} yields $\left\langle
x,x\right\rangle >0$. Let $a:=\left\langle x,x\right\rangle $. Then,
$a=\left\langle x,x\right\rangle >0$. Furthermore, let $b:=\left\langle
y,x\right\rangle \in\mathbb{C}$. Thus, $\overline{b}=\overline{\left\langle
y,x\right\rangle }=\left\langle x,y\right\rangle $ (by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(b)}).

Now, Proposition \ref{prop.unitary.innerprod.pos} \textbf{(a)} (applied to
$bx-ay$ instead of $x$) yields that
\begin{equation}
\left\langle bx-ay,bx-ay\right\rangle \geq0.
\label{pf.thm.unitary.innerprod.cs.main}%
\end{equation}
Since%
\begin{align*}
&  \left\langle bx-ay,bx-ay\right\rangle \\
&  =\underbrace{\left\langle bx,bx-ay\right\rangle }_{\substack{=\left\langle
bx,bx\right\rangle -\left\langle bx,ay\right\rangle \\\text{(by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(h)})}}}-\underbrace{\left\langle
ay,bx-ay\right\rangle }_{\substack{=\left\langle ay,bx\right\rangle
-\left\langle ay,ay\right\rangle \\\text{(by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(h)})}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by Proposition \ref{prop.unitary.innerprod.props} \textbf{(g)}}\right)
\\
&  =\left\langle bx,bx\right\rangle -\left\langle bx,ay\right\rangle -\left(
\left\langle ay,bx\right\rangle -\left\langle ay,ay\right\rangle \right) \\
&  =\underbrace{\left\langle bx,bx\right\rangle }_{\substack{=b\left\langle
x,bx\right\rangle \\\text{(by Proposition \ref{prop.unitary.innerprod.props}
\textbf{(e)})}}}+\underbrace{\left\langle ay,ay\right\rangle }%
_{\substack{=a\left\langle y,ay\right\rangle \\\text{(by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(e)})}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left\langle bx,ay\right\rangle
}_{\substack{=b\left\langle x,ay\right\rangle \\\text{(by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(e)})}}}-\underbrace{\left\langle
ay,bx\right\rangle }_{\substack{=a\left\langle y,bx\right\rangle \\\text{(by
Proposition \ref{prop.unitary.innerprod.props} \textbf{(e)})}}}\\
&  =b\underbrace{\left\langle x,bx\right\rangle }_{\substack{=\overline
{b}\left\langle x,x\right\rangle \\\text{(by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(f)})}}}+a\underbrace{\left\langle
y,ay\right\rangle }_{\substack{=\overline{a}\left\langle y,y\right\rangle
\\\text{(by Proposition \ref{prop.unitary.innerprod.props} \textbf{(f)})}}}\\
&  \ \ \ \ \ \ \ \ \ \ -b\underbrace{\left\langle x,ay\right\rangle
}_{\substack{=\overline{a}\left\langle x,y\right\rangle \\\text{(by
Proposition \ref{prop.unitary.innerprod.props} \textbf{(f)})}}%
}-a\underbrace{\left\langle y,bx\right\rangle }_{\substack{=\overline
{b}\left\langle y,x\right\rangle \\\text{(by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(f)})}}}\\
&  =b\overline{b}\underbrace{\left\langle x,x\right\rangle }_{=a}%
+a\underbrace{\overline{a}}_{\substack{=a\\\text{(since }a\in\mathbb{R}%
\text{)}}}\left\langle y,y\right\rangle -b\underbrace{\overline{a}%
}_{\substack{=a\\\text{(since }a\in\mathbb{R}\text{)}}%
}\underbrace{\left\langle x,y\right\rangle }_{=\overline{b}}-a\overline
{b}\underbrace{\left\langle y,x\right\rangle }_{=b}\\
&  =b\overline{b}a+\underbrace{aa}_{=a^{2}}\left\langle y,y\right\rangle
-\underbrace{ba}_{=ab}\overline{b}-\underbrace{a\overline{b}b}_{=b\overline
{b}a}\\
&  =b\overline{b}a+a^{2}\left\langle y,y\right\rangle -ab\overline
{b}-b\overline{b}a=a^{2}\left\langle y,y\right\rangle -ab\overline{b}=a\left(
a\left\langle y,y\right\rangle -b\overline{b}\right)  ,
\end{align*}
we can rewrite this as
\[
a\left(  a\left\langle y,y\right\rangle -b\overline{b}\right)  \geq0.
\]
We can divide both sides of this inequality by $a$ (since $a>0$). Thus, we
obtain
\[
a\left\langle y,y\right\rangle -b\overline{b}\geq0.
\]
In other words,%
\[
a\left\langle y,y\right\rangle \geq b\overline{b}.
\]
In view of
\[
a=\left\langle x,x\right\rangle =\left\vert \left\vert x\right\vert
\right\vert ^{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert \left\vert
x\right\vert \right\vert =\sqrt{\left\langle x,x\right\rangle }\text{ (by the
definition of }\left\vert \left\vert x\right\vert \right\vert \text{)}\right)
\]
and%
\[
\left\langle y,y\right\rangle =\left\vert \left\vert y\right\vert \right\vert
^{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert \left\vert
y\right\vert \right\vert =\sqrt{\left\langle y,y\right\rangle }\text{ (by the
definition of }\left\vert \left\vert y\right\vert \right\vert \text{)}\right)
\]
and%
\[
b\overline{b}=\overline{b}\underbrace{b}_{=\overline{\overline{b}}}%
=\overline{b}\overline{\overline{b}}=\left\vert \overline{b}\right\vert
^{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{because }z\overline{z}=\left\vert
z\right\vert ^{2}\text{ for any }z\in\mathbb{C}\right)  ,
\]
we can rewrite this as $\left\vert \left\vert x\right\vert \right\vert
^{2}\left\vert \left\vert y\right\vert \right\vert ^{2}\geq\left\vert
\overline{b}\right\vert ^{2}$. Since $\left\vert \left\vert x\right\vert
\right\vert $ and $\left\vert \left\vert y\right\vert \right\vert $ and
$\left\vert \overline{b}\right\vert $ are nonnegative reals, we can take
square roots on both sides of this inequality, and obtain $\left\vert
\left\vert x\right\vert \right\vert \cdot\left\vert \left\vert y\right\vert
\right\vert \geq\left\vert \overline{b}\right\vert $. In other words,
$\left\vert \left\vert x\right\vert \right\vert \cdot\left\vert \left\vert
y\right\vert \right\vert \geq\left\vert \left\langle x,y\right\rangle
\right\vert $ (since $\overline{b}=\left\langle x,y\right\rangle $). This
proves Theorem \ref{thm.unitary.innerprod.cs} \textbf{(a)}. \medskip

\textbf{(b)} Our above proof of the inequality $\left\vert \left\vert
x\right\vert \right\vert \cdot\left\vert \left\vert y\right\vert \right\vert
\geq\left\vert \left\langle x,y\right\rangle \right\vert $ shows that this
inequality can only become an equality if $\left\langle
bx-ay,bx-ay\right\rangle =0$ (since it was obtained by a chain of reversible
transformations from the inequality (\ref{pf.thm.unitary.innerprod.cs.main})).
But this happens if and only if $bx-ay=0$ (since Proposition
\ref{prop.unitary.innerprod.pos} \textbf{(b)} shows that $\left\langle
bx-ay,bx-ay\right\rangle >0$ in any other case). In turn, $bx-ay=0$ entails
that the pair $\left(  x,y\right)  $ is linearly dependent (since $a>0$).
Thus, the inequality $\left\vert \left\vert x\right\vert \right\vert
\cdot\left\vert \left\vert y\right\vert \right\vert \geq\left\vert
\left\langle x,y\right\rangle \right\vert $ can only become an equality if the
pair $\left(  x,y\right)  $ is linearly dependent. Conversely, it is easy to
see that if the pair $\left(  x,y\right)  $ is linearly dependent, then the
inequality $\left\vert \left\vert x\right\vert \right\vert \cdot\left\vert
\left\vert y\right\vert \right\vert \geq\left\vert \left\langle
x,y\right\rangle \right\vert $ indeed becomes an equality (because in light of
$x\neq0$, the linear dependence of the pair $\left(  x,y\right)  $ yields that
$y=\lambda x$ for some $\lambda\in\mathbb{C}$). Thus, Theorem
\ref{thm.unitary.innerprod.cs} \textbf{(b)} is proven.
\end{proof}

Using Theorem \ref{thm.unitary.innerprod.cs} and Exercise
\ref{exe.unitary.innerprod.x+y}, we can easily obtain the following:

\begin{theorem}
[triangle inequality]\label{thm.unitary.innerprod.norm-tria} Let
$x\in\mathbb{C}^{n}$ and $y\in\mathbb{C}^{n}$. Then:

\textbf{(a)} The inequality $\left\vert \left\vert x\right\vert \right\vert
+\left\vert \left\vert y\right\vert \right\vert \geq\left\vert \left\vert
x+y\right\vert \right\vert $ holds.

\textbf{(b)} This inequality becomes an equality if and only if we have $y=0$
or $x=\lambda y$ for some nonnegative real $x$.
\end{theorem}

\begin{exercise}
\label{exe.unitary.innerprod.norm-tria}\fbox{3} Prove Theorem
\ref{thm.unitary.innerprod.norm-tria}.
\end{exercise}

Theorem \ref{thm.unitary.innerprod.norm-tria} \textbf{(a)} is the reason why
the map $\mathbb{C}^{n}\rightarrow\mathbb{R},\ x\mapsto\left\vert \left\vert
x\right\vert \right\vert $ is called a \textquotedblleft
norm\textquotedblright.

\subsection{Orthogonality and orthonormality}

We shall now define orthogonality first for two vectors, then for any tuple of vectors.

\begin{definition}
\label{def.unitary.innerprod.orth}Let $x\in\mathbb{C}^{n}$ and $y\in
\mathbb{C}^{n}$ be two vectors. We say that $x$ is \emph{orthogonal} to $y$ if
and only if $\left\langle x,y\right\rangle =0$. The shorthand notation for
this is \textquotedblleft$x\perp y$\textquotedblright.
\end{definition}

The relation $\perp$ is symmetric:

\begin{proposition}
\label{prop.unitary.innerprod.orth-symm}Let $x\in\mathbb{C}^{n}$ and
$y\in\mathbb{C}^{n}$ be two vectors. Then, $x\perp y$ holds if and only if
$y\perp x$.
\end{proposition}

\begin{proof}
Follows from Proposition \ref{prop.unitary.innerprod.props} \textbf{(b)}.
\end{proof}

\begin{definition}
\label{def.unitary.innerprod.orth-n}Let $\left(  u_{1},u_{2},\ldots
,u_{k}\right)  $ be a tuple of vectors in $\mathbb{C}^{n}$. Then: \medskip

\textbf{(a)} We say that the tuple $\left(  u_{1},u_{2},\ldots,u_{k}\right)  $
is \emph{orthogonal} if we have
\[
u_{p}\perp u_{q}\ \ \ \ \ \ \ \ \ \ \text{whenever }p\neq q.
\]


\textbf{(b)} We say that the tuple $\left(  u_{1},u_{2},\ldots,u_{k}\right)  $
is \emph{orthonormal} if it is orthogonal \textbf{and} satisfies%
\[
\left\vert \left\vert u_{1}\right\vert \right\vert =\left\vert \left\vert
u_{2}\right\vert \right\vert =\cdots=\left\vert \left\vert u_{k}\right\vert
\right\vert =1.
\]


\textbf{(c)} We note that the orthogonality and the orthonormality of a tuple
are preserved when the entries of the tuple are permuted. Thus, we can extend
both notions (\textquotedblleft orthogonal\textquotedblright\ and
\textquotedblleft orthonormal\textquotedblright) to finite sets of vectors in
$\mathbb{C}^{n}$: A set $\left\{  u_{1},u_{2},\ldots,u_{k}\right\}  $ of
vectors in $\mathbb{C}^{n}$ (with $u_{1},u_{2},\ldots,u_{k}$ being distinct)
is said to be \emph{orthogonal} (or \emph{orthonormal}, respectively) if and
only if the tuple $\left(  u_{1},u_{2},\ldots,u_{k}\right)  $ is orthogonal
(resp., orthonormal). \medskip

\textbf{(d)} Sometimes, we (sloppily) say \textquotedblleft the vectors
$u_{1},u_{2},\ldots,u_{k}$ are orthogonal\textquotedblright\ when we mean
\textquotedblleft the tuple $\left(  u_{1},u_{2},\ldots,u_{k}\right)  $ is
orthogonal\textquotedblright. The same applies to \textquotedblleft
orthonormal\textquotedblright.
\end{definition}

\begin{example}
\textbf{(a)} The tuple $\left(  \left(
\begin{array}
[c]{c}%
1\\
0\\
0
\end{array}
\right)  ,\left(
\begin{array}
[c]{c}%
0\\
1\\
0
\end{array}
\right)  ,\left(
\begin{array}
[c]{c}%
0\\
0\\
1
\end{array}
\right)  \right)  $ of vectors in $\mathbb{C}^{3}$ is orthonormal. It is also
a basis of $\mathbb{C}^{3}$, and known as the \emph{standard basis}. \medskip

\textbf{(b)} More generally: Let $n\in\mathbb{N}$. Let $e_{1},e_{2}%
,\ldots,e_{n}\in\mathbb{C}^{n}$ be the vectors defined by%
\[
e_{i}=\underbrace{\left(
\begin{array}
[c]{ccccccccc}%
0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0
\end{array}
\right)  ^{T}}_{\substack{\text{the }1\text{ is in the }i\text{-th
position;}\\\text{all other entries are }0}}.
\]
Then, $\left(  e_{1},e_{2},\ldots,e_{n}\right)  $ is an orthonormal basis of
$\mathbb{C}^{n}$, and is known as the \emph{standard basis} of $\mathbb{C}%
^{n}$. \medskip

\textbf{(c)} The pair $\left(  \left(
\begin{array}
[c]{c}%
1\\
-i\\
2
\end{array}
\right)  ,\left(
\begin{array}
[c]{c}%
0\\
2i\\
1
\end{array}
\right)  \right)  $ of vectors in $\mathbb{C}^{3}$ is orthogonal (but not
orthonormal). Indeed,%
\[
\left\langle \left(
\begin{array}
[c]{c}%
1\\
-i\\
2
\end{array}
\right)  ,\left(
\begin{array}
[c]{c}%
0\\
2i\\
1
\end{array}
\right)  \right\rangle =1\cdot\overline{0}+\left(  -i\right)  \cdot
\overline{2i}+2\cdot\overline{1}=0-2+2=0.
\]


\textbf{(d)} The pair $\left(  \dfrac{1}{\sqrt{6}}\left(
\begin{array}
[c]{c}%
1\\
-i\\
2
\end{array}
\right)  ,\dfrac{1}{\sqrt{5}}\left(
\begin{array}
[c]{c}%
0\\
2i\\
1
\end{array}
\right)  \right)  $ of vectors in $\mathbb{C}^{3}$ is orthonormal. (This is
just the previous pair, with each vector scaled so that its length becomes $1$.)
\end{example}

\begin{proposition}
\label{prop.unitary.innerprod.orth-norm}Let $\left(  u_{1},u_{2},\ldots
,u_{k}\right)  $ be an orthogonal tuple of nonzero vectors in $\mathbb{C}^{n}%
$. Then, the tuple%
\[
\left(  \dfrac{1}{\left\vert \left\vert u_{1}\right\vert \right\vert }%
u_{1},\ \ \dfrac{1}{\left\vert \left\vert u_{2}\right\vert \right\vert }%
u_{2},\ \ \ldots,\ \ \dfrac{1}{\left\vert \left\vert u_{k}\right\vert
\right\vert }u_{k}\right)
\]
is orthonormal.
\end{proposition}

\begin{proof}
Straightforward. (Observe that any two orthogonal vectors remain orthogonal
when they are scaled by scalars.)
\end{proof}

\begin{proposition}
\label{prop.unitary.orthog.indep}Any orthogonal tuple of nonzero vectors in
$\mathbb{C}^{n}$ is linearly independent.
\end{proposition}

\begin{proof}
Let $\left(  u_{1},u_{2},\ldots,u_{k}\right)  $ be an orthogonal tuple of
nonzero vectors in $\mathbb{C}^{n}$. We must prove that it is linearly independent.

Indeed, for any $i\in\left[  k\right]  $ and any $\lambda_{1},\lambda
_{2},\ldots,\lambda_{k}\in\mathbb{C}$, we have
\begin{align}
&  \left\langle \lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots+\lambda_{k}%
u_{k},u_{i}\right\rangle \nonumber\\
&  =\lambda_{1}\left\langle u_{1},u_{i}\right\rangle +\lambda_{2}\left\langle
u_{2},u_{i}\right\rangle +\cdots+\lambda_{k}\left\langle u_{k},u_{i}%
\right\rangle \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  \text{by parts \textbf{(c)}
and \textbf{(e)} of Proposition \ref{prop.unitary.innerprod.props}}\right)
\nonumber\\
&  =\lambda_{i}\left\langle u_{i},u_{i}\right\rangle +\sum_{\substack{j\in
\left[  k\right]  ;\\j\neq i}}\lambda_{j}\underbrace{\left\langle u_{j}%
,u_{i}\right\rangle }_{\substack{=0\\\text{(since }u_{j}\perp u_{i}%
\\\text{(because }\left(  u_{1},u_{2},\ldots,u_{k}\right)  \text{
is}\\\text{an orthogonal tuple))}}}\nonumber\\
&  =\lambda_{i}\left\langle u_{i},u_{i}\right\rangle .
\label{pf.prop.unitary.orthog.indep.1}%
\end{align}


For any $i\in\left[  k\right]  $, we have $u_{i}\neq0$ (since $\left(
u_{1},u_{2},\ldots,u_{k}\right)  $ is a tuple of nonzero vectors) and thus
\begin{equation}
\left\langle u_{i},u_{i}\right\rangle >0
\label{pf.prop.unitary.orthog.indep.pos}%
\end{equation}
(by Proposition \ref{prop.unitary.innerprod.pos} \textbf{(b)}, applied to
$x=u_{i}$).

Now, let $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}\in\mathbb{C}$ be such
that $\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots+\lambda_{k}u_{k}=0$. Then, for
each $i\in\left[  k\right]  $, we have%
\begin{align*}
\lambda_{i}\left\langle u_{i},u_{i}\right\rangle  &  =\left\langle
\underbrace{\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots+\lambda_{k}u_{k}}%
_{=0},u_{i}\right\rangle \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.unitary.orthog.indep.1})}\right) \\
&  =\left\langle 0,u_{i}\right\rangle =0
\end{align*}
and therefore $\lambda_{i}=0$ (indeed, we can divide by $\left\langle
u_{i},u_{i}\right\rangle $, because of (\ref{pf.prop.unitary.orthog.indep.pos})).

Forget that we fixed $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$. We thus
have shown that if $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}\in\mathbb{C}$
are such that $\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots+\lambda_{k}u_{k}=0$,
then we have $\lambda_{i}=0$ for each $i\in\left[  k\right]  $. In other
words, $\left(  u_{1},u_{2},\ldots,u_{k}\right)  $ is linearly independent.
This proves Proposition \ref{prop.unitary.orthog.indep}.
\end{proof}

The following simple lemma will be used further below:

\begin{lemma}
\label{lem.unitary.orthog.one-more}Let $k<n$. Let $a_{1},a_{2},\ldots,a_{k}$
be $k$ vectors in $\mathbb{C}^{n}$. Then, there exists a nonzero vector
$b\in\mathbb{C}^{n}$ that is orthogonal to each of $a_{1},a_{2},\ldots,a_{k}$.
\end{lemma}

\begin{proof}
Write each vector $a_{i}$ as $a_{i}=\left(
\begin{array}
[c]{cccc}%
a_{i,1} & a_{i,2} & \cdots & a_{i,n}%
\end{array}
\right)  ^{T}$. Now, consider an arbitrary vector $b=\left(
\begin{array}
[c]{cccc}%
b_{1} & b_{2} & \cdots & b_{n}%
\end{array}
\right)  ^{T}\in\mathbb{C}^{n}$, whose entries $b_{1},b_{2},\ldots,b_{n}$ are
so far undetermined. This new vector $b$ is orthogonal to each of $a_{1}%
,a_{2},\ldots,a_{k}$ if and only if it satisfies%
\[
\left\langle b,a_{i}\right\rangle =0\ \ \ \ \ \ \ \ \ \ \text{for all }%
i\in\left[  k\right]  .
\]
In other words, this new vector $b$ is orthogonal to each of $a_{1}%
,a_{2},\ldots,a_{k}$ if and only if it satisfies%
\[
b_{1}\overline{a_{i,1}}+b_{2}\overline{a_{i,2}}+\cdots+b_{n}\overline{a_{i,n}%
}=0\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left[  k\right]
\]
(since $\left\langle b,a_{i}\right\rangle =b_{1}\overline{a_{i,1}}%
+b_{2}\overline{a_{i,2}}+\cdots+b_{n}\overline{a_{i,n}}$ for each $i\in\left[
k\right]  $). In other words, this new vector $b$ is orthogonal to each of
$a_{1},a_{2},\ldots,a_{k}$ if and only if it satisfies the system of equations%
\[
\left\{
\begin{array}
[c]{c}%
b_{1}\overline{a_{1,1}}+b_{2}\overline{a_{1,2}}+\cdots+b_{n}\overline{a_{1,n}%
}=0;\\
b_{1}\overline{a_{2,1}}+b_{2}\overline{a_{2,2}}+\cdots+b_{n}\overline{a_{2,n}%
}=0;\\
\cdots;\\
b_{1}\overline{a_{k,1}}+b_{2}\overline{a_{k,2}}+\cdots+b_{n}\overline{a_{k,n}%
}=0.
\end{array}
\right.
\]
But this is a system of $k$ homogeneous linear equations in the $n$ unknowns
$b_{1},b_{2},\ldots,b_{n}$, and thus (by a classical fact in linear
algebra\footnote{The fact we are using here is the following: If $p$ and $q$
are two integers such that $0\leq p<q$, then any system of $p$ homogeneous
linear equations in $q$ unknowns has at least one nonzero solution. Rewritten
in terms of matrices, this is saying that if $p$ and $q$ are two integers such
that $0\leq p<q$, then any $p\times q$-matrix has a nonzero vector in its
kernel (= nullspace). For a proof, see, e.g., \cite[Remark 8.9]{Strick20} or
(rewritten in the language of linear maps) \cite[Corollary 6.5.3 item
1]{LaNaSc16}.}) has at least one nonzero solution (since $k<n$). In other
words, there exists at least one nonzero vector $b=\left(
\begin{array}
[c]{cccc}%
b_{1} & b_{2} & \cdots & b_{n}%
\end{array}
\right)  ^{T}\in\mathbb{C}^{n}$ that is orthogonal to each of $a_{1}%
,a_{2},\ldots,a_{k}$. This proves Lemma \ref{lem.unitary.orthog.one-more}.
\medskip

\begin{fineprint}
Here is a neater way to state the same argument: We define a map
$f:\mathbb{C}^{n}\rightarrow\mathbb{C}^{k}$ by setting%
\[
f\left(  w\right)  =\left(
\begin{array}
[c]{c}%
\left\langle w,a_{1}\right\rangle \\
\left\langle w,a_{2}\right\rangle \\
\vdots\\
\left\langle w,a_{k}\right\rangle
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{for each }w\in\mathbb{C}^{n}.
\]
It is easy to see that this map $f$ is $\mathbb{C}$-linear. (Indeed,
Proposition \ref{prop.unitary.innerprod.props} \textbf{(c)} shows that every
two vectors $x,x^{\prime}\in\mathbb{C}^{n}$ and every $i\in\left[  k\right]  $
satisfy $\left\langle x+x^{\prime},a_{i}\right\rangle =\left\langle
x,a_{i}\right\rangle +\left\langle x^{\prime},a_{i}\right\rangle $; therefore,
every two vectors $x,x^{\prime}\in\mathbb{C}^{n}$ satisfy $f\left(
x+x^{\prime}\right)  =f\left(  x\right)  +f\left(  x^{\prime}\right)  $.
Similarly, Proposition \ref{prop.unitary.innerprod.props} \textbf{(e)} can be
used to show that $f\left(  \lambda x\right)  =\lambda f\left(  x\right)  $
for each $\lambda\in\mathbb{C}$ and $x\in\mathbb{C}^{n}$. Hence, $f$ is
$\mathbb{C}$-linear.)

Now, we know that $f$ is a $\mathbb{C}$-linear map from $\mathbb{C}^{n}$ to
$\mathbb{C}^{k}$. Hence, the rank-nullity theorem (see, e.g., \cite[Chapter 2,
Theorem 7.2]{Treil15} or \cite[Chapter II, Corollary 2.15]{Knapp1} or
\cite[Proposition 3.3.35]{Goodman}) yields that%
\[
n=\dim\left(  \operatorname*{Ker}f\right)  +\dim\left(  \operatorname{Im}%
f\right)  ,
\]
where $\operatorname*{Ker}f$ denotes the kernel of $f$ (that is, the subspace
of $\mathbb{C}^{n}$ that consists of all vectors $v\in\mathbb{C}^{n}$
satisfying $f\left(  v\right)  =0$), and where $\operatorname{Im}f$ denotes
the image\footnote{also known as \textquotedblleft range\textquotedblright} of
$f$ (that is, the subspace of $\mathbb{C}^{k}$ consisting of all vectors of
the form $f\left(  v\right)  $ with $v\in\mathbb{C}^{n}$). Therefore,%
\[
\dim\left(  \operatorname*{Ker}f\right)  =n-\dim\left(  \operatorname{Im}%
f\right)  .
\]
However, $\operatorname{Im}f$ is a vector subspace of $\mathbb{C}^{k}$, and
thus has dimension $\leq k$. Thus, $\dim\left(  \operatorname{Im}f\right)
\leq k<n$, so that%
\[
\dim\left(  \operatorname*{Ker}f\right)  =n-\underbrace{\dim\left(
\operatorname{Im}f\right)  }_{<n}>n-n=0.
\]
This shows that the vector space $\operatorname*{Ker}f$ contains at least one
nonzero vector $b$. Consider this $b$. Thus, $b\in\operatorname*{Ker}%
f\subseteq\mathbb{C}^{n}$.

However, $b\in\operatorname*{Ker}f$ shows that $f\left(  b\right)  =0$. But
the definition of $f$ yields $f\left(  b\right)  =\left(
\begin{array}
[c]{c}%
\left\langle b,a_{1}\right\rangle \\
\left\langle b,a_{2}\right\rangle \\
\vdots\\
\left\langle b,a_{k}\right\rangle
\end{array}
\right)  $. Thus, $\left(
\begin{array}
[c]{c}%
\left\langle b,a_{1}\right\rangle \\
\left\langle b,a_{2}\right\rangle \\
\vdots\\
\left\langle b,a_{k}\right\rangle
\end{array}
\right)  =f\left(  b\right)  =0$. In other words, each $i\in\left[  k\right]
$ satisfies $\left\langle b,a_{i}\right\rangle =0$. In other words, each
$i\in\left[  k\right]  $ satisfies $b\perp a_{i}$. In other words, $b$ is
orthogonal to each of $a_{1},a_{2},\ldots,a_{k}$. Thus, we have found a
nonzero vector $b\in\mathbb{C}^{n}$ that is orthogonal to each of $a_{1}%
,a_{2},\ldots,a_{k}$. This proves Lemma \ref{lem.unitary.orthog.one-more}.
\end{fineprint}
\end{proof}

\begin{corollary}
\label{cor.unitary.orthog-extend}Let $\left(  u_{1},u_{2},\ldots,u_{k}\right)
$ be an orthogonal $k$-tuple of nonzero vectors in $\mathbb{C}^{n}$. Then, we
have $k\leq n$, and we can find $n-k$ further nonzero vectors $u_{k+1}%
,u_{k+2},\ldots,u_{n}$ such that $\left(  u_{1},u_{2},\ldots,u_{n}\right)  $
is an orthogonal basis of $\mathbb{C}^{n}$.
\end{corollary}

\begin{exercise}
\label{exe.unitary.orthog-extend}\fbox{2} Prove Corollary
\ref{cor.unitary.orthog-extend}.
\end{exercise}

\subsection{Conjugate transposes}

The following definition generalizes Definition
\ref{def.unitary.innerprod.ystar}:

\begin{definition}
\label{def.unitary.innerprod.A*}Let $A=\left(
\begin{array}
[c]{ccc}%
a_{1,1} & \cdots & a_{1,m}\\
\vdots & \ddots & \vdots\\
a_{n,1} & \cdots & a_{n,m}%
\end{array}
\right)  \in\mathbb{C}^{n\times m}$ be any $n\times m$-matrix. Then, we define
the $m\times n$-matrix%
\[
A^{\ast}:=\left(
\begin{array}
[c]{ccc}%
\overline{a_{1,1}} & \cdots & \overline{a_{n,1}}\\
\vdots & \ddots & \vdots\\
\overline{a_{1,m}} & \cdots & \overline{a_{n,m}}%
\end{array}
\right)  \in\mathbb{C}^{m\times n}.
\]
This matrix $A^{\ast}$ is called the \emph{conjugate transpose} of $A$.
\end{definition}

This conjugate transpose $A^{\ast}$ can thus be obtained from the usual
transpose $A^{T}$ by conjugating all entries.

\begin{example}
$\left(
\begin{array}
[c]{ccc}%
1+i & 2-3i & 5i\\
6 & 2+4i & 10-i
\end{array}
\right)  ^{\ast}=\left(
\begin{array}
[c]{cc}%
1-i & 6\\
2+3i & 2-4i\\
-5i & 10+i
\end{array}
\right)  $.
\end{example}

In the olden days, the conjugate transpose of a matrix was also known as the
\textquotedblleft adjoint\textquotedblright\ of $A$. Unsurprisingly, this word
has at least one other meaning, which opens the door to a lot of unwanted
confusion; thus we will speak of the \textquotedblleft conjugate
transpose\textquotedblright\ instead.

Some authors use the alternative notation $A^{\dag}$ (read \textquotedblleft%
$A$ dagger\textquotedblright) for $A^{\ast}$. (The Wikipedia suggests calling
it the \textquotedblleft bedaggered matrix $A$\textquotedblright, although I
am not aware of anyone using this terminology outside of the Wikipedia.)
\medskip

The following rules for conjugate transposes are straightforward to check:

\begin{proposition}
\label{prop.unitary.(AB)*}\textbf{(a)} If $A\in\mathbb{C}^{n\times m}$ and
$B\in\mathbb{C}^{n\times m}$ are two matrices, then $\left(  A+B\right)
^{\ast}=A^{\ast}+B^{\ast}$. \medskip

\textbf{(b)} If $A\in\mathbb{C}^{n\times m}$ and $\lambda\in\mathbb{C}$, then
$\left(  \lambda A\right)  ^{\ast}=\overline{\lambda}A^{\ast}$. \medskip

\textbf{(c)} If $A\in\mathbb{C}^{n\times m}$ and $B\in\mathbb{C}^{m\times k}$
are two matrices, then $\left(  AB\right)  ^{\ast}=B^{\ast}A^{\ast}$. \medskip

\textbf{(d)} If $A\in\mathbb{C}^{n\times m}$, then $\left(  A^{\ast}\right)
^{\ast}=A$.
\end{proposition}

\subsection{Isometries}

\begin{definition}
\label{def.unitary.innerprod.isometry}An $n\times k$-matrix $A$ is said to be
an \emph{isometry} if $A^{\ast}A=I_{k}$.
\end{definition}

\begin{proposition}
\label{prop.unitary.innerprod.isometry.2}An $n\times k$-matrix $A$ is an
isometry if and only if its columns form an orthonormal tuple of vectors.
\end{proposition}

\begin{proof}
Let $A$ be an $n\times k$-matrix with columns $a_{1},a_{2},\ldots,a_{k}$ from
left to right. Therefore,%
\[
A=\left(
\begin{array}
[c]{ccc}%
\mid &  & \mid\\
a_{1} & \cdots & a_{k}\\
\mid &  & \mid
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and thus}\ \ \ \ \ \ \ \ \ \ A^{\ast
}=\left(
\begin{array}
[c]{ccc}%
\text{---} & a_{1}^{\ast} & \text{---}\\
& \vdots & \\
\text{---} & a_{k}^{\ast} & \text{---}%
\end{array}
\right)  .
\]
Hence,%
\begin{align*}
A^{\ast}A  &  =\left(
\begin{array}
[c]{cccc}%
a_{1}^{\ast}a_{1} & a_{1}^{\ast}a_{2} & \cdots & a_{1}^{\ast}a_{k}\\
a_{2}^{\ast}a_{1} & a_{2}^{\ast}a_{2} & \cdots & a_{2}^{\ast}a_{k}\\
\vdots & \vdots & \ddots & \vdots\\
a_{k}^{\ast}a_{1} & a_{k}^{\ast}a_{2} & \cdots & a_{k}^{\ast}a_{k}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
\left\vert \left\vert a_{1}\right\vert \right\vert ^{2} & \left\langle
a_{1},a_{2}\right\rangle  & \cdots & \left\langle a_{1},a_{k}\right\rangle \\
\left\langle a_{2},a_{1}\right\rangle  & \left\vert \left\vert a_{2}%
\right\vert \right\vert ^{2} & \cdots & \left\langle a_{2},a_{k}\right\rangle
\\
\vdots & \vdots & \ddots & \vdots\\
\left\langle a_{k},a_{1}\right\rangle  & \left\langle a_{k},a_{2}\right\rangle
& \cdots & \left\vert \left\vert a_{k}\right\vert \right\vert ^{2}%
\end{array}
\right)  .
\end{align*}
On the other hand,%
\[
I_{k}=\left(
\begin{array}
[c]{cccc}%
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  .
\]
Thus, $A^{\ast}A=I_{k}$ holds if and only if we have%
\[
\left\langle a_{p},a_{q}\right\rangle =0\ \ \ \ \ \ \ \ \ \ \text{for all
}p\neq q
\]
and%
\[
\left\vert \left\vert a_{p}\right\vert \right\vert ^{2}%
=1\ \ \ \ \ \ \ \ \ \ \text{for each }p.
\]
In other words, $A^{\ast}A=I_{k}$ holds if and only if we have%
\[
a_{p}\perp a_{q}\ \ \ \ \ \ \ \ \ \ \text{for all }p\neq q
\]
and%
\[
\left\vert \left\vert a_{1}\right\vert \right\vert =\left\vert \left\vert
a_{2}\right\vert \right\vert =\cdots=\left\vert \left\vert a_{k}\right\vert
\right\vert =1.
\]
In other words, $A$ is an isometry if and only if $\left(  a_{1},a_{2}%
,\ldots,a_{k}\right)  $ is orthonormal. This proves Proposition
\ref{prop.unitary.innerprod.isometry.2}.
\end{proof}

Isometries are called isometries because they preserve lengths:

\begin{proposition}
\label{prop.unitary.innerprod.isometry.len}Let $A\in\mathbb{C}^{n\times k}$ be
an isometry. Then, each $x\in\mathbb{C}^{k}$ satisfies $\left\vert \left\vert
Ax\right\vert \right\vert =\left\vert \left\vert x\right\vert \right\vert $.
\end{proposition}

\begin{proof}
We have $A^{\ast}A=I_{k}$ (since $A$ is an isometry). Let $x\in\mathbb{C}^{k}%
$. Then, the definition of $\left\vert \left\vert Ax\right\vert \right\vert $
yields $\left\vert \left\vert Ax\right\vert \right\vert =\sqrt{\left\langle
Ax,Ax\right\rangle }$. Hence,%
\begin{align}
\left\vert \left\vert Ax\right\vert \right\vert ^{2}  &  =\left\langle
Ax,Ax\right\rangle \nonumber\\
&  =\underbrace{\left(  Ax\right)  ^{\ast}}_{\substack{=x^{\ast}A^{\ast
}\\\text{(by Proposition \ref{prop.unitary.(AB)*} \textbf{(c)})}%
}}Ax\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(a)}}\right) \nonumber\\
&  =x^{\ast}A^{\ast}Ax\label{pf.prop.unitary.innerprod.isometry.len.1}\\
&  =x^{\ast}x\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A^{\ast}A=I_{k}\right)
\nonumber\\
&  =\left\langle x,x\right\rangle \ \ \ \ \ \ \ \ \ \ \left(  \text{by
Proposition \ref{prop.unitary.innerprod.props} \textbf{(a)}}\right)
\nonumber\\
&  =\left\vert \left\vert x\right\vert \right\vert ^{2}\nonumber
\end{align}
(since the definition of $\left\vert \left\vert x\right\vert \right\vert $
yields $\left\vert \left\vert x\right\vert \right\vert =\sqrt{\left\langle
x,x\right\rangle }$). In other words, we have $\left\vert \left\vert
Ax\right\vert \right\vert =\left\vert \left\vert x\right\vert \right\vert $
(since $\left\vert \left\vert Ax\right\vert \right\vert $ and $\left\vert
\left\vert x\right\vert \right\vert $ are nonnegative reals). This proves
Proposition \ref{prop.unitary.innerprod.isometry.len}.
\end{proof}

\begin{remark}
Another warning on terminology: Some authors (e.g., Conrad in
\cite[\textquotedblleft Isometries\textquotedblright]{Conrad}) use the word
\textquotedblleft isometry\textquotedblright\ in a wider sense than we do.
Namely, they use it for arbitrary maps from $\mathbb{C}^{k}$ to $\mathbb{C}%
^{n}$ that preserve distances. Our isometries can be viewed as \textbf{linear}
isometries in this wider sense, because a matrix $A\in\mathbb{C}^{n\times k}$
corresponds to a linear map from $\mathbb{C}^{k}$ to $\mathbb{C}^{n}$.
However, not all isometries in this wider sense are linear.
\end{remark}

\subsection{Unitary matrices}

\subsubsection{Definition, examples, basic properties}

\begin{definition}
\label{def.unitary.unitary.unitary}A matrix $U\in\mathbb{C}^{n\times k}$ is
said to be \emph{unitary} if and only if both $U$ and $U^{\ast}$ are isometries.
\end{definition}

%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 2 starts here.}\\\hline\hline
\end{tabular}
\]


\begin{example}
\label{exa.unitary.unitary.exas}\textbf{(a)} The matrix $A=\dfrac{1}{\sqrt{2}%
}\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & -1
\end{array}
\right)  $ is unitary. Indeed, it is easy to see that $A^{\ast}A=I_{2}$, so
that $A$ is an isometry. Thus, $A^{\ast}$ is an isometry as well, since
$A^{\ast}=A$. Hence, $A$ is unitary. \medskip

\textbf{(b)} A $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
\lambda
\end{array}
\right)  \in\mathbb{C}^{1\times1}$ is unitary if and only if $\left\vert
\lambda\right\vert =1$. \medskip

\textbf{(c)} For any $n\in\mathbb{N}$, the identity matrix $I_{n}$ is unitary.
\medskip

\textbf{(d)} Let $n\in\mathbb{N}$, and let $\sigma$ be a permutation of
$\left[  n\right]  $ (that is, a bijective map from $\left[  n\right]  $ to
$\left[  n\right]  $). Let $P_{\sigma}$ be the \emph{permutation matrix} of
$\sigma$; this is the $n\times n$-matrix whose $\left(  \sigma\left(
j\right)  ,j\right)  $-th entry is $1$ for each $j\in\left[  n\right]  $, and
whose all other entries are $0$. For instance, if $n=3$ and if $\sigma$ is the
cyclic permutation sending $1,2,3$ to $2,3,1$ (respectively), then%
\[
P_{\sigma}=\left(
\begin{array}
[c]{ccc}%
0 & 0 & 1\\
1 & 0 & 0\\
0 & 1 & 0
\end{array}
\right)  .
\]


The permutation matrix $P_{\sigma}$ is always unitary (for any $n$ and any
permutation $\sigma$). Indeed, its conjugate transpose $\left(  P_{\sigma
}\right)  ^{\ast}$ is easily seen to be the permutation matrix $P_{\sigma
^{-1}}$ of the inverse permutation $\sigma^{-1}$; but this latter permutation
matrix $P_{\sigma^{-1}}$ is also the inverse of $P_{\sigma}$. \medskip

\textbf{(e)} A diagonal matrix $\operatorname*{diag}\left(  \lambda
_{1},\lambda_{2},\ldots,\lambda_{n}\right)  \in\mathbb{C}^{n\times n}$ is
unitary if and only if its diagonal entries $\lambda_{1},\lambda_{2}%
,\ldots,\lambda_{n}$ lie on the unit circle (i.e., their absolute values
$\left\vert \lambda_{1}\right\vert ,\left\vert \lambda_{2}\right\vert
,\ldots,\left\vert \lambda_{n}\right\vert $ all equal $1$).
\end{example}

Unitary matrices can be characterized in many other ways:

\begin{theorem}
\label{thm.unitary.unitary.eqs}Let $U\in\mathbb{C}^{n\times k}$ be a matrix.
The following six statements are equivalent:

\begin{itemize}
\item $\mathcal{A}$: The matrix $U$ is unitary.

\item $\mathcal{B}$: The matrices $U$ and $U^{\ast}$ are isometries.

\item $\mathcal{C}$: We have $UU^{\ast}=I_{n}$ and $U^{\ast}U=I_{k}$.

\item $\mathcal{D}$: The matrix $U$ is square (that is, $n=k$) and invertible
and satisfies $U^{-1}=U^{\ast}$.

\item $\mathcal{E}$: The columns of $U$ form an orthonormal basis of
$\mathbb{C}^{n}$.

\item $\mathcal{F}$: The matrix $U$ is square (that is, $n=k$) and is an isometry.
\end{itemize}
\end{theorem}

\begin{proof}
The equivalence $\mathcal{A}\Longleftrightarrow\mathcal{B}$ follows
immediately from Definition \ref{def.unitary.unitary.unitary}. The equivalence
$\mathcal{B}\Longleftrightarrow\mathcal{C}$ follows immediately from the
definition of an isometry (since $\left(  U^{\ast}\right)  ^{\ast}=U$). The
implication $\mathcal{D}\Longrightarrow\mathcal{C}$ is obvious. The
implication $\mathcal{C}\Longrightarrow\mathcal{D}$ follows from the known
fact (see, e.g., \cite[Chapter 2, Corollary 3.7]{Treil15}) that every
invertible matrix is square. Let us now prove some of the other implications:

\begin{itemize}
\item $\mathcal{D}\Longrightarrow\mathcal{E}$\textit{:} Assume that statement
$\mathcal{D}$ holds. Then, $U^{\ast}U=I_{k}$ (since $U^{-1}=U^{\ast}$), and
therefore $U$ is an isometry. Hence, Proposition
\ref{prop.unitary.innerprod.isometry.2} shows that the tuple of columns of $U$
is orthonormal. However, the columns of $U$ form a basis of $\mathbb{C}^{n}$
(because $U$ is invertible), and this basis is orthonormal (since we have just
shown that the tuple of columns of $U$ is orthonormal). Thus, statement
$\mathcal{E}$ holds. We have thus proved the implication $\mathcal{D}%
\Longrightarrow\mathcal{E}$.

\item $\mathcal{E}\Longrightarrow\mathcal{D}$\textit{:} Assume that statement
$\mathcal{E}$ holds. Then, the columns of $U$ form an orthonormal basis, hence
an orthonormal tuple. Thus, Proposition
\ref{prop.unitary.innerprod.isometry.2} shows that $U$ is an isometry, so that
$U^{\ast}U=I_{k}$. However, $U$ is invertible because the columns of $U$ form
a basis of $\mathbb{C}^{n}$. Therefore, from $U^{\ast}U=I_{k}$, we obtain
$U^{-1}=U^{\ast}$. Finally, the matrix $U$ is square, since any invertible
matrix is square. Thus, statement $\mathcal{D}$ holds. We have thus proved the
implication $\mathcal{E}\Longrightarrow\mathcal{D}$.

\item $\mathcal{D}\Longrightarrow\mathcal{F}$\textit{:} The implication
$\mathcal{D}\Longrightarrow\mathcal{F}$ is easy (since $U^{-1}=U^{\ast}$
entails $U^{\ast}U=I_{k}$, which shows that $U$ is an isometry).

\item $\mathcal{F}\Longrightarrow\mathcal{D}$\textit{:} Assume that statement
$\mathcal{F}$ holds. Thus, $U$ is an isometry; that is, we have $U^{\ast
}U=I_{k}=I_{n}$ (since $k=n$). However, it is known\footnote{This is one part
of the infamous \textquotedblleft inverse matrix theorem\textquotedblright%
\ that lists many equivalent conditions for invertibility. See, for example,
\cite[Chapter 2, Proposition 3.8]{Treil15}.} that a square matrix $A$ that has
a left inverse (i.e., a further square matrix $B$ satisfying $BA=I$) must be
invertible. We can apply this to the square matrix $U$ (which has a left
inverse, since $U^{\ast}U=I_{n}$), and thus conclude that $U$ is invertible.
Hence, from $U^{\ast}U=I_{n}$, we obtain $U^{-1}=U^{\ast}$. Therefore,
statement $\mathcal{D}$ holds. We have thus proved the implication
$\mathcal{F}\Longrightarrow\mathcal{D}$.
\end{itemize}

Altogether, we have thus proved that all six statements $\mathcal{A}%
,\mathcal{B},\mathcal{C},\mathcal{D},\mathcal{E},\mathcal{F}$ are equivalent.
\end{proof}

Note that Theorem \ref{thm.unitary.unitary.eqs} (specifically, the implication
$\mathcal{A}\Longrightarrow\mathcal{D}$) shows that any unitary matrix is
square. In contrast, an isometry can be rectangular -- but only tall, not
wide, as the following exercise shows:

\begin{exercise}
\label{exe.unitary.isometry-tall}\fbox{1} Let $A\in\mathbb{C}^{n\times k}$ be
an isometry. Show that $n\geq k$.
\end{exercise}

\begin{exercise}
\label{exe.unitary.group}\fbox{3} \textbf{(a)} Prove that the product $AB$ of
two isometries $A\in\mathbb{C}^{n\times m}$ and $B\in\mathbb{C}^{m\times k}$
is always an isometry. \medskip

\textbf{(b)} Prove that the product $AB$ of two unitary matrices
$A\in\mathbb{C}^{n\times n}$ and $B\in\mathbb{C}^{n\times n}$ is always
unitary. \medskip

\textbf{(c)} Prove that the inverse of a unitary matrix $A\in\mathbb{C}%
^{n\times n}$ is always unitary.
\end{exercise}

Exercise \ref{exe.unitary.group} shows that the set of all unitary $n\times
n$-matrices over $\mathbb{C}$ (for a given $n\in\mathbb{N}$) is a group under
multiplication. This group is known as the $n$\emph{-th unitary group}, and is
denoted by $\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)  $.

\begin{exercise}
\label{exe.unitary.det-eval}\fbox{2} Let $U\in\mathbb{C}^{n\times n}$ be a
unitary matrix. \medskip

\textbf{(a)} Prove that $\left\vert \det U\right\vert =1$. \medskip

\textbf{(b)} Prove that any eigenvalue $\lambda$ of $U$ satisfies $\left\vert
\lambda\right\vert =1$.
\end{exercise}

\subsubsection{Various constructions of unitary matrices}

The next two exercises show some ways to generate unitary matrices:

\begin{exercise}
\label{exe.unitary.house}\fbox{3} Let $w\in\mathbb{C}^{n}$ be a nonzero
vector. Then, $w^{\ast}w=\left\langle w,w\right\rangle >0$ (by Proposition
\ref{prop.unitary.innerprod.pos} \textbf{(b)}). Thus, we can define an
$n\times n$-matrix
\[
U_{w}:=I_{n}-2\left(  w^{\ast}w\right)  ^{-1}ww^{\ast}\in\mathbb{C}^{n\times
n}.
\]
This is called a \emph{Householder matrix}.

Show that this matrix $U_{w}$ is unitary and satisfies $U_{w}^{\ast}=U_{w}$.
\end{exercise}

The next exercise uses the notion of a skew-Hermitian matrix:

\begin{definition}
\label{def.unitary.skew-herm}A matrix $S\in\mathbb{C}^{n\times n}$ is said to
be \emph{skew-Hermitian} if and only if $S^{\ast}=-S$.
\end{definition}

For instance, the matrix $\left(
\begin{array}
[c]{cc}%
i & 1\\
-1 & 0
\end{array}
\right)  $ is skew-Hermitian.

\begin{exercise}
\fbox{5} \label{exe.unitary.skew-herm.1}Let $S\in\mathbb{C}^{n\times n}$ be a
skew-Hermitian matrix. \medskip

\textbf{(a)} Prove that the matrix $I_{n}-S$ is invertible. \medskip

[\textbf{Hint:} Show first that the matrix $I_{n}+S^{\ast}S$ is invertible,
since each nonzero vector $v\in\mathbb{C}^{n}$ satisfies $v^{\ast}\left(
I_{n}+S^{\ast}S\right)  v=\underbrace{\left\langle v,v\right\rangle }%
_{>0}+\underbrace{\left\langle Sv,Sv\right\rangle }_{\geq0}>0$. Then, expand
the product $\left(  I_{n}-S^{\ast}\right)  \left(  I_{n}-S\right)  $.]
\medskip

\textbf{(b)} Prove that the matrices $I_{n}+S$ and $\left(  I_{n}-S\right)
^{-1}$ commute (i.e., satisfy $\left(  I_{n}+S\right)  \cdot\left(
I_{n}-S\right)  ^{-1}=\left(  I_{n}-S\right)  ^{-1}\cdot\left(  I_{n}%
+S\right)  $). \medskip

\textbf{(c)} Prove that the matrix $U:=\left(  I_{n}-S\right)  ^{-1}%
\cdot\left(  I_{n}+S\right)  $ is unitary. \medskip

\textbf{(d)} Prove that the matrix $U+I_{n}$ is invertible. \medskip

\textbf{(e)} Prove that $S=\left(  U-I_{n}\right)  \cdot\left(  U+I_{n}%
\right)  ^{-1}$.
\end{exercise}

Exercise \ref{exe.unitary.skew-herm.1} constructs a map\footnote{Recall that
$\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)  $ denotes the $n$-th
unitary group (i.e., the set of all unitary $n\times n$-matrices).}%
\begin{align*}
\left\{  \text{skew-Hermitian matrices in }\mathbb{C}^{n\times n}\right\}   &
\rightarrow\left\{  U\in\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}%
\right)  \text{ }\mid\ U+I_{n}\text{ is invertible}\right\}  ,\\
S  &  \mapsto\left(  I_{n}-S\right)  ^{-1}\cdot\left(  I_{n}+S\right)  .
\end{align*}
This map is known as the \emph{Cayley parametrization} of the unitary matrices
(and can be seen as an $n$-dimensional generalization of the stereographic
projection from the imaginary axis to the unit circle -- which is what it does
for $n=1$). Exercise \ref{exe.unitary.skew-herm.1} \textbf{(e)} shows that it
is injective. It is not hard to check that it is surjective, too.

How close is the set $\left\{  U\in\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  \text{ }\mid\ U+I_{n}\text{ is invertible}\right\}  $ to
the whole unitary group $\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}%
\right)  $ ? The answer is that it is almost the entire group
$\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)  $. Here is a
rigorous way to state this:

\begin{exercise}
\label{exe.unitary.skew-herm.2}\fbox{3} Let $A\in\mathbb{C}^{n\times n}$ be a
matrix. Prove the following: \medskip

\textbf{(a)} If $A$ is unitary, then the matrix $\lambda A$ is unitary for
each $\lambda\in\mathbb{C}$ satisfying $\left\vert \lambda\right\vert =1$.
\medskip

\textbf{(b)} The matrix $\lambda A+I_{n}$ is invertible for all but finitely
many $\lambda\in\mathbb{C}$. \medskip

[\textbf{Hint:} The determinant $\det\left(  \lambda A+I_{n}\right)  $ is a
polynomial function in $\lambda$.] \medskip

\textbf{(c)} The set $\left\{  U\in\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  \text{ }\mid\ U+I_{n}\text{ is invertible}\right\}  $ is
dense in $\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)  $. (That
is, each unitary matrix in $\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  $ can be written as a limit $\lim\limits_{k\rightarrow
\infty}U_{k}$ of a sequence of unitary matrices $U_{k}$ such that $U_{k}%
+I_{n}$ is invertible for each $k$.)
\end{exercise}

Thus, if the Cayley parametrization does not hit a unitary matrix, then at
least it comes arbitrarily close.

\begin{remark}
A square matrix $A\in\mathbb{C}^{n\times n}$ satisfying $AA^{T}=A^{T}A=I_{n}$
is called \emph{orthogonal}. Thus, unitary matrices differ from orthogonal
matrices only in the use of the conjugate transpose $A^{\ast}$ instead of the
transpose $A^{T}$. In particular, a matrix $A\in\mathbb{R}^{n\times n}$ (with
real entries) is orthogonal if and only if it is unitary.
\end{remark}

\begin{exercise}
\label{exe.unitary.skew-herm.pyth}\fbox{5} A \emph{Pythagorean triple} is a
triple $\left(  p,q,r\right)  $ of positive integers satisfying $p^{2}%
+q^{2}=r^{2}$. (In other words, it is a triple of positive integers that are
the sides of a right-angled triangle.) Two famous Pythagorean triples are
$\left(  3,4,5\right)  $ and $\left(  5,12,13\right)  $. \medskip

\textbf{(a)} Prove that a triple $\left(  p,q,r\right)  $ of positive integers
is Pythagorean if and only if the matrix $\left(
\begin{array}
[c]{cc}%
p/r & -q/r\\
q/r & p/r
\end{array}
\right)  $ is unitary. \medskip

\textbf{(b)} Let $\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $ be any unitary matrix with rational entries. Assume that $a$ and
$c$ are positive, and write $a$ and $c$ as $p/r$ and $q/r$ for some positive
integers $p,q,r$. Show that $\left(  p,q,r\right)  $ is a Pythagorean triple.
\medskip

\textbf{(c)} Find infinitely many Pythagorean triples that are pairwise
non-proportional (i.e., no two of them are obtained from one another just by
multiplying all three entries by the same number.) \medskip

[\textbf{Hint:} Use the $S\mapsto U$ construction from Exercise
\ref{exe.unitary.skew-herm.1}.]
\end{exercise}

We shall soon see one more way to construct unitary matrices from smaller
ones, using the notion of block matrices, which we shall now introduce.

\subsection{Block matrices}

\subsubsection{Definition}

\begin{definition}
\label{def.blockmats.2x2}Let $\mathbb{F}$ be a field. Let $n,m,p,q\in
\mathbb{N}$. Let $A\in\mathbb{F}^{n\times p}$, $B\in\mathbb{F}^{n\times q}$,
$C\in\mathbb{F}^{m\times p}$ and $D\in\mathbb{F}^{m\times q}$ be four
matrices. Then, $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ shall denote the $\left(  n+m\right)  \times\left(  p+q\right)
$-matrix obtained by \textquotedblleft gluing\textquotedblright\ the four
matrices $A,B,C,D$ together in the manner suggested by the notation (i.e., we
glue $B$ to the right edge of $A$, we glue $C$ to the bottom edge of $A$, and
we glue $D$ to the right edge of $C$ and to the bottom edge of $B$). In other
words, we set%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  :=\left(
\begin{array}
[c]{cccccccc}%
A_{1,1} & A_{1,2} & \cdots & A_{1,p} & B_{1,1} & B_{1,2} & \cdots & B_{1,q}\\
A_{2,1} & A_{2,2} & \cdots & A_{2,p} & B_{2,1} & B_{2,2} & \cdots & B_{2,q}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
A_{n,1} & A_{n,2} & \cdots & A_{n,p} & B_{n,1} & B_{n,2} & \cdots & B_{n,q}\\
C_{1,1} & C_{1,2} & \cdots & C_{1,p} & D_{1,1} & D_{1,2} & \cdots & D_{1,q}\\
C_{2,1} & C_{2,2} & \cdots & C_{2,p} & D_{2,1} & D_{2,2} & \cdots & D_{2,q}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
C_{m,1} & C_{m,2} & \cdots & C_{m,p} & D_{m,1} & D_{m,2} & \cdots & D_{m,q}%
\end{array}
\right)
\]
(where, as we recall, the notation $M_{i,j}$ denotes the $\left(  i,j\right)
$-th entry of a matrix $M$).
\end{definition}

\begin{example}
If $A=\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
a^{\prime\prime} & a^{\prime\prime\prime}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{c}%
b\\
b^{\prime}%
\end{array}
\right)  $ and $C=\left(
\begin{array}
[c]{cc}%
c & c^{\prime}%
\end{array}
\right)  $ and $D=\left(
\begin{array}
[c]{c}%
d
\end{array}
\right)  $, then $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & b\\
a^{\prime\prime} & a^{\prime\prime\prime} & b^{\prime}\\
c & c^{\prime} & d
\end{array}
\right)  $.
\end{example}

The notation introduced in Definition \ref{def.blockmats.2x2} is called
\emph{block matrix notation}, and can be generalized to more than four matrices:

\begin{definition}
\label{def.blockmatrix.uxv}Let $\mathbb{F}$ be a field. Let $u,v\in\mathbb{N}%
$. Let $n_{1},n_{2},\ldots,n_{u}\in\mathbb{N}$ and $p_{1},p_{2},\ldots
,p_{v}\in\mathbb{N}$. For each $i\in\left[  u\right]  $ and $j\in\left[
v\right]  $, let $A\left(  i,j\right)  \in\mathbb{F}^{n_{i}\times p_{j}}$ be a
matrix. (We denote it by $A\left(  i,j\right)  $ instead of $A_{i,j}$ to avoid
mistaking it for a single entry.) Then,%
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  & A\left(  1,2\right)  & \cdots & A\left(  1,v\right) \\
A\left(  2,1\right)  & A\left(  2,2\right)  & \cdots & A\left(  2,v\right) \\
\vdots & \vdots & \ddots & \vdots\\
A\left(  u,1\right)  & A\left(  u,2\right)  & \cdots & A\left(  u,v\right)
\end{array}
\right)  \label{eq.def.blockmatrix.uxv.blockmat}%
\end{equation}
shall denote the $\left(  n_{1}+n_{2}+\cdots+n_{u}\right)  \times\left(
p_{1}+p_{2}+\cdots+p_{v}\right)  $-matrix obtained by \textquotedblleft
gluing\textquotedblright\ the matrices $A\left(  i,j\right)  $ together in the
manner suggested by the notation. In other words,%
\[
\left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  & A\left(  1,2\right)  & \cdots & A\left(  1,v\right) \\
A\left(  2,1\right)  & A\left(  2,2\right)  & \cdots & A\left(  2,v\right) \\
\vdots & \vdots & \ddots & \vdots\\
A\left(  u,1\right)  & A\left(  u,2\right)  & \cdots & A\left(  u,v\right)
\end{array}
\right)
\]
shall denote the $\left(  n_{1}+n_{2}+\cdots+n_{u}\right)  \times\left(
p_{1}+p_{2}+\cdots+p_{v}\right)  $-matrix whose $\left(  n_{1}+n_{2}%
+\cdots+n_{i-1}+k,\ \ p_{1}+p_{2}+\cdots+p_{j-1}+\ell\right)  $-th entry is
$\left(  A\left(  i,j\right)  \right)  _{k,\ell}$ for all $i\in\left[
u\right]  $ and $j\in\left[  v\right]  $ and $k\in\left[  n_{i}\right]  $ and
$\ell\in\left[  p_{j}\right]  $.
\end{definition}

Alternatively, this matrix can be defined abstractly using direct sums of
vector spaces; see \cite[Chapter II, \S 10, section 2]{Bourba74} for this definition.

\begin{example}
Let $0_{2\times2}$ denote the zero matrix of size $2\times2$. Then,%
\[
\left(
\begin{array}
[c]{ccc}%
0_{2\times2} & I_{2} & 0_{2\times2}\\
I_{2} & 0_{2\times2} & 0_{2\times2}\\
0_{2\times2} & -I_{2} & I_{2}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0\\
1 & 0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & -1 & 0 & 1 & 0\\
0 & 0 & 0 & -1 & 0 & 1
\end{array}
\right)  .
\]

\end{example}

In Definition \ref{def.blockmatrix.uxv}, the big matrix
(\ref{eq.def.blockmatrix.uxv.blockmat}) is called the \emph{block matrix}
formed out of the matrices $A\left(  i,j\right)  $; the single matrices
$A\left(  i,j\right)  $ are called its \emph{blocks}.

\subsubsection{Multiplying block matrices}

One of the most useful properties of block matrices is that they can be
multiplied \textquotedblleft as if the blocks were numbers\textquotedblright%
\ (i.e., by the same formula as for regular matrices), provided that the
products make sense. Let us state this more precisely -- first for the case of
four blocks:

\begin{proposition}
\label{prop.blockmatrix.mult-2x2}Let $\mathbb{F}$ be a field. Let $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $\ell$ and $\ell^{\prime}$ be six nonnegative
integers. Let $A\in\mathbb{F}^{n\times m}$, $B\in\mathbb{F}^{n\times
m^{\prime}}$, $C\in\mathbb{F}^{n^{\prime}\times m}$, $D\in\mathbb{F}%
^{n^{\prime}\times m^{\prime}}$, $A^{\prime}\in\mathbb{F}^{m\times\ell}$,
$B^{\prime}\in\mathbb{F}^{m\times\ell^{\prime}}$, $C^{\prime}\in
\mathbb{F}^{m^{\prime}\times\ell}$ and $D^{\prime}\in\mathbb{F}^{m^{\prime
}\times\ell^{\prime}}$. Then,
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  .
\]

\end{proposition}

For comparison, here is the formula for the product of two $2\times2$-matrices
(consisting of numbers, not blocks):%
\[
\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
a^{\prime} & b^{\prime}\\
c^{\prime} & d^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
aa^{\prime}+bc^{\prime} & ab^{\prime}+bd^{\prime}\\
ca^{\prime}+dc^{\prime} & cb^{\prime}+dd^{\prime}%
\end{array}
\right)
\]
(for any $a,b,c,d,a^{\prime},b^{\prime},c^{\prime},d^{\prime}\in\mathbb{F}$).
Thus, Proposition \ref{prop.blockmatrix.mult-2x2} is saying that the same
formula can be used to multiply block matrices made of appropriately sized
blocks. Thus, roughly speaking, we can multiply block matrices
\textquotedblleft as if the blocks were numbers\textquotedblright. To be fully
honest, two caveats apply here:

\begin{itemize}
\item In the formula for $\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
a^{\prime} & b^{\prime}\\
c^{\prime} & d^{\prime}%
\end{array}
\right)  $, we can write the right hand side in many different ways: e.g., we
can replace $aa^{\prime}$ by $a^{\prime}a$, because multiplication of numbers
is commutative. In contrast, multiplication of matrices is not commutative, so
that we cannot replace $AA^{\prime}$ by $A^{\prime}A$ in Proposition
\ref{prop.blockmatrix.mult-2x2}. Thus, we can multiply block matrices
\textquotedblleft as if the blocks were numbers\textquotedblright, but we have
to keep the blocks in the correct order (viz., in the order in which they
appear on the left hand side).

\item We cannot use Proposition \ref{prop.blockmatrix.mult-2x2} to multiply
two arbitrary block matrices; indeed, Proposition
\ref{prop.blockmatrix.mult-2x2} requires the blocks to have \textquotedblleft
matching\textquotedblright\ dimensions. For example, $A$ must have as many
columns as $A^{\prime}$ has rows (this is enforced by the assumptions
$A\in\mathbb{F}^{n\times m}$ and $A^{\prime}\in\mathbb{F}^{m\times\ell}$). If
this wasn't the case, then the product $AA^{\prime}$ on the right hand side
wouldn't even make sense!
\end{itemize}

\begin{proof}
[Proof of Proposition \ref{prop.blockmatrix.mult-2x2}.]Just check that each
entry on the left hand side equals the corresponding entry on the right. This
is a straightforward computation that is made painful by the notational load
and the need to distinguish between four cases (depending on which block our
entry lies in). Do one of the four cases to convince yourself that there is
nothing difficult here. (See \cite{detnotes} for all the gory details.)
\end{proof}

Unsurprisingly, Proposition \ref{prop.blockmatrix.mult-2x2} generalizes to the
multi-block case:

\begin{proposition}
\label{prop.blockmatrix.mult-uxv}Let $\mathbb{F}$ be a field. Let
$u,v,w\in\mathbb{N}$. Let $n_{1},n_{2},\ldots,n_{u}\in\mathbb{N}$ and
$p_{1},p_{2},\ldots,p_{v}\in\mathbb{N}$ and $q_{1},q_{2},\ldots,q_{w}%
\in\mathbb{N}$. For each $i\in\left[  u\right]  $ and $j\in\left[  v\right]
$, let $A\left(  i,j\right)  \in\mathbb{F}^{n_{i}\times p_{j}}$ be a matrix.
For each $j\in\left[  v\right]  $ and $k\in\left[  w\right]  $, let $B\left(
j,k\right)  \in\mathbb{F}^{p_{j}\times q_{k}}$ be a matrix. Then,%
\begin{align*}
&  \left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  & A\left(  1,2\right)  & \cdots & A\left(  1,v\right) \\
A\left(  2,1\right)  & A\left(  2,2\right)  & \cdots & A\left(  2,v\right) \\
\vdots & \vdots & \ddots & \vdots\\
A\left(  u,1\right)  & A\left(  u,2\right)  & \cdots & A\left(  u,v\right)
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,w\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,w\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  v,1\right)  & B\left(  v,2\right)  & \cdots & B\left(  v,w\right)
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
\sum_{j=1}^{v}A\left(  1,j\right)  B\left(  j,1\right)  & \sum_{j=1}%
^{v}A\left(  1,j\right)  B\left(  j,2\right)  & \cdots & \sum_{j=1}%
^{v}A\left(  1,j\right)  B\left(  j,w\right) \\
\sum_{j=1}^{v}A\left(  2,j\right)  B\left(  j,1\right)  & \sum_{j=1}%
^{v}A\left(  2,j\right)  B\left(  j,2\right)  & \cdots & \sum_{j=1}%
^{v}A\left(  2,j\right)  B\left(  j,w\right) \\
\vdots & \vdots & \ddots & \vdots\\
\sum_{j=1}^{v}A\left(  u,j\right)  B\left(  j,1\right)  & \sum_{j=1}%
^{v}A\left(  u,j\right)  B\left(  j,2\right)  & \cdots & \sum_{j=1}%
^{v}A\left(  u,j\right)  B\left(  j,w\right)
\end{array}
\right)  .
\end{align*}

\end{proposition}

\begin{proof}
Just like Proposition \ref{prop.blockmatrix.mult-2x2}, but with more indices.
In short, fun!
\end{proof}

\subsubsection{Block-diagonal matrices}

\begin{definition}
\emph{Block-diagonal matrices} are block matrices of the form
(\ref{eq.def.blockmatrix.uxv.blockmat}), where

\begin{itemize}
\item we have $u=v$,

\item all matrices $A\left(  i,i\right)  $ are square (i.e., we have
$n_{i}=p_{i}$ for all $i\in\left[  u\right]  $), and

\item all $A\left(  i,j\right)  $ with $i\neq j$ are zero matrices.
\end{itemize}

In other words, block-diagonal matrices are block matrices of the form%
\[
\left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  & 0 & \cdots & 0\\
0 & A\left(  2,2\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A\left(  u,u\right)
\end{array}
\right)  ,
\]
where $A\left(  1,1\right)  ,A\left(  2,2\right)  ,\ldots,A\left(  u,u\right)
$ are arbitrary square matrices, and where each \textquotedblleft%
$0$\textquotedblright\ means a zero matrix of appropriate dimensions.
\end{definition}

As an easy consequence of Proposition \ref{prop.blockmatrix.mult-uxv}, we
obtain a multiplication rule for block-diagonal matrices that looks exactly
like multiplication of usual diagonal matrices:

\begin{corollary}
\label{cor.blockmatrix.mult-diag}Let $u\in\mathbb{N}$. Let $n_{1},n_{2}%
,\ldots,n_{u}\in\mathbb{N}$. For each $i\in\left[  u\right]  $, let $A\left(
i,i\right)  $ and $B\left(  i,i\right)  $ be two $n_{i}\times n_{i}$-matrices.
Then,%
\begin{align*}
&  \left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  & 0 & \cdots & 0\\
0 & A\left(  2,2\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A\left(  u,u\right)
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & 0 & \cdots & 0\\
0 & B\left(  2,2\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & B\left(  u,u\right)
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  B\left(  1,1\right)  & 0 & \cdots & 0\\
0 & A\left(  2,2\right)  B\left(  2,2\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A\left(  u,u\right)  B\left(  u,u\right)
\end{array}
\right)  .
\end{align*}
(Here, each \textquotedblleft$0$\textquotedblright\ means a zero matrix of
appropriate dimensions.)
\end{corollary}

\begin{example}
Let $u=2$ and $n_{1}=1$ and $n_{2}=2$. Let $A\left(  1,1\right)  =\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)  $ and $A\left(  2,2\right)  =\left(
\begin{array}
[c]{cc}%
b & c\\
d & e
\end{array}
\right)  $ and $B\left(  1,1\right)  =\left(
\begin{array}
[c]{c}%
a^{\prime}%
\end{array}
\right)  $ and $B\left(  2,2\right)  =\left(
\begin{array}
[c]{cc}%
b^{\prime} & c^{\prime}\\
d^{\prime} & e^{\prime}%
\end{array}
\right)  $. Then, Corollary \ref{cor.blockmatrix.mult-diag} says that%
\[
\left(
\begin{array}
[c]{cc}%
A\left(  1,1\right)  & 0\\
0 & A\left(  2,2\right)
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
B\left(  1,1\right)  & 0\\
0 & B\left(  2,2\right)
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A\left(  1,1\right)  B\left(  1,1\right)  & 0\\
0 & A\left(  2,2\right)  B\left(  2,2\right)
\end{array}
\right)  ,
\]
i.e., that%
\[
\left(
\begin{array}
[c]{ccc}%
a & 0 & 0\\
0 & b & c\\
0 & d & e
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & 0 & 0\\
0 & b^{\prime} & c^{\prime}\\
0 & d^{\prime} & e^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime} & 0 & 0\\
0 & bb^{\prime}+cd^{\prime} & bc^{\prime}+ce^{\prime}\\
0 & db^{\prime}+ed^{\prime} & dc^{\prime}+ee^{\prime}%
\end{array}
\right)  .
\]

\end{example}

Corollary \ref{cor.blockmatrix.mult-diag} can be stated (somewhat imprecisely)
as follows: To multiply two block-diagonal matrices, we just multiply
respective blocks with each other. The same applies to addition instead of
multiplication. Thus, one can think of the diagonal blocks in a block-diagonal
matrix as separate matrices, which are stuck together in a block-diagonal
shape but don't interfere with each other.

Taking powers of block-diagonal matrices follows the same paradigm:

\begin{corollary}
\label{cor.blockmatrix.powt-diag}Let $u\in\mathbb{N}$. Let $n_{1},n_{2}%
,\ldots,n_{u}\in\mathbb{N}$. For each $i\in\left[  u\right]  $, let $A\left(
i,i\right)  $ be an $n_{i}\times n_{i}$-matrix. Then,%
\begin{align*}
&  \left(
\begin{array}
[c]{cccc}%
A\left(  1,1\right)  & 0 & \cdots & 0\\
0 & A\left(  2,2\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A\left(  u,u\right)
\end{array}
\right)  ^{k}\\
&  =\left(
\begin{array}
[c]{cccc}%
\left(  A\left(  1,1\right)  \right)  ^{k} & 0 & \cdots & 0\\
0 & \left(  A\left(  2,2\right)  \right)  ^{k} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \left(  A\left(  u,u\right)  \right)  ^{k}%
\end{array}
\right)
\end{align*}
for any $k\in\mathbb{N}$. (Here, each \textquotedblleft$0$\textquotedblright%
\ means a zero matrix of appropriate dimensions.)
\end{corollary}

\begin{proof}
Straightforward induction on $k$. The base case ($k=0$) says that%
\[
I_{n_{1}+n_{2}+\cdots+n_{u}}=\left(
\begin{array}
[c]{cccc}%
I_{n_{1}} & 0 & \cdots & 0\\
0 & I_{n_{2}} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & I_{n_{u}}%
\end{array}
\right)  ,
\]
which should be fairly clear. The induction step is an easy application of
Corollary \ref{cor.blockmatrix.mult-diag}.
\end{proof}

Finally, the \textquotedblleft diagonal blocks stuck
together\textquotedblright\ philosophy for block-diagonal matrices holds for
nullities as well. To wit, the nullity of a block-diagonal matrix is the sum
of the nullities of its diagonal blocks. In other words:

\begin{proposition}
\label{prop.blockmatrix.nullity-diag}Let $u\in\mathbb{N}$. Let $n_{1}%
,n_{2},\ldots,n_{u}\in\mathbb{N}$. For each $i\in\left[  u\right]  $, let
$A_{i}$ be an $n_{i}\times n_{i}$-matrix. Then,%
\begin{align}
&  \dim\left(  \operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  \right) \nonumber\\
&  =\dim\left(  \operatorname*{Ker}\left(  A_{1}\right)  \right)  +\dim\left(
\operatorname*{Ker}\left(  A_{2}\right)  \right)  +\cdots+\dim\left(
\operatorname*{Ker}\left(  A_{u}\right)  \right)  .
\label{eq.prop.blockmatrix.nullity-diag.clm}%
\end{align}

\end{proposition}

\begin{proof}
Let $\mathbb{F}$ be the field that our matrices are defined over. If
$v_{\left\langle 1\right\rangle },v_{\left\langle 2\right\rangle }%
,\ldots,v_{\left\langle u\right\rangle }$ are $u$ column vectors (of whatever
sizes), then $\left(
\begin{array}
[c]{c}%
v_{\left\langle 1\right\rangle }\\
v_{\left\langle 2\right\rangle }\\
\vdots\\
v_{\left\langle u\right\rangle }%
\end{array}
\right)  $ shall mean the big column vector obtained by stacking these $u$
column vectors $v_{\left\langle 1\right\rangle },v_{\left\langle
2\right\rangle },\ldots,v_{\left\langle u\right\rangle }$ atop one another.
(This is the particular case of the block matrix notation from Definition
\ref{def.blockmatrix.uxv} for $v=1$ and $p_{1}=1$.) It is easy to see (e.g.,
using Proposition \ref{prop.blockmatrix.mult-uxv}) that if $v_{\left\langle
1\right\rangle },v_{\left\langle 2\right\rangle },\ldots,v_{\left\langle
u\right\rangle }$ are $u$ column vectors with $v_{\left\langle i\right\rangle
}\in\mathbb{F}^{n_{i}}$ for each $i\in\left[  u\right]  $, then
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
v_{\left\langle 1\right\rangle }\\
v_{\left\langle 2\right\rangle }\\
\vdots\\
v_{\left\langle u\right\rangle }%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
A_{1}v_{\left\langle 1\right\rangle }\\
A_{2}v_{\left\langle 2\right\rangle }\\
\vdots\\
A_{u}v_{\left\langle u\right\rangle }%
\end{array}
\right)  . \label{pf.prop.blockmatrix.nullity-diag.1}%
\end{equation}


Let $N:=n_{1}+n_{2}+\cdots+n_{u}$. Any vector $v\in\mathbb{F}^{N}$ can be
uniquely written in block-matrix notation as $\left(
\begin{array}
[c]{c}%
v_{\left\langle 1\right\rangle }\\
v_{\left\langle 2\right\rangle }\\
\vdots\\
v_{\left\langle u\right\rangle }%
\end{array}
\right)  $, where each $v_{\left\langle i\right\rangle }$ is a vector in
$\mathbb{F}^{n_{i}}$. (To wit, we just subdivide $v$ into blocks of sizes
$n_{1},n_{2},\ldots,n_{u}$ from top to bottom; the topmost block will be
$v_{\left\langle 1\right\rangle }$, the second-topmost will be
$v_{\left\langle 2\right\rangle }$, and so on. Formally speaking, for each
$i\in\left[  u\right]  $, we set $N_{i}:=n_{1}+n_{2}+\cdots+n_{i-1}$, and we
let $v_{\left\langle i\right\rangle }$ be the column vector in $\mathbb{F}%
^{n_{i}}$ whose entries are the $\left(  N_{i}+1\right)  $-st, $\left(
N_{i}+2\right)  $-nd, $\ldots$, $\left(  N_{i}+n_{i}\right)  $-th entries of
$v$.)

Now, consider a vector $v\in\mathbb{F}^{N}$ that is written in block-matrix
notation $\left(
\begin{array}
[c]{c}%
v_{\left\langle 1\right\rangle }\\
v_{\left\langle 2\right\rangle }\\
\vdots\\
v_{\left\langle u\right\rangle }%
\end{array}
\right)  $, where each $v_{\left\langle i\right\rangle }$ is a vector in
$\mathbb{F}^{n_{i}}$. Then,
\begin{align*}
\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  v  &  =\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
v_{\left\langle 1\right\rangle }\\
v_{\left\langle 2\right\rangle }\\
\vdots\\
v_{\left\langle u\right\rangle }%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
A_{1}v_{\left\langle 1\right\rangle }\\
A_{2}v_{\left\langle 2\right\rangle }\\
\vdots\\
A_{u}v_{\left\langle u\right\rangle }%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.blockmatrix.nullity-diag.1})}\right)  .
\end{align*}
Hence, $\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  v=0$ holds if and only if $A_{i}v_{\left\langle i\right\rangle }=0$
holds for each $i\in\left[  u\right]  $. In other words, $v\in
\operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  $ holds if and only if $v_{\left\langle i\right\rangle }%
\in\operatorname*{Ker}\left(  A_{i}\right)  $ holds for each $i\in\left[
u\right]  $. In other words, the vectors in $\operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  $ are precisely the vectors of the form $\left(
\begin{array}
[c]{c}%
v_{\left\langle 1\right\rangle }\\
v_{\left\langle 2\right\rangle }\\
\vdots\\
v_{\left\langle u\right\rangle }%
\end{array}
\right)  $, where $v_{\left\langle i\right\rangle }\in\operatorname*{Ker}%
\left(  A_{i}\right)  $ for each $i\in\left[  u\right]  $. Thus,%
\[
\operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  \cong\operatorname*{Ker}\left(  A_{1}\right)  \oplus
\operatorname*{Ker}\left(  A_{2}\right)  \oplus\cdots\oplus\operatorname*{Ker}%
\left(  A_{u}\right)
\]
as vector spaces. By taking dimensions on both sides, this yields
(\ref{eq.prop.blockmatrix.nullity-diag.clm}).
\end{proof}

\subsubsection{Unitarity}

Now, we claim that a block-diagonal matrix is unitary if and only if its
diagonal blocks are unitary:

\begin{proposition}
\label{prop.blockmatrix.unitary-diag}Let $u\in\mathbb{N}$. Let $n_{1}%
,n_{2},\ldots,n_{u}\in\mathbb{N}$. For each $i\in\left[  u\right]  $, let
$A_{i}\in\mathbb{C}^{n_{i}\times n_{i}}$ be a matrix. Then, the block-diagonal
matrix $\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  $ is unitary if and only if all $u$ matrices $A_{1},A_{2}%
,\ldots,A_{u}$ are unitary.
\end{proposition}

\begin{proof}
Let $N=n_{1}+n_{2}+\cdots+n_{u}$. Let
\begin{equation}
A=\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right)  . \label{pf.prop.blockmatrix.unitary-diag.A=}%
\end{equation}
Thus, we must prove that $A$ is unitary if and only if all $u$ matrices
$A_{1},A_{2},\ldots,A_{u}$ are unitary.

It is easy to see that
\[
A^{\ast}=\left(
\begin{array}
[c]{cccc}%
A_{1}^{\ast} & 0 & \cdots & 0\\
0 & A_{2}^{\ast} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}^{\ast}%
\end{array}
\right)  .
\]
Multiplying this equality by (\ref{pf.prop.blockmatrix.unitary-diag.A=}), we
obtain%
\begin{align*}
A^{\ast}A  &  =\left(
\begin{array}
[c]{cccc}%
A_{1}^{\ast} & 0 & \cdots & 0\\
0 & A_{2}^{\ast} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}^{\ast}%
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
A_{1}^{\ast}A_{1} & 0 & \cdots & 0\\
0 & A_{2}^{\ast}A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}^{\ast}A_{u}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Corollary
\ref{cor.blockmatrix.mult-diag}}\right)  .
\end{align*}
On the other hand, it is again easy to see that
\[
I_{N}=\left(
\begin{array}
[c]{cccc}%
I_{n_{1}} & 0 & \cdots & 0\\
0 & I_{n_{2}} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & I_{n_{u}}%
\end{array}
\right)
\]
(since $N=n_{1}+n_{2}+\cdots+n_{u}$). In light of these two equalities, we see
that $A^{\ast}A=I_{N}$ holds if and only if
\[
\left(
\begin{array}
[c]{cccc}%
A_{1}^{\ast}A_{1} & 0 & \cdots & 0\\
0 & A_{2}^{\ast}A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{u}^{\ast}A_{u}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cccc}%
I_{n_{1}} & 0 & \cdots & 0\\
0 & I_{n_{2}} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & I_{n_{u}}%
\end{array}
\right)
\]
holds, i.e., if and only if we have $A_{i}^{\ast}A_{i}=I_{n_{i}}$ for each
$i\in\left[  u\right]  $. Likewise, we can see that $AA^{\ast}=I_{N}$ holds if
and only if we have $A_{i}A_{i}^{\ast}=I_{n_{i}}$ for each $i\in\left[
u\right]  $. Hence, we have the following chain of equivalences:%
\begin{align*}
&  \ \left(  A\text{ is unitary}\right) \\
&  \Longleftrightarrow\ \left(  AA^{\ast}=I_{N}\text{ and }A^{\ast}%
A=I_{N}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  \text{by the equivalence
}\mathcal{A}\Longleftrightarrow\mathcal{C}\text{ in Theorem
\ref{thm.unitary.unitary.eqs}}\right) \\
&  \Longleftrightarrow\ \left(  \text{we have }A_{i}A_{i}^{\ast}=I_{n_{i}%
}\text{ and }A_{i}^{\ast}A_{i}=I_{n_{i}}\text{ for each }i\in\left[  u\right]
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since we have shown that }AA^{\ast}=I_{N}\text{ holds if and only if}\\
\text{we have }A_{i}A_{i}^{\ast}=I_{n_{i}}\text{ for each }i\in\left[
u\right]  \text{, and since we have}\\
\text{shown that }A^{\ast}A=I_{N}\text{ holds if and only if}\\
\text{we have }A_{i}^{\ast}A_{i}=I_{n_{i}}\text{ for each }i\in\left[
u\right]
\end{array}
\right) \\
&  \Longleftrightarrow\ \left(  \text{the matrix }A_{i}\text{ is unitary for
each }i\in\left[  u\right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  \text{by the equivalence
}\mathcal{C}\Longleftrightarrow\mathcal{A}\text{ in Theorem
\ref{thm.unitary.unitary.eqs}}\right) \\
&  \Longleftrightarrow\ \left(  \text{all }u\text{ matrices }A_{1}%
,A_{2},\ldots,A_{u}\text{ are unitary}\right)  .
\end{align*}
But this is precisely what we need to show. Thus, Proposition
\ref{prop.blockmatrix.unitary-diag} is proven.
\end{proof}

\subsection{The Gram--Schmidt process}%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 3 starts here.}\\\hline\hline
\end{tabular}
\ \ \
\]


We now come to one of the most crucial algorithms in linear algebra.

\begin{theorem}
[Gram--Schmidt process]\label{thm.unitary.gs}Let $\left(  v_{1},v_{2}%
,\ldots,v_{m}\right)  $ be a linearly independent tuple of vectors in
$\mathbb{C}^{n}$.

Then, there is an orthogonal tuple $\left(  z_{1},z_{2},\ldots,z_{m}\right)  $
of vectors in $\mathbb{C}^{n}$ that satisfies%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]  .
\]


Furthermore, such a tuple $\left(  z_{1},z_{2},\ldots,z_{m}\right)  $ can be
constructed by the following recursive process:

\begin{itemize}
\item For each $p\in\left[  m\right]  $, if the first $p-1$ entries
$z_{1},z_{2},\ldots,z_{p-1}$ of this tuple have already been constructed, then
we define the $p$-th entry $z_{p}$ by the equality%
\begin{equation}
z_{p}=v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k}. \label{eq.thm.unitary.gs.zp=}%
\end{equation}
(Note that the sum on the right hand side of (\ref{eq.thm.unitary.gs.zp=}) is
an empty sum when $p=1$; thus, (\ref{eq.thm.unitary.gs.zp=}) simplifies to
$z_{1}=v_{1}$ in this case.)
\end{itemize}
\end{theorem}

Roughly speaking, the claim of Theorem \ref{thm.unitary.gs} is that if we
start with any linearly independent tuple $\left(  v_{1},v_{2},\ldots
,v_{m}\right)  $ of vectors in $\mathbb{C}^{n}$, then we can make this tuple
orthogonal by tweaking it as follows:

\begin{itemize}
\item leave $v_{1}$ unchanged;

\item modify $v_{2}$ by subtracting some scalar multiple of $v_{1}$;

\item modify $v_{3}$ by subtracting some linear combination of $v_{1}$ and
$v_{2}$;

\item modify $v_{4}$ by subtracting some linear combination of $v_{1}%
,v_{2},v_{3}$;

\item and so on.
\end{itemize}

\noindent Specifically, the equation (\ref{eq.thm.unitary.gs.zp=}) tells us
(recursively) the precise multiples (and linear combinations) that we need to
subtract. This recursive tweaking process is known as \emph{Gram--Schmidt
orthogonalization} or the \emph{Gram--Schmidt process}.

\begin{example}
Here is how the equalities (\ref{eq.thm.unitary.gs.zp=}) in Theorem
\ref{thm.unitary.gs} look like for $p\in\left\{  1,2,3,4\right\}  $:%
\begin{align*}
z_{1}  &  =v_{1};\\
z_{2}  &  =v_{2}-\dfrac{\left\langle v_{2},z_{1}\right\rangle }{\left\langle
z_{1},z_{1}\right\rangle }z_{1};\\
z_{3}  &  =v_{3}-\dfrac{\left\langle v_{3},z_{1}\right\rangle }{\left\langle
z_{1},z_{1}\right\rangle }z_{1}-\dfrac{\left\langle v_{3},z_{2}\right\rangle
}{\left\langle z_{2},z_{2}\right\rangle }z_{2};\\
z_{4}  &  =v_{4}-\dfrac{\left\langle v_{4},z_{1}\right\rangle }{\left\langle
z_{1},z_{1}\right\rangle }z_{1}-\dfrac{\left\langle v_{4},z_{2}\right\rangle
}{\left\langle z_{2},z_{2}\right\rangle }z_{2}-\dfrac{\left\langle v_{4}%
,z_{3}\right\rangle }{\left\langle z_{3},z_{3}\right\rangle }z_{3}.
\end{align*}

\end{example}

\begin{example}
Let us try out the recursive construction of $\left(  z_{1},z_{2},\ldots
,z_{m}\right)  $ from Theorem \ref{thm.unitary.gs} on an example. Let $n=4$
and $m=3$ and
\[
v_{1}=\left(
\begin{array}
[c]{c}%
1\\
1\\
1\\
1
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ v_{2}=\left(
\begin{array}
[c]{c}%
0\\
-2\\
0\\
-2
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ v_{3}=\left(
\begin{array}
[c]{c}%
2\\
-2\\
0\\
0
\end{array}
\right)  .
\]
Then, (\ref{eq.thm.unitary.gs.zp=}) becomes%
\begin{align*}
z_{1}  &  =v_{1}=\left(
\begin{array}
[c]{c}%
1\\
1\\
1\\
1
\end{array}
\right)  ;\\
z_{2}  &  =v_{2}-\dfrac{\left\langle v_{2},z_{1}\right\rangle }{\left\langle
z_{1},z_{1}\right\rangle }z_{1}=\left(
\begin{array}
[c]{c}%
0\\
-2\\
0\\
-2
\end{array}
\right)  -\dfrac{-4}{4}\left(
\begin{array}
[c]{c}%
1\\
1\\
1\\
1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1\\
-1\\
1\\
-1
\end{array}
\right)  ;\\
z_{3}  &  =v_{3}-\dfrac{\left\langle v_{3},z_{1}\right\rangle }{\left\langle
z_{1},z_{1}\right\rangle }z_{1}-\dfrac{\left\langle v_{3},z_{2}\right\rangle
}{\left\langle z_{2},z_{2}\right\rangle }z_{2}\\
&  =\left(
\begin{array}
[c]{c}%
2\\
-2\\
0\\
0
\end{array}
\right)  -\dfrac{0}{4}\left(
\begin{array}
[c]{c}%
1\\
1\\
1\\
1
\end{array}
\right)  -\dfrac{4}{4}\left(
\begin{array}
[c]{c}%
1\\
-1\\
1\\
-1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1\\
-1\\
-1\\
1
\end{array}
\right)  .
\end{align*}
So
\[
\left(  z_{1},z_{2},z_{3}\right)  =\left(  \left(
\begin{array}
[c]{c}%
1\\
1\\
1\\
1
\end{array}
\right)  ,\ \ \left(
\begin{array}
[c]{c}%
1\\
-1\\
1\\
-1
\end{array}
\right)  ,\ \ \left(
\begin{array}
[c]{c}%
1\\
-1\\
-1\\
1
\end{array}
\right)  \right)
\]
is an orthogonal tuple of vectors.

According to Proposition \ref{prop.unitary.innerprod.orth-norm}, we thus
obtain an orthonormal tuple%
\[
\left(  \dfrac{1}{\left\vert \left\vert z_{1}\right\vert \right\vert }%
z_{1},\ \ \dfrac{1}{\left\vert \left\vert z_{2}\right\vert \right\vert }%
z_{2},\ \ \dfrac{1}{\left\vert \left\vert z_{3}\right\vert \right\vert }%
z_{3}\right)  =\left(  \left(
\begin{array}
[c]{c}%
1/2\\
1/2\\
1/2\\
1/2
\end{array}
\right)  ,\ \ \left(
\begin{array}
[c]{c}%
1/2\\
-1/2\\
1/2\\
-1/2
\end{array}
\right)  ,\ \ \left(
\begin{array}
[c]{c}%
1/2\\
-1/2\\
-1/2\\
1/2
\end{array}
\right)  \right)  .
\]
(We are in luck with this example; normally we would get square roots at this step.)
\end{example}

For more examples of the Gram--Schmidt process, see \cite[Week 3,
\S 4]{Bartle14}. (These examples all use vectors in $\mathbb{R}^{n}$ rather
than $\mathbb{C}^{n}$, which allows for visualization and saves one the
trouble of complex conjugates.)

Our proof of Theorem \ref{thm.unitary.gs} will require a simple lemma from
elementary linear algebra:

\begin{lemma}
\label{lem.span-last-vec-change}Let $V$ be a vector space over some field. Let
$v_{1},v_{2},\ldots,v_{k}$ be some vectors in $V$. Let $x$ and $y$ be two
further vectors in $V$. Assume that $x-y\in\operatorname*{span}\left\{
v_{1},v_{2},\ldots,v_{k}\right\}  $. Then,%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},x\right\}
=\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.span-last-vec-change}.]Set%
\begin{align*}
S  &  :=\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k}\right\}  ;\\
X  &  :=\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},x\right\}  ;\\
Y  &  :=\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  .
\end{align*}
These three sets $S$, $X$ and $Y$ are subspaces of $V$ (since a span is always
a vector subspace). By assumption, we have $x-y\in\operatorname*{span}\left\{
v_{1},v_{2},\ldots,v_{k}\right\}  =S$. Therefore, $-\left(  x-y\right)  \in S$
as well (since $S$ is a vector subspace of $V$). In other words, $y-x\in S$
(since $-\left(  x-y\right)  =y-x$). Hence, $x$ and $y$ play symmetric roles
in our situation.

However, $x-y\in S=\operatorname*{span}\left\{  v_{1},v_{2},\ldots
,v_{k}\right\}  $ shows that $x-y$ is a linear combination of $v_{1}%
,v_{2},\ldots,v_{k}$. In other words,%
\begin{equation}
x-y=\lambda_{1}v_{1}+\lambda_{2}v_{2}+\cdots+\lambda_{k}v_{k},
\label{pf.lem.span-last-vec-change.1}%
\end{equation}
where $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$ are some scalars (i.e.,
elements of the base field). Consider these scalars. Solving the equality
(\ref{pf.lem.span-last-vec-change.1}) for $x$, we obtain%
\[
x=\lambda_{1}v_{1}+\lambda_{2}v_{2}+\cdots+\lambda_{k}v_{k}+y.
\]
This shows that $x$ is a linear combination of $v_{1},v_{2},\ldots,v_{k},y$.
In other words, $x\in\operatorname*{span}\left\{  v_{1},v_{2},\ldots
,v_{k},y\right\}  $. In other words, $x\in Y$ (since $Y=\operatorname*{span}%
\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  $). On the other hand, each
$i\in\left[  k\right]  $ satisfies%
\[
v_{i}\in\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  \subseteq
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  =Y.
\]
In other words, the $k$ vectors $v_{1},v_{2},\ldots,v_{k}$ belong to $Y$.
Since we also know that $x\in Y$, we thus conclude that all $k+1$ vectors
$v_{1},v_{2},\ldots,v_{k},x$ belong to $Y$. Since $Y$ is a vector subspace of
$V$, this entails that any linear combination of $v_{1},v_{2},\ldots,v_{k},x$
must belong to $Y$. In other words,%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},x\right\}  \subseteq Y
\]
(since $\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},x\right\}  $ is
the set of all linear combinations of $v_{1},v_{2},\ldots,v_{k},x$). In other
words, $X\subseteq Y$ (since $X=\operatorname*{span}\left\{  v_{1}%
,v_{2},\ldots,v_{k},x\right\}  $).

However, as we explained, $x$ and $y$ play symmetric roles in our situation.
Swapping $x$ with $y$ results in the exchange of $X$ with $Y$. Thus, just as
we have proved $X\subseteq Y$, we can show that $Y\subseteq X$. Combining
these two inclusions, we obtain $X=Y$. In view of $X=\operatorname*{span}%
\left\{  v_{1},v_{2},\ldots,v_{k},x\right\}  $ and $Y=\operatorname*{span}%
\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  $, this rewrites as%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},x\right\}
=\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{k},y\right\}  .
\]
This proves Lemma \ref{lem.span-last-vec-change}.
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm.unitary.gs}.]We define a tuple $\left(  z_{1}%
,z_{2},\ldots,z_{m}\right)  $ recursively by (\ref{eq.thm.unitary.gs.zp=}).
First, we need to show that this tuple is actually well-defined -- i.e., that
the denominators $\left\langle z_{k},z_{k}\right\rangle $ in the equality
(\ref{eq.thm.unitary.gs.zp=}) never become $0$ in the process (which would
render (\ref{eq.thm.unitary.gs.zp=}) meaningless and therefore prevent $z_{p}$
from being well-defined). Second, we need to show that the resulting tuple
does indeed satisfy
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]  .
\]
Finally, we need to show that the resulting tuple is orthogonal.

Let us prove the first two of these three claims in lockstep, by showing the
following claim:

\begin{statement}
\textit{Claim 1:} For each $p\in\left\{  0,1,\ldots,m\right\}  $, the vectors
$z_{1},z_{2},\ldots,z_{p}$ are well-defined and satisfy%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  .
\]

\end{statement}

[\textit{Proof of Claim 1:} We induct on $p$.

\textit{Induction base:} Claim 1 is obviously true for $p=0$ (since
$\operatorname*{span}\left\{  {}\right\}  =\operatorname*{span}\left\{
{}\right\}  $).

\textit{Induction step:} Fix some $p\in\left[  m\right]  $, and assume that
the vectors $z_{1},z_{2},\ldots,z_{p-1}$ are well-defined and satisfy
\begin{equation}
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}  .
\label{pf.thm.unitary.gs.6}%
\end{equation}
We now need to show that the vectors $z_{1},z_{2},\ldots,z_{p}$ are
well-defined and satisfy%
\begin{equation}
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  .
\label{pf.thm.unitary.gs.5}%
\end{equation}


The tuple $\left(  v_{1},v_{2},\ldots,v_{p}\right)  $ is linearly independent
(since the tuple $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ is linearly
independent). Thus, the span $\operatorname*{span}\left\{  v_{1},v_{2}%
,\ldots,v_{p-1}\right\}  $ is $\left(  p-1\right)  $-dimensional and we have
$v_{p}\notin\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1}\right\}
$. Hence,%
\[
v_{p}\notin\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.unitary.gs.6})}\right)  .
\]


Now, recall that the span $\operatorname*{span}\left\{  v_{1},v_{2}%
,\ldots,v_{p-1}\right\}  $ is $\left(  p-1\right)  $-dimensional. In view of
(\ref{pf.thm.unitary.gs.6}), we can rewrite this as follows: The span
$\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}  $ is
$\left(  p-1\right)  $-dimensional. In other words, the tuple $\left(
z_{1},z_{2},\ldots,z_{p-1}\right)  $ is linearly independent. Hence, for each
$k\in\left[  p-1\right]  $, we have $z_{k}\neq0$ and therefore $\left\langle
z_{k},z_{k}\right\rangle >0$ (by Proposition \ref{prop.unitary.innerprod.pos}
\textbf{(b)}), so that $\left\langle z_{k},z_{k}\right\rangle \neq0$. Thus,
the denominators on the right hand side of (\ref{eq.thm.unitary.gs.zp=}) are
nonzero, so that $z_{p}$ is well-defined. Hence, the vectors $z_{1}%
,z_{2},\ldots,z_{p}$ are well-defined (since we already know that the vectors
$z_{1},z_{2},\ldots,z_{p-1}$ are well-defined).

It remains to prove that
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  .
\]
But this is easy: From (\ref{eq.thm.unitary.gs.zp=}), we obtain%
\[
v_{p}-z_{p}=\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k}\in\operatorname*{span}\left\{
z_{1},z_{2},\ldots,z_{p-1}\right\}
\]
(since $\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k}$ is clearly a linear
combination of $z_{1},z_{2},\ldots,z_{p-1}$). Hence, Lemma
\ref{lem.span-last-vec-change} (applied to $k=p-1$ and $x=v_{p}$ and $y=z_{p}%
$) yields%
\begin{align*}
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1},v_{p}\right\}   &
=\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1},z_{p}\right\} \\
&  =\underbrace{\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1}%
\right\}  }_{=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}
}+\operatorname*{span}\left\{  z_{p}\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{span}\left(  A\cup B\right)  =\operatorname*{span}%
A+\operatorname*{span}B\\
\text{for any two sets }A\text{ and }B\text{ of vectors}%
\end{array}
\right) \\
&  =\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}
+\operatorname*{span}\left\{  z_{p}\right\} \\
&  =\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1},z_{p}\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{span}A+\operatorname*{span}B=\operatorname*{span}%
\left(  A\cup B\right) \\
\text{for any two sets }A\text{ and }B\text{ of vectors}%
\end{array}
\right)  .
\end{align*}
In other words, $\operatorname*{span}\left\{  v_{1},v_{2},\ldots
,v_{p}\right\}  =\operatorname*{span}\left\{  z_{1},z_{2},\ldots
,z_{p}\right\}  $. Thus, the induction step is complete, so that Claim 1 is
proved by induction.] \medskip

Claim 1 (applied to $p=m$) shows that the vectors $z_{1},z_{2},\ldots,z_{m}$
are well-defined. In other words, the tuple $\left(  z_{1},z_{2},\ldots
,z_{m}\right)  $ is well-defined. Furthermore, this tuple satisfies%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}
=\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]
\]
(by Claim 1, applied to $p=j$). It now remains to show that this tuple is
orthogonal. We shall achieve this by showing the following claim:

\begin{statement}
\textit{Claim 2:} For any $j\in\left\{  0,1,\ldots,m\right\}  $, the tuple
$\left(  z_{1},z_{2},\ldots,z_{j}\right)  $ is orthogonal.
\end{statement}

[\textit{Proof of Claim 2:} We proceed by induction on $j$:

\textit{Induction base:} Claim 2 clearly holds for $j=0$, since the (empty)
$0$-tuple is vacuously orthogonal.

\textit{Induction step:} Let $p\in\left[  m\right]  $. Assume (as the
induction hypothesis) that Claim 2 holds for $j=p-1$. We must show that Claim
2 holds for $j=p$.

Our induction hypothesis says that Claim 2 holds for $j=p-1$. In other words,
the tuple $\left(  z_{1},z_{2},\ldots,z_{p-1}\right)  $ is orthogonal. In
other words, we have%
\begin{equation}
z_{a}\perp z_{b}\ \ \ \ \ \ \ \ \ \ \text{whenever }a,b\in\left[  p-1\right]
\text{ satisfy }a\neq b. \label{pf.thm.unitary.gs.12}%
\end{equation}
In other words, we have%
\begin{equation}
\left\langle z_{a},z_{b}\right\rangle =0\ \ \ \ \ \ \ \ \ \ \text{whenever
}a,b\in\left[  p-1\right]  \text{ satisfy }a\neq b.
\label{pf.thm.unitary.gs.12b}%
\end{equation}


We must show that Claim 2 holds for $j=p$. In other words, we must show that
the tuple $\left(  z_{1},z_{2},\ldots,z_{p}\right)  $ is orthogonal. In other
words, we must show that%
\begin{equation}
z_{a}\perp z_{b}\ \ \ \ \ \ \ \ \ \ \text{whenever }a,b\in\left[  p\right]
\text{ satisfy }a\neq b. \label{pf.thm.unitary.gs.13}%
\end{equation}
It will clearly suffice to prove (\ref{pf.thm.unitary.gs.13}) in the case when
one of $a$ and $b$ equals $p$ (because in all other cases, we have
$a,b\in\left[  p-1\right]  $, and thus $z_{a}\perp z_{b}$ follows from
(\ref{pf.thm.unitary.gs.12})).

Thus, let $a,b\in\left[  p\right]  $ satisfy $a\neq b$, and assume that one of
$a$ and $b$ equals $p$. We must prove that $z_{a}\perp z_{b}$. Proposition
\ref{prop.unitary.innerprod.orth-symm} shows that $z_{a}\perp z_{b}$ is
equivalent to $z_{b}\perp z_{a}$. Thus, $a$ and $b$ play symmetric roles in
our claim. Hence, in our proof of $z_{a}\perp z_{b}$, we can WLOG assume that
$a\leq b$ (since otherwise, we can swap $a$ with $b$). Assume this. Hence,
$a<b$ (since $a\neq b$). Thus, $a<b\leq p$, so that $a\neq p$. However, we
assumed that one of $a$ and $b$ equals $p$; hence, $b=p$ (since $a\neq p$).
Also, we have $a\in\left[  p-1\right]  $ (since $a<b=p$).

Now, (\ref{eq.thm.unitary.gs.zp=}) yields%
\[
\left\langle z_{p},z_{a}\right\rangle =\left\langle v_{p}-\sum_{k=1}%
^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle }{\left\langle z_{k}%
,z_{k}\right\rangle }z_{k},z_{a}\right\rangle =\left\langle v_{p}%
,z_{a}\right\rangle -\left\langle \sum_{k=1}^{p-1}\dfrac{\left\langle
v_{p},z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle }z_{k}%
,z_{a}\right\rangle
\]
(by Proposition \ref{prop.unitary.innerprod.props} \textbf{(h)}). In view of%
\begin{align*}
&  \left\langle \sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k},z_{a}\right\rangle \\
&  =\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }\left\langle z_{k},z_{a}\right\rangle
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.unitary.innerprod.props} \textbf{(i)}}\right) \\
&  =\sum_{\substack{k\in\left[  p-1\right]  ;\\k\neq a}}\dfrac{\left\langle
v_{p},z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle
}\underbrace{\left\langle z_{k},z_{a}\right\rangle }_{\substack{=0\\\text{(by
(\ref{pf.thm.unitary.gs.13}), applied to }k\text{ and }a\\\text{instead of
}a\text{ and }b\text{)}}}+\underbrace{\dfrac{\left\langle v_{p},z_{a}%
\right\rangle }{\left\langle z_{a},z_{a}\right\rangle }\left\langle
z_{a},z_{a}\right\rangle }_{=\left\langle v_{p},z_{a}\right\rangle }\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=a\\
\text{from the sum, since }a\in\left[  p-1\right]
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{k\in\left[  p-1\right]  ;\\k\neq a}%
}\dfrac{\left\langle v_{p},z_{k}\right\rangle }{\left\langle z_{k}%
,z_{k}\right\rangle }0}_{=0}+\left\langle v_{p},z_{a}\right\rangle
=\left\langle v_{p},z_{a}\right\rangle ,
\end{align*}
we can rewrite this as%
\[
\left\langle z_{p},z_{a}\right\rangle =\left\langle v_{p},z_{a}\right\rangle
-\left\langle v_{p},z_{a}\right\rangle =0.
\]
In view of $b=p$, this rewrites as $\left\langle z_{b},z_{a}\right\rangle =0$.
Thus, $z_{b}\perp z_{a}$, so that $z_{a}\perp z_{b}$ (by Proposition
\ref{prop.unitary.innerprod.orth-symm}).

As explained above, this completes our proof of the fact that Claim 2 holds
for $j=p$. Thus, the induction step is complete, and Claim 2 is proven.]
\medskip

Now, applying Claim 2 to $j=m$, we obtain that the tuple $\left(  z_{1}%
,z_{2},\ldots,z_{m}\right)  $ is orthogonal. Thus, the proof of Theorem
\ref{thm.unitary.gs} is complete.
\end{proof}

One might wonder how the Gram--Schmidt process could be adapted to a tuple
$\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ of vectors that is \textbf{not}
linearly independent. The equality (\ref{eq.thm.unitary.gs.zp=}) requires the
vectors $z_{k}$ to be nonzero, since the denominators in which they appear
would be $0$ otherwise. In Theorem \ref{thm.unitary.gs}, this requirement is
indeed satisfied (as we have shown in the proof above). However, if we do not
assume $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ to be linearly independent,
then some of the $z_{k}$ can be zero, and so the construction of the following
$z_{p}$ will fail. There are several ways to adapt the process to this
complication. We will take the most stupid-sounding one: In the cases where
the equality (\ref{eq.thm.unitary.gs.zp=}) would produce a zero vector $z_{p}%
$, we opt to instead pick some nonzero vector orthogonal to $z_{1}%
,z_{2},\ldots,z_{p-1}$ (using Lemma \ref{lem.unitary.orthog.one-more}) and
declare it to be $z_{p}$. This works well as long as $m\leq n$; here is the result:

\begin{theorem}
[Gram--Schmidt process, take 2]\label{thm.unitary.gs-2}Let $\left(
v_{1},v_{2},\ldots,v_{m}\right)  $ be any tuple of vectors in $\mathbb{C}^{n}$
with $m\leq n$.

Then, there is an orthogonal tuple $\left(  z_{1},z_{2},\ldots,z_{m}\right)  $
of nonzero vectors in $\mathbb{C}^{n}$ that satisfies%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]  .
\]


Furthermore, such a tuple $\left(  z_{1},z_{2},\ldots,z_{m}\right)  $ can be
constructed by the following recursive process:

\begin{itemize}
\item For each $p\in\left[  m\right]  $, if the first $p-1$ entries
$z_{1},z_{2},\ldots,z_{p-1}$ of this tuple have already been constructed, then
we define the $p$-th entry $z_{p}$ as follows:

\begin{itemize}
\item \textit{If }$v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p}%
,z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle }z_{k}\neq0$, then
we define $z_{p}$ by the equality%
\begin{equation}
z_{p}=v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k}.
\label{eq.thm.unitary.gs-2.zp=sum}%
\end{equation}


\item \textit{If }$v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p}%
,z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle }z_{k}=0$, then we
pick an arbitrary nonzero vector $b\in\mathbb{C}^{n}$ that is orthogonal to
each of $z_{1},z_{2},\ldots,z_{p-1}$ (indeed, such a vector $b$ exists by
Lemma \ref{lem.unitary.orthog.one-more}, because $p-1<p\leq m\leq n$), and we
set%
\begin{equation}
z_{p}=b. \label{eq.thm.unitary.gs-2.zp=b}%
\end{equation}

\end{itemize}
\end{itemize}
\end{theorem}

\begin{proof}
[Proof of Theorem \ref{thm.unitary.gs-2}.]We define a tuple $\left(
z_{1},z_{2},\ldots,z_{m}\right)  $ by the recursive process described in
Theorem \ref{thm.unitary.gs-2}. It is clear that this tuple is actually
well-defined (indeed, the vectors $z_{p}$ are nonzero by their construction,
and thus the denominators $\left\langle z_{k},z_{k}\right\rangle $ in
(\ref{eq.thm.unitary.gs-2.zp=sum}) never become $0$, because Proposition
\ref{prop.unitary.innerprod.pos} \textbf{(b)} shows that any nonzero vector
$z$ satisfies $\left\langle z,z\right\rangle \neq0$). We do, however, need to
show that the resulting tuple does indeed satisfy
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]  ,
\]
and that this tuple is orthogonal.

Let us prove the first of these two claims:

\begin{statement}
\textit{Claim 1:} For each $p\in\left\{  0,1,\ldots,m\right\}  $, we have
$\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  $.
\end{statement}

[\textit{Proof of Claim 1:} We induct on $p$:

\textit{Induction base:} Claim 1 obviously holds for $p=0$.

\textit{Induction step:} Fix some $p\in\left[  m\right]  $, and assume that%
\begin{equation}
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1}\right\}
\subseteq\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}  .
\label{pf.thm.unitary.gs-2.4}%
\end{equation}
We now need to show that
\begin{equation}
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  .
\label{pf.thm.unitary.gs-2.5}%
\end{equation}


We shall first show that
\begin{equation}
v_{p}\in\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  .
\label{pf.thm.unitary.gs-2.5a}%
\end{equation}


Indeed, we recall our definition of $z_{p}$. This definition distinguishes
between two cases, depending on whether the difference $v_{p}-\sum_{k=1}%
^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle }{\left\langle z_{k}%
,z_{k}\right\rangle }z_{k}$ is $\neq0$ or $=0$. Let us analyze these two cases separately:

\begin{itemize}
\item \textit{Case 1:} We have $v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle
v_{p},z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle }z_{k}\neq0$.
In this case, $z_{p}$ is defined by the equality
(\ref{eq.thm.unitary.gs-2.zp=sum}). Solving this equality for $v_{p}$, we
obtain%
\[
v_{p}=z_{p}+\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k}\in\operatorname*{span}\left\{
z_{1},z_{2},\ldots,z_{p}\right\}  .
\]
Thus, (\ref{pf.thm.unitary.gs-2.5a}) is proved in Case 1.

\item \textit{Case 2:} We have $v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle
v_{p},z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle }z_{k}=0$. In
this case, we have%
\[
v_{p}=\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p},z_{k}\right\rangle
}{\left\langle z_{k},z_{k}\right\rangle }z_{k}\in\operatorname*{span}\left\{
z_{1},z_{2},\ldots,z_{p-1}\right\}  \subseteq\operatorname*{span}\left\{
z_{1},z_{2},\ldots,z_{p}\right\}  .
\]
Hence, (\ref{pf.thm.unitary.gs-2.5a}) is proved in Case 2.
\end{itemize}

We have now proved (\ref{pf.thm.unitary.gs-2.5a}) in both cases. However, for
each $i\in\left[  p-1\right]  $, we have%
\begin{align*}
v_{i}  &  \in\left\{  v_{1},v_{2},\ldots,v_{p-1}\right\}  \subseteq
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p-1}\right\} \\
&  \subseteq\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p-1}\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.unitary.gs-2.4})}\right) \\
&  \subseteq\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  .
\end{align*}
In other words, all $p-1$ vectors $v_{1},v_{2},\ldots,v_{p-1}$ belong to
$\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  $. Since the
vector $v_{p}$ also belongs to $\operatorname*{span}\left\{  z_{1}%
,z_{2},\ldots,z_{p}\right\}  $ (by (\ref{pf.thm.unitary.gs-2.5a})), we thus
conclude that all $p$ vectors $v_{1},v_{2},\ldots,v_{p}$ belong to
$\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  $. Therefore,
$\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{p}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{p}\right\}  $. Thus, the
induction step is complete, so that Claim 1 is proved by induction.] \medskip

It now remains to show that the tuple $\left(  z_{1},z_{2},\ldots
,z_{m}\right)  $ is orthogonal. We shall achieve this by showing the following claim:

\begin{statement}
\textit{Claim 2:} For any $j\in\left\{  0,1,\ldots,m\right\}  $, the tuple
$\left(  z_{1},z_{2},\ldots,z_{j}\right)  $ is orthogonal.
\end{statement}

[\textit{Proof of Claim 2:} We proceed by induction on $j$, similarly to the
proof of Claim 2 in the proof of Theorem \ref{thm.unitary.gs}. Only one minor
complication emerges in the induction step:

\textit{Induction step:} Let $p\in\left[  m\right]  $. Assume (as the
induction hypothesis) that Claim 2 holds for $j=p-1$. We must show that Claim
2 holds for $j=p$.

As in the proof of Theorem \ref{thm.unitary.gs}, we can convince ourselves
that it suffices to show that
\begin{equation}
z_{a}\perp z_{b}\ \ \ \ \ \ \ \ \ \ \text{whenever }a,b\in\left[  p\right]
\text{ satisfy }a\neq b. \label{pf.thm.unitary.gs-2.13}%
\end{equation}
Moreover, we only need to show this in the case when one of $a$ and $b$ equals
$p$ (because in all other cases, it follows from the induction hypothesis). In
other words, we only need to show that the vector $z_{p}$ is orthogonal to
each of $z_{1},z_{2},\ldots,z_{p-1}$.

Recall our definition of $z_{p}$. This definition distinguishes between two
cases, depending on whether the difference $v_{p}-\sum_{k=1}^{p-1}%
\dfrac{\left\langle v_{p},z_{k}\right\rangle }{\left\langle z_{k}%
,z_{k}\right\rangle }z_{k}$ is $\neq0$ or $=0$. In the first of these two
cases, the proof proceeds exactly as in the proof of Theorem
\ref{thm.unitary.gs}. Let us thus WLOG assume that we are in the second case.
That is, we assume that $v_{p}-\sum_{k=1}^{p-1}\dfrac{\left\langle v_{p}%
,z_{k}\right\rangle }{\left\langle z_{k},z_{k}\right\rangle }z_{k}=0$. Hence,
$z_{p}$ is defined by (\ref{eq.thm.unitary.gs-2.zp=b}), where $b$ is a nonzero
vector in $\mathbb{C}^{n}$ that is orthogonal to each of $z_{1},z_{2}%
,\ldots,z_{p-1}$. This shows that $z_{p}$ is orthogonal to each of
$z_{1},z_{2},\ldots,z_{p-1}$. But as we explained above, this is exactly what
we need to show. Thus, Claim 2 holds for $j=p$. The induction step is
complete, and Claim 2 is proved.] \medskip

Now, applying Claim 2 to $j=m$, we obtain that the tuple $\left(  z_{1}%
,z_{2},\ldots,z_{m}\right)  $ is orthogonal. Thus, the proof of Theorem
\ref{thm.unitary.gs-2} is complete.
\end{proof}

\begin{corollary}
\label{cor.unitary.gs-2nor}Let $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ be
any tuple of vectors in $\mathbb{C}^{n}$ with $m\leq n$.

Then, there is an orthonormal tuple $\left(  q_{1},q_{2},\ldots,q_{m}\right)
$ of vectors in $\mathbb{C}^{n}$ that satisfies%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}  \subseteq
\operatorname*{span}\left\{  q_{1},q_{2},\ldots,q_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]  .
\]

\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.unitary.gs-2nor}.]We have $m\leq n$. Hence,
Theorem \ref{thm.unitary.gs-2} shows that there is an orthogonal tuple
$\left(  z_{1},z_{2},\ldots,z_{m}\right)  $ of nonzero vectors in
$\mathbb{C}^{n}$ that satisfies%
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  m\right]  .
\]
Consider this tuple $\left(  z_{1},z_{2},\ldots,z_{m}\right)  $. Proposition
\ref{prop.unitary.innerprod.orth-norm} (applied to $\left(  z_{1},z_{2}%
,\ldots,z_{m}\right)  $ instead of $\left(  u_{1},u_{2},\ldots,u_{k}\right)
$) then shows that the tuple%
\[
\left(  \dfrac{1}{\left\vert \left\vert z_{1}\right\vert \right\vert }%
z_{1},\ \ \dfrac{1}{\left\vert \left\vert z_{2}\right\vert \right\vert }%
z_{2},\ \ \ldots,\ \ \dfrac{1}{\left\vert \left\vert z_{m}\right\vert
\right\vert }z_{m}\right)
\]
is orthonormal. Moreover, we have
\[
\operatorname*{span}\left\{  v_{1},v_{2},\ldots,v_{j}\right\}  \subseteq
\operatorname*{span}\left\{  z_{1},z_{2},\ldots,z_{j}\right\}
=\operatorname*{span}\left\{  \dfrac{1}{\left\vert \left\vert z_{1}\right\vert
\right\vert }z_{1},\ \ \dfrac{1}{\left\vert \left\vert z_{2}\right\vert
\right\vert }z_{2},\ \ \ldots,\ \ \dfrac{1}{\left\vert \left\vert
z_{j}\right\vert \right\vert }z_{j}\right\}
\]
for all $j\in\left[  m\right]  $. Hence, Corollary \ref{cor.unitary.gs-2nor}
is proven (just take $q_{i}=\dfrac{1}{\left\vert \left\vert z_{i}\right\vert
\right\vert }z_{i}$).
\end{proof}

\subsection{QR factorization}

Recall that an isometry is a matrix whose columns form an orthonormal tuple.
(We saw this in Proposition \ref{prop.unitary.innerprod.isometry.2}.)

\begin{theorem}
[QR factorization, isometry version]\label{thm.unitary.QR1}Let $A\in
\mathbb{C}^{n\times m}$ satisfy $n\geq m$. Then, there exists an isometry
$Q\in\mathbb{C}^{n\times m}$ and an upper-triangular matrix $R\in
\mathbb{C}^{m\times m}$ such that $A=QR$.
\end{theorem}

The pair $\left(  Q,R\right)  $ in Theorem \ref{thm.unitary.QR1} is called a
\emph{QR factorization} of $A$. (We are using the indefinite article, since it
is usually not unique.)

\begin{example}
Let%
\[
A=\left(
\begin{array}
[c]{cccc}%
1 & 0 & 1 & 2\\
1 & -2 & 0 & 2\\
1 & 0 & 1 & 0\\
1 & -2 & 0 & 0
\end{array}
\right)  \in\mathbb{C}^{4\times4}.
\]
Then, one QR factorization of $A$ is given by%
\[
A=\underbrace{\left(
\begin{array}
[c]{cccc}%
1/2 & 1/2 & 1/2 & 1/2\\
1/2 & -1/2 & 1/2 & -1/2\\
1/2 & 1/2 & -1/2 & -1/2\\
1/2 & -1/2 & -1/2 & 1/2
\end{array}
\right)  }_{=Q}\underbrace{\left(
\begin{array}
[c]{cccc}%
2 & -2 & 1 & 0\\
0 & 2 & 1 & 2\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 2
\end{array}
\right)  }_{=R}.
\]
Another is given by%
\[
A=\underbrace{\left(
\begin{array}
[c]{cccc}%
1/2 & 1/2 & \sqrt{2}/2 & 0\\
1/2 & -1/2 & 0 & \sqrt{2}/2\\
1/2 & 1/2 & -\sqrt{2}/2 & 0\\
1/2 & -1/2 & 0 & -\sqrt{2}/2
\end{array}
\right)  }_{=Q}\underbrace{\left(
\begin{array}
[c]{cccc}%
2 & -2 & 1 & 2\\
0 & 2 & 1 & 0\\
0 & 0 & 0 & \sqrt{2}\\
0 & 0 & 0 & \sqrt{2}%
\end{array}
\right)  }_{=R}.
\]

\end{example}

\begin{proof}
[Proof of Theorem \ref{thm.unitary.QR1}.]Recall that $A_{\bullet,1}%
,A_{\bullet,2},\ldots,A_{\bullet,m}$ denote the $m$ columns of the matrix $A$.
We have $m\leq n$ (since $n\geq m$). Hence, applying Corollary
\ref{cor.unitary.gs-2nor} to $\left(  v_{1},v_{2},\ldots,v_{m}\right)
=\left(  A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m}\right)  $, we
conclude that there is an orthonormal tuple $\left(  q_{1},q_{2},\ldots
,q_{m}\right)  $ of vectors in $\mathbb{C}^{n}$ that satisfies%
\begin{align}
\operatorname*{span}\left\{  A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet
,j}\right\}   &  \subseteq\operatorname*{span}\left\{  q_{1},q_{2}%
,\ldots,q_{j}\right\} \label{pf.thm.unitary.QR1.spansequal}\\
\ \ \ \ \ \ \ \ \ \ \text{for all }j  &  \in\left[  m\right]  .\nonumber
\end{align}
Consider this tuple $\left(  q_{1},q_{2},\ldots,q_{m}\right)  $. Let
$Q\in\mathbb{C}^{n\times m}$ be the matrix whose columns are $q_{1}%
,q_{2},\ldots,q_{m}$. Then, $Q$ is an isometry (by Proposition
\ref{prop.unitary.innerprod.isometry.2}, since its columns form an orthonormal
tuple). The definition of $Q$ shows that%
\begin{equation}
Q_{\bullet,i}=q_{i}\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  m\right]  .
\label{pf.thm.unitary.QR1.Qbull}%
\end{equation}


Now, let $j\in\left[  m\right]  $. Then,%
\[
A_{\bullet,j}\in\operatorname*{span}\left\{  A_{\bullet,1},A_{\bullet
,2},\ldots,A_{\bullet,j}\right\}  \subseteq\operatorname*{span}\left\{
q_{1},q_{2},\ldots,q_{j}\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.unitary.QR1.spansequal})}\right)  .
\]
In other words, there exist scalars $r_{1,j},r_{2,j},\ldots,r_{j,j}%
\in\mathbb{C}$ such that $A_{\bullet,j}=\sum_{i=1}^{j}r_{i,j}q_{i}$. Consider
these scalars $r_{1,j},r_{2,j},\ldots,r_{j,j}$. Also, set
\begin{equation}
r_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{for each integer }i>j.
\label{pf.thm.unitary.QR1.triang}%
\end{equation}
Thus,%
\begin{equation}
A_{\bullet,j}=\sum_{i=1}^{j}r_{i,j}q_{i}=\sum_{i=1}^{m}r_{i,j}q_{i}
\label{pf.thm.unitary.QR1.col=}%
\end{equation}
(since $\sum_{i=1}^{m}r_{i,j}q_{i}=\sum_{i=1}^{j}r_{i,j}q_{i}+\sum_{i=j+1}%
^{m}\underbrace{r_{i,j}}_{\substack{=0\\\text{(by
(\ref{pf.thm.unitary.QR1.triang}))}}}q_{i}=\sum_{i=1}^{j}r_{i,j}q_{i}$).

Forget that we fixed $j$. Thus, for each $j\in\left[  m\right]  $, we have
defined scalars $r_{1,j},r_{2,j},r_{3,j},\ldots\in\mathbb{C}$ that satisfy
(\ref{pf.thm.unitary.QR1.triang}) and (\ref{pf.thm.unitary.QR1.col=}).

Now, let $R\in\mathbb{C}^{m\times m}$ be the $m\times m$-matrix whose $\left(
i,j\right)  $-th entry is $r_{i,j}$ for each $i,j\in\left[  m\right]  $. This
matrix $R$ is upper-triangular, because of (\ref{pf.thm.unitary.QR1.triang}).
The definition of $R$ yields\footnote{Recall that $M_{i,j}$ is our general
notation for the $\left(  i,j\right)  $-th entry of a matrix $M$.}%
\begin{equation}
R_{i,j}=r_{i,j}\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left[  m\right]  .
\label{pf.thm.unitary.QR1.Rij=}%
\end{equation}
Furthermore, for each $j\in\left[  m\right]  $, we have%
\begin{align*}
A_{\bullet,j}  &  =\sum_{i=1}^{m}\underbrace{r_{i,j}}_{\substack{=R_{i,j}%
\\\text{(by (\ref{pf.thm.unitary.QR1.Rij=}))}}}\ \ \underbrace{q_{i}%
}_{\substack{=Q_{\bullet,i}\\\text{(by (\ref{pf.thm.unitary.QR1.Qbull}))}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.unitary.QR1.col=})}\right)
\\
&  =\sum_{i=1}^{m}R_{i,j}Q_{\bullet,i}=\left(  QR\right)  _{\bullet,j}%
\end{align*}
(by the definition of the product of two matrices\footnote{Actually, let's be
a bit more explicit here: The standard definition of the product of two
matrices yields%
\[
\left(  QR\right)  _{k,j}=\sum_{i=1}^{m}\underbrace{Q_{k,i}R_{i,j}}%
_{=R_{i,j}Q_{k,i}}=\sum_{i=1}^{m}R_{i,j}Q_{k,i}\ \ \ \ \ \ \ \ \ \ \text{for
each }k\in\left[  n\right]  .
\]
In other words, $\left(  QR\right)  _{\bullet,j}=\sum_{i=1}^{m}R_{i,j}%
Q_{\bullet,i}$, which is precisely what we are claiming.}). In other words,
$A=QR$.

Thus, we have found an isometry $Q\in\mathbb{C}^{n\times m}$ and an
upper-triangular matrix $R\in\mathbb{C}^{m\times m}$ such that $A=QR$. This
proves Theorem \ref{thm.unitary.QR1}.
\end{proof}

Note that there are other variants of QR factorization, such as the following one:

\begin{theorem}
[QR factorization, unitary version]\label{thm.unitary.QR2}Let $A\in
\mathbb{C}^{n\times m}$. Then, there exists a unitary matrix $Q\in
\mathbb{C}^{n\times n}$ and an upper-triangular matrix $R\in\mathbb{C}%
^{n\times m}$ such that $A=QR$. Here, a rectangular matrix $R\in
\mathbb{C}^{n\times m}$ is said to be \emph{upper-triangular} if and only if
it satisfies%
\[
R_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{for all }i>j.
\]

\end{theorem}

\begin{exercise}
\label{exe.unitary.QR2}\fbox{5} Prove Theorem \ref{thm.unitary.QR2}.

[\textbf{Hint:} Reduce both cases $n>m$ and $n<m$ to the case $n=m$.]
\end{exercise}

\section{Schur triangularization (\cite[Chapter 2]{HorJoh13})}

In this chapter, we will meet \emph{Schur triangularization}: a way to
transform an arbitrary $n\times n$-matrix with complex entries into an
upper-triangular matrix by conjugating it (i.e., multiplying it by an
invertible matrix $W$ on the left and simultaneously by its inverse $W^{-1}$
on the right). This is both of theoretical and of practical significance, but
we will focus on the theoretical applications.

Before we get to Schur triangularization, we will have to set some groundwork.\setcounter{subsection}{-1}

\subsection{Reminders on the characteristic polynomial and eigenvalues}

First, let us recall some properties of the characteristic polynomial of an
$n\times n$-matrix $A$, starting with its definition:

\begin{definition}
\label{def.schurtri.ch.pA}Let $\mathbb{F}$ be a field. Let $A\in
\mathbb{F}^{n\times n}$ be an $n\times n$-matrix over $\mathbb{F}$.

The \emph{characteristic polynomial} of $A$ is defined to be the polynomial%
\[
\det\left(  tI_{n}-A\right)  \ \ \ \ \ \ \ \ \ \ \text{in the indeterminate
}t\text{ with coefficients in }\mathbb{F}.
\]
(Note that $tI_{n}-A$ is an $n\times n$-matrix whose entries are polynomials
in $t$. Thus, its determinant $\det\left(  tI_{n}-A\right)  $ is itself a
polynomial in $t$.)

The characteristic polynomial of $A$ is denoted by $p_{A}$.
\end{definition}

\begin{example}
\label{exa.schurtri.ch.pA.2x2}Let $n=2$ and $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $. Then,%
\begin{align*}
tI_{n}-A  &  =tI_{2}-A=t\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  -\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
t & 0\\
0 & t
\end{array}
\right)  -\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
t-a & -b\\
-c & t-d
\end{array}
\right)  ,
\end{align*}
so that%
\begin{align*}
p_{A}  &  =\det\left(  tI_{n}-A\right)  =\det\left(
\begin{array}
[c]{cc}%
t-a & -b\\
-c & t-d
\end{array}
\right)  =\left(  t-a\right)  \left(  t-d\right)  -\left(  -b\right)  \left(
-c\right) \\
&  =t^{2}-\left(  a+d\right)  t+\left(  ad-bc\right)  .
\end{align*}

\end{example}

\begin{example}
\label{exa.schurtri.ch.pA.3x3}Let $n=3$ and $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
a^{\prime} & b^{\prime} & c^{\prime}\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime}%
\end{array}
\right)  $. Then,%
\begin{align*}
tI_{n}-A  &  =tI_{3}-A=t\left(
\begin{array}
[c]{ccc}%
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right)  -\left(
\begin{array}
[c]{ccc}%
a & b & c\\
a^{\prime} & b^{\prime} & c^{\prime}\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{ccc}%
t-a & -b & -c\\
-a^{\prime} & t-b^{\prime} & -c^{\prime}\\
-a^{\prime\prime} & -b^{\prime\prime} & t-c^{\prime\prime}%
\end{array}
\right)  ,
\end{align*}
so that%
\begin{align*}
p_{A}  &  =\det\left(  tI_{n}-A\right)  =\det\left(
\begin{array}
[c]{ccc}%
t-a & -b & -c\\
-a^{\prime} & t-b^{\prime} & -c^{\prime}\\
-a^{\prime\prime} & -b^{\prime\prime} & t-c^{\prime\prime}%
\end{array}
\right) \\
&  =t^{3}-\left(  a+b^{\prime}+c^{\prime\prime}\right)  t^{2}+\left(
ab^{\prime}-ba^{\prime}+ac^{\prime\prime}-ca^{\prime\prime}+b^{\prime
}c^{\prime\prime}-b^{\prime\prime}c^{\prime}\right)  t\\
&  \ \ \ \ \ \ \ \ \ \ -\left(  ab^{\prime}c^{\prime\prime}-ab^{\prime\prime
}c^{\prime}-ba^{\prime}c^{\prime\prime}+ba^{\prime\prime}c^{\prime}%
+ca^{\prime}b^{\prime\prime}-ca^{\prime\prime}b^{\prime}\right)  .
\end{align*}

\end{example}

\begin{example}
If $n=1$ and $A=\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)  $, then $tI_{n}-A=\left(
\begin{array}
[c]{c}%
t-a
\end{array}
\right)  $ and thus $p_{A}=t-a$.
\end{example}

\begin{example}
If $n=0$ and $A=\left(  {}\right)  $, then $tI_{n}-A=\left(  {}\right)  $ and
thus $p_{A}=1$ (since the determinant of the $0\times0$-matrix $\left(
{}\right)  $ is defined to be $1$).
\end{example}

\begin{remark}
\label{rmk.schurtri.ch.conventions}\textbf{(a)} Some authors define the
characteristic polynomial $p_{A}$ of an $n\times n$-matrix $A$ to be
$\det\left(  A-tI_{n}\right)  $ instead of $\det\left(  tI_{n}-A\right)  $.
This differs from our definition only by a factor of $\left(  -1\right)  ^{n}%
$, which is immaterial for most properties of the characteristic polynomial
but still can cause the occasional confusion. \medskip

\textbf{(b)} Some other common notations for $p_{A}$ are $\chi_{A}$ and
$c_{A}$.
\end{remark}

The patterns you might have spotted in Example \ref{exa.schurtri.ch.pA.2x2}
and in Example \ref{exa.schurtri.ch.pA.3x3} are not accidental. Indeed, the
coefficients of the characteristic polynomial of any square matrix can be
expressed explicitly, if you consider sums of determinants to be explicit:

\begin{proposition}
\label{prop.schurtri.ch.props}Let $\mathbb{F}$ be a field. Let $A\in
\mathbb{F}^{n\times n}$ be an $n\times n$-matrix over $\mathbb{F}$. \medskip

\textbf{(a)} The characteristic polynomial $p_{A}$ is a monic polynomial in
$t$ of degree $n$. (That is, its leading term is $t^{n}$.) \medskip

\textbf{(b)} The constant term of the polynomial $p_{A}$ is $\left(
-1\right)  ^{n}\det A$. \medskip

\textbf{(c)} The $t^{n-1}$-coefficient of the polynomial $p_{A}$ is
$-\operatorname*{Tr}A$. (Recall that $\operatorname*{Tr}A$ is defined to be
the sum of all diagonal entries of $A$; this sum is known as the \emph{trace}
of $A$.) \medskip

\textbf{(d)} More generally: Let $k\in\left\{  0,1,\ldots,n\right\}  $. Then,
the $t^{n-k}$-coefficient of the polynomial $p_{A}$ is $\left(  -1\right)
^{k}$ times the sum of all principal $k\times k$-minors of $A$. (Recall that a
$k\times k$\emph{-minor} of $A$ means the determinant of a $k\times
k$-submatrix of $A$. This $k\times k$-minor is said to be \emph{principal} if
the $k\times k$-submatrix is obtained by removing some $n-k$ rows and the
corresponding $n-k$ columns from $A$. For example, the principal $2\times
2$-minors of a $3\times3$-matrix $A$ are $\det\left(
\begin{array}
[c]{cc}%
A_{1,1} & A_{1,2}\\
A_{2,1} & A_{2,2}%
\end{array}
\right)  $, $\det\left(
\begin{array}
[c]{cc}%
A_{1,1} & A_{1,3}\\
A_{3,1} & A_{3,3}%
\end{array}
\right)  $ and $\det\left(
\begin{array}
[c]{cc}%
A_{2,2} & A_{2,3}\\
A_{3,2} & A_{3,3}%
\end{array}
\right)  $.) In other words, the $t^{n-k}$-coefficient of $p_{A}$ is
\[
\left(  -1\right)  ^{k}\sum_{1\leq i_{1}<i_{2}<\cdots<i_{k}\leq n}\det\left(
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}^{i_{1},i_{2}%
,\ldots,i_{k}}A\right)  ,
\]
where $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}^{i_{1}%
,i_{2},\ldots,i_{k}}A$ denotes the $k\times k$-matrix whose $\left(
u,v\right)  $-th entry is $A_{i_{u},i_{v}}$ for all $u,v\in\left[  k\right]  $.
\end{proposition}

\begin{proof}
[Proof sketch.]We have $A=\left(
\begin{array}
[c]{cccc}%
A_{1,1} & A_{1,2} & \cdots & A_{1,n}\\
A_{2,1} & A_{2,2} & \cdots & A_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
A_{n,1} & A_{n,2} & \cdots & A_{n,n}%
\end{array}
\right)  $, so that%
\begin{align*}
tI_{n}-A  &  =t\left(
\begin{array}
[c]{cccc}%
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  -\left(
\begin{array}
[c]{cccc}%
A_{1,1} & A_{1,2} & \cdots & A_{1,n}\\
A_{2,1} & A_{2,2} & \cdots & A_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
A_{n,1} & A_{n,2} & \cdots & A_{n,n}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
t-A_{1,1} & -A_{1,2} & \cdots & -A_{1,n}\\
-A_{2,1} & t-A_{2,2} & \cdots & -A_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
-A_{n,1} & -A_{n,2} & \cdots & t-A_{n,n}%
\end{array}
\right)  .
\end{align*}
The determinant $\det\left(  tI_{n}-A\right)  $ of this matrix is a sum of
certain products of its entries\footnote{Here, we are using the \emph{Leibniz
formula} for the determinant of a matrix, which says that $\det B=\sum
_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}B_{1,\sigma\left(  1\right)
}B_{2,\sigma\left(  2\right)  }\cdots B_{n,\sigma\left(  n\right)  }$ for each
$n\times n$-matrix $B$. (We are applying this to $B=tI_{n}-A$.)}. One of these
products is%
\begin{align*}
&  \left(  t-A_{1,1}\right)  \left(  t-A_{2,2}\right)  \cdots\left(
t-A_{n,n}\right) \\
&  =t^{n}-\left(  A_{1,1}+A_{2,2}+\cdots+A_{n,n}\right)  t^{n-1}+\left(
\text{terms with lower powers of }t\right)  .
\end{align*}
None of the other products appearing in this sum includes any power of $t$
higher than $t^{n-2}$ (because the product picks out at least two entries of
$A$ that lie outside of the main diagonal, and thus contain no $t$ whatsoever;
the remaining factors of the product contribute at most $t^{n-2}$). Hence, the
entire determinant $\det\left(  tI_{n}-A\right)  $ can be written as
\[
\det\left(  tI_{n}-A\right)  =t^{n}-\left(  A_{1,1}+A_{2,2}+\cdots
+A_{n,n}\right)  t^{n-1}+\left(  \text{terms with lower powers of }t\right)
.
\]
In other words,%
\[
p_{A}=t^{n}-\left(  A_{1,1}+A_{2,2}+\cdots+A_{n,n}\right)  t^{n-1}+\left(
\text{terms with lower powers of }t\right)
\]
(since $p_{A}=\det\left(  tI_{n}-A\right)  $). This yields parts \textbf{(a)}
and \textbf{(c)} of Proposition \ref{prop.schurtri.ch.props}.

To prove Proposition \ref{prop.schurtri.ch.props} \textbf{(b)}, we substitute
$0$ for $t$ in the polynomial identity $p_{A}=\det\left(  tI_{n}-A\right)  $.
We obtain%
\[
p_{A}\left(  0\right)  =\det\left(  0I_{n}-A\right)  =\det\left(  -A\right)
=\left(  -1\right)  ^{n}\det A.
\]
Since $p_{A}\left(  0\right)  $ is the constant term of $p_{A}$ (in fact, if
$f$ is any polynomial, then $f\left(  0\right)  $ is the constant term of
$f$), we thus conclude that the constant term of $p_{A}$ is $\left(
-1\right)  ^{n}\det A$. This proves Proposition \ref{prop.schurtri.ch.props}
\textbf{(b).}

Finally, Proposition \ref{prop.schurtri.ch.props} \textbf{(d)} can be
established through a more accurate combinatorial analysis of the products
that sum up to $\det\left(  tI_{n}-A\right)  $. See \cite[Proposition
6.4.29]{21s} for the details. (A combinatorially prepared reader might glean
the idea from Example \ref{exa.schurtri.ch.pA.3x3}.)

We note that parts \textbf{(a)}, \textbf{(b)} and \textbf{(c)} of Proposition
\ref{prop.schurtri.ch.props} can all be derived from part \textbf{(d)} as well.
\end{proof}

Next, we recall some basic notions around the eigenvalues of a matrix:

\begin{definition}
\label{def.schurtri.ch.evals}Let $\mathbb{F}$ be a field. Let $A\in
\mathbb{F}^{n\times n}$ be an $n\times n$-matrix, and let $\lambda
\in\mathbb{F}$. \medskip

\textbf{(a)} We say that $\lambda$ is an \emph{eigenvalue} of $A$ if and only
if $\det\left(  \lambda I_{n}-A\right)  =0$. In other words, $\lambda$ is an
\emph{eigenvalue} of $A$ if and only if $\lambda$ is a root of the
characteristic polynomial $p_{A}=\det\left(  tI_{n}-A\right)  $. \medskip

\textbf{(b)} The $\lambda$\emph{-eigenspace} of $A$ is defined to be the set
of all vectors $v\in\mathbb{F}^{n}$ satisfying $Av=\lambda v$. In other words,
it is the kernel $\operatorname*{Ker}\left(  \lambda I_{n}-A\right)
=\operatorname*{Ker}\left(  A-\lambda I_{n}\right)  $. Thus, it is a vector
subspace of $\mathbb{F}^{n}$. The elements of this $\lambda$-eigenspace are
called the $\lambda$\emph{-eigenvectors} of $A$ (or the \emph{eigenvectors} of
$A$ for eigenvalue $\lambda$). (Some authors exclude the zero vector $0$ from
being an eigenvector; we allow it. Thus, $0$ is a $\lambda$-eigenvector for
any $\lambda$, even if $\lambda$ is not an eigenvalue.) \medskip

\textbf{(c)} The \emph{algebraic multiplicity} of $\lambda$ as an eigenvalue
of $A$ is defined to be the multiplicity of $\lambda$ as a root of $p_{A}$.
(If $\lambda$ is not an eigenvalue of $A$, then this is $0$.) \medskip

\textbf{(d)} The \emph{geometric multiplicity} of $\lambda$ as an eigenvalue
of $A$ is defined to be $\dim\left(  \operatorname*{Ker}\left(  A-\lambda
I_{n}\right)  \right)  $. In other words, it is the dimension of the $\lambda
$-eigenspace of $A$. In other words, it is the maximum number of linearly
independent $\lambda$-eigenvectors. (If $\lambda$ is not an eigenvalue of $A$,
then this is $0$.)
\end{definition}

It can be shown that if $A\in\mathbb{F}^{n\times n}$ is a matrix and
$\lambda\in\mathbb{F}$ is arbitrary, then the geometric multiplicity of
$\lambda$ as an eigenvalue of $A$ is always $\leq$ to the algebraic
multiplicity of $\lambda$ as an eigenvalue of $A$. The two multiplicities can
be equal, but don't have to be.

\begin{example}
Let $A=\left(
\begin{array}
[c]{ccc}%
1 & 0 & 0\\
0 & 1 & 2\\
0 & 0 & 1
\end{array}
\right)  \in\mathbb{C}^{3\times3}$. Then, the only eigenvalue of $A$ is $1$.
Its algebraic multiplicity is $3$, while its geometric multiplicity is $2$.
\end{example}

\begin{theorem}
\label{thm.schurtri.ch.fta-cons}Let $A\in\mathbb{C}^{n\times n}$ be an
$n\times n$-matrix with complex entries. Then:

\textbf{(a)} Its characteristic polynomial $p_{A}$ factors into $n$ linear
terms:%
\begin{equation}
p_{A}=\left(  t-\lambda_{1}\right)  \left(  t-\lambda_{2}\right)
\cdots\left(  t-\lambda_{n}\right)  , \label{eq.schurtri.ch.pA-factors}%
\end{equation}
where $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\in\mathbb{C}$ are its roots
(with their algebraic multiplicities). \medskip

\textbf{(b)} These roots $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ are the
eigenvalues of $A$, appearing with their algebraic multiplicities. \medskip

\textbf{(c)} The sum of the algebraic multiplicities of all eigenvalues of $A$
is $n$. \medskip

\textbf{(d)} The sum of all eigenvalues of $A$ (with their algebraic
multiplicities) is $\operatorname*{Tr}A$ (that is, the trace of $A$). \medskip

\textbf{(e)} The product of all eigenvalues of $A$ (with their algebraic
multiplicities) is $\det A$. \medskip

\textbf{(f)} If $n>0$, then the matrix $A$ has at least one eigenvalue and at
least one nonzero eigenvector.
\end{theorem}

\begin{proof}
The polynomial $p_{A}$ is a monic polynomial of degree $n$ (by Proposition
\ref{prop.schurtri.ch.props} \textbf{(a)}), and therefore factors into linear
terms (by
\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{the
Fundamental Theorem of Algebra}). This proves Theorem
\ref{thm.schurtri.ch.fta-cons} \textbf{(a)}. \medskip

\textbf{(b)} This follows from the definition of eigenvalues and algebraic
multiplicities. \medskip

\textbf{(c)} This follows from part \textbf{(b)}. \medskip

\textbf{(d)} Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ be the roots of
$p_{A}$ (with their algebraic multiplicities). Then, these roots are the
eigenvalues of $A$, appearing with their algebraic multiplicities (by Theorem
\ref{thm.schurtri.ch.fta-cons} \textbf{(b)}). Hence, their sum $\lambda
_{1}+\lambda_{2}+\cdots+\lambda_{n}$ is the sum of the eigenvalues of $A$
(with their algebraic multiplicities). On the other hand, we know from Theorem
\ref{thm.schurtri.ch.fta-cons} \textbf{(a)} that the equality
(\ref{eq.schurtri.ch.pA-factors}) holds. Comparing the coefficients of
$t^{n-1}$ on both sides of this equality, we obtain%
\begin{align*}
\left(  \text{the coefficient of }t^{n-1}\text{ in }p_{A}\right)   &  =\left(
\text{the coefficient of }t^{n-1}\text{ in }\left(  t-\lambda_{1}\right)
\left(  t-\lambda_{2}\right)  \cdots\left(  t-\lambda_{n}\right)  \right) \\
&  =-\left(  \lambda_{1}+\lambda_{2}+\cdots+\lambda_{n}\right)  .
\end{align*}
However, Proposition \ref{prop.schurtri.ch.props} \textbf{(c)} yields
\[
\left(  \text{the coefficient of }t^{n-1}\text{ in }p_{A}\right)
=-\operatorname*{Tr}A.
\]
Comparing these two equalities, we obtain $-\left(  \lambda_{1}+\lambda
_{2}+\cdots+\lambda_{n}\right)  =-\operatorname*{Tr}A$. In other words,
$\lambda_{1}+\lambda_{2}+\cdots+\lambda_{n}=\operatorname*{Tr}A$. This proves
Theorem \ref{thm.schurtri.ch.fta-cons} \textbf{(d)} (since $\lambda
_{1}+\lambda_{2}+\cdots+\lambda_{n}$ is the sum of the eigenvalues of $A$).
\medskip

\textbf{(e)} This is similar to part \textbf{(d)}, except that we have to
compare the coefficients of $t^{0}$ (instead of $t^{n-1}$) on both sides of
(\ref{eq.schurtri.ch.pA-factors}), and we have to use Proposition
\ref{prop.schurtri.ch.props} \textbf{(b)} (instead of Proposition
\ref{prop.schurtri.ch.props} \textbf{(c)}).\medskip

\textbf{(f)} Assume that $n>0$. Thus, $n\geq1$. However, Theorem
\ref{thm.schurtri.ch.fta-cons} \textbf{(b)} shows that $A$ has exactly $n$
eigenvalues, counted with algebraic multiplicities. Hence, $A$ has at least
one eigenvalue $\lambda$ (since $n\geq1$). Consider this $\lambda$. Since
$\lambda$ is an eigenvalue of $A$, we have $\det\left(  \lambda I_{n}%
-A\right)  =0$. Hence, the $n\times n$-matrix $\lambda I_{n}-A$ is singular,
so that its kernel $\operatorname*{Ker}\left(  \lambda I_{n}-A\right)  $ is
nonzero. In other words, there exists a nonzero vector $v\in
\operatorname*{Ker}\left(  \lambda I_{n}-A\right)  $. This vector $v$ must be
a $\lambda$-eigenvector of $A$ (since $v\in\operatorname*{Ker}\left(  \lambda
I_{n}-A\right)  $). Hence, the matrix $A$ has a nonzero eigenvector (namely,
$v$). This completes the proof of Theorem \ref{thm.schurtri.ch.fta-cons}
\textbf{(f)}.
\end{proof}

\begin{noncompile}%
\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Old lecture 2 starts here.}\\\hline\hline
\end{tabular}
\
\]

\end{noncompile}

\subsection{\label{sec.schur.similar}Similarity of matrices}

Next, let us recall the notion of similar matrices:

\begin{definition}
\label{def.schurtri.similar.def}Let $\mathbb{F}$ be a field. Let $A$ and $B$
be two matrices in $\mathbb{F}^{n\times n}$. We say that $A$ is \emph{similar}
to $B$ if there exists an invertible matrix $W\in\mathbb{F}^{n\times n}$ such
that $B=WAW^{-1}$.

We write \textquotedblleft$A\sim B$\textquotedblright\ for \textquotedblleft%
$A$ is similar to $B$\textquotedblright.
\end{definition}

\begin{example}
The matrix $\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 1
\end{array}
\right)  $ is similar to the matrix $\left(
\begin{array}
[c]{cc}%
2 & 0\\
0 & 0
\end{array}
\right)  $, since $\left(
\begin{array}
[c]{cc}%
2 & 0\\
0 & 0
\end{array}
\right)  =W\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 1
\end{array}
\right)  W^{-1}$ for the invertible matrix $W=\left(
\begin{array}
[c]{cc}%
1 & 1\\
-1 & 1
\end{array}
\right)  $. In other words, we have $\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 1
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cc}%
2 & 0\\
0 & 0
\end{array}
\right)  $.
\end{example}

The relation $\sim$\ is easily seen to be an equivalence
relation:\footnote{Algebraists will recognize the relation $\sim$\ (for
matrices in $\mathbb{F}^{n\times n}$) as just being the conjugacy relation in
the ring $\mathbb{F}^{n\times n}$ of all $n\times n$-matrices. (The meaning of
the word \textquotedblleft conjugacy\textquotedblright\ here has nothing to do
with conjugates of complex numbers or with the conjugate transpose!)}

\begin{proposition}
\label{prop.schurtri.similar.eqrel}Let $\mathbb{F}$ be a field. Then: \medskip

\textbf{(a)} Any matrix $A\in\mathbb{F}^{n\times n}$ is similar to itself.
\medskip

\textbf{(b)} If $A$ and $B$ are two matrices in $\mathbb{F}^{n\times n}$ such
that $A$ is similar to $B$, then $B$ is similar to $A$. \medskip

\textbf{(c)} If $A$, $B$ and $C$ are three matrices in $\mathbb{F}^{n\times
n}$ such that $A$ is similar to $B$ and such that $B$ is similar to $C$, then
$A$ is similar to $C$.
\end{proposition}

\begin{proof}
\textbf{(a)} This follows from $A=I_{n}AI_{n}^{-1}$. \medskip

\textbf{(b)} Let $A$ and $B$ be two matrices in $\mathbb{F}^{n\times n}$ such
that $A$ is similar to $B$. Thus, there exists an invertible matrix
$W\in\mathbb{F}^{n\times n}$ such that $B=WAW^{-1}$. Consider this $W$. From
$B=WAW^{-1}$, we obtain $BW=WA$, so that $W^{-1}BW=A$. Thus, $A=W^{-1}%
B\underbrace{W}_{=\left(  W^{-1}\right)  ^{-1}}=W^{-1}B\left(  W^{-1}\right)
^{-1}$. Since $W^{-1}$ is invertible, this shows that $B$ is similar to $A$.
This proves Proposition \ref{prop.schurtri.similar.eqrel} \textbf{(b)}.
\medskip

\textbf{(c)} Let $A$, $B$ and $C$ be three matrices in $\mathbb{F}^{n\times
n}$ such that $A$ is similar to $B$ and such that $B$ is similar to $C$. Thus,
there exists an invertible matrix $U\in\mathbb{F}^{n\times n}$ such that
$B=UAU^{-1}$ (since $A$ is similar to $B$), and there exists an invertible
matrix $V\in\mathbb{F}^{n\times n}$ such that $C=VBV^{-1}$ (since $B$ is
similar to $C$). Consider these $U$ and $V$.

The matrices $V$ and $U$ are invertible. Thus, so is their product $VU$, and
its inverse is $\left(  VU\right)  ^{-1}=U^{-1}V^{-1}$. (This is the famous
\textquotedblleft socks-and-shoes rule\textquotedblright\ for inverting
products or compositions.) Now,%
\[
C=V\underbrace{B}_{=UAU^{-1}}V^{-1}=VUA\underbrace{U^{-1}V^{-1}}_{=\left(
VU\right)  ^{-1}}=VUA\left(  VU\right)  ^{-1}.
\]
In other words, $C=WAW^{-1}$ for the invertible matrix $W=VU$ (since we know
that $VU$ is invertible). This shows that $A$ is similar to $C$. This proves
Proposition \ref{prop.schurtri.similar.eqrel} \textbf{(c)}.
\end{proof}

Since the relation $\sim$\ is symmetric (by Proposition
\ref{prop.schurtri.similar.eqrel} \textbf{(b)}), we can make the following definition:

\begin{definition}
Let $\mathbb{F}$ be a field. Let $A$ and $B$ be two matrices in $\mathbb{F}%
^{n\times n}$. We say that $A$ and $B$ are \emph{similar} if $A$ is similar to
$B$ (or, equivalently, $B$ is similar to $A$).
\end{definition}

Similar matrices have a lot in common. Here is a selection of invariants:

\begin{proposition}
\label{prop.schurtri.similar.same}Let $\mathbb{F}$ be a field. Let
$A\in\mathbb{F}^{n\times n}$ and $B\in\mathbb{F}^{n\times n}$ be two similar
matrices. Then: \medskip

\textbf{(a)} The matrices $A$ and $B$ have the same rank. \medskip

\textbf{(b)} The matrices $A$ and $B$ have the same nullity. \medskip

\textbf{(c)} The matrices $A$ and $B$ have the same determinant. \medskip

\textbf{(d)} The matrices $A$ and $B$ have the same characteristic polynomial.
\medskip

\textbf{(e)} The matrices $A$ and $B$ have the same eigenvalues, with the same
algebraic multiplicities and with the same geometric multiplicities. \medskip

\textbf{(f)} For any $k\in\mathbb{N}$, the matrix $A^{k}$ is similar to
$B^{k}$. \medskip

\textbf{(g)} For any $\lambda\in\mathbb{F}$, the matrix $\lambda I_{n}-A$ is
similar to $\lambda I_{n}-B$.\medskip

\textbf{(h)} For any $\lambda\in\mathbb{F}$, the matrix $A-\lambda I_{n}$ is
similar to $B-\lambda I_{n}$.
\end{proposition}

\begin{proof}
Since $A$ is similar to $B$, there exists an invertible matrix $W\in
\mathbb{F}^{n\times n}$ such that $B=WAW^{-1}$. Consider this $W$. \medskip

\textbf{(b)} Consider the kernels\footnote{Recall that \textquotedblleft
kernel\textquotedblright\ is a synonym for \textquotedblleft
nullspace\textquotedblright.} $\operatorname*{Ker}A$ and $\operatorname*{Ker}%
B$ of $A$ and $B$. For any $v\in\operatorname*{Ker}A$, we have $Wv\in
\operatorname*{Ker}B$ (because $v\in\operatorname*{Ker}A$ implies $Av=0$, so
that $\underbrace{B}_{=WAW^{-1}}Wv=WA\underbrace{W^{-1}W}_{=I_{n}%
}v=W\underbrace{Av}_{=0}=0$ and therefore $Wv\in\operatorname*{Ker}B$). Thus,
we have found a linear map%
\begin{align*}
\operatorname*{Ker}A  &  \rightarrow\operatorname*{Ker}B,\\
v  &  \mapsto Wv.
\end{align*}
This linear map is furthermore injective (because $W$ is invertible, so that
$Wu=Wv$ entails $u=v$). Hence, we obtain $\dim\left(  \operatorname*{Ker}%
A\right)  \leq\dim\left(  \operatorname*{Ker}B\right)  $. But $A$ and $B$ play
symmetric roles in our situation (since the relation \textquotedblleft
similar\textquotedblright\ is symmetric), so that we can use the same
reasoning to obtain $\dim\left(  \operatorname*{Ker}B\right)  \leq\dim\left(
\operatorname*{Ker}A\right)  $. Combining these two inequalities, we obtain
$\dim\left(  \operatorname*{Ker}A\right)  =\dim\left(  \operatorname*{Ker}%
B\right)  $. In other words, $A$ and $B$ have the same nullity. This proves
Proposition \ref{prop.schurtri.similar.same} \textbf{(b)}. \medskip

\textbf{(a)} The rank of an $n\times n$-matrix equals $n$ minus its nullity
(by the rank-nullity theorem). Hence, two $n\times n$-matrices that have the
same nullity must also have the same rank. Thus, Proposition
\ref{prop.schurtri.similar.same} \textbf{(a)} follows from Proposition
\ref{prop.schurtri.similar.same} \textbf{(b)}. \medskip

\textbf{(c)} From $B=WAW^{-1}$, we obtain%
\begin{align*}
\det B  &  =\det\left(  WAW^{-1}\right)  =\det W\cdot\det A\cdot
\underbrace{\det\left(  W^{-1}\right)  }_{=\left(  \det W\right)  ^{-1}}\\
&  =\det W\cdot\det A\cdot\left(  \det W\right)  ^{-1}=\det A.
\end{align*}
This proves Proposition \ref{prop.schurtri.similar.same} \textbf{(c)}.
\medskip

\textbf{(d)} The characteristic polynomial of an $n\times n$-matrix $M$ is
defined to be $\det\left(  tI_{n}-M\right)  $ (where $t$ is the
indeterminate)\footnote{At least this is our definition. As we already
mentioned in Remark \ref{rmk.schurtri.ch.conventions} \textbf{(a)}, another
popular definition is $\det\left(  M-tI_{n}\right)  $. However, the two
definitions differ only in a factor of $\left(  -1\right)  ^{n}$, so they
behave almost completely the same (and our argument works equally well for
either of them).}. Thus, we must show that $\det\left(  tI_{n}-A\right)
=\det\left(  tI_{n}-B\right)  $. However, we have%
\begin{align*}
t\underbrace{I_{n}}_{\substack{=WW^{-1}\\=WI_{n}W^{-1}}}-\underbrace{B}%
_{=WAW^{-1}}  &  =\underbrace{tWI_{n}}_{=W\left(  tI_{n}\right)  }%
W^{-1}-WAW^{-1}=W\left(  tI_{n}\right)  W^{-1}-WAW^{-1}\\
&  =W\left(  tI_{n}-A\right)  W^{-1}.
\end{align*}
Thus,%
\begin{align*}
\det\left(  tI_{n}-B\right)   &  =\det\left(  W\left(  tI_{n}-A\right)
W^{-1}\right)  =\det W\cdot\det\left(  tI_{n}-A\right)  \cdot\underbrace{\det
\left(  W^{-1}\right)  }_{=\left(  \det W\right)  ^{-1}}\\
&  =\det W\cdot\det\left(  tI_{n}-A\right)  \cdot\left(  \det W\right)
^{-1}=\det\left(  tI_{n}-A\right)  .
\end{align*}
Thus, $\det\left(  tI_{n}-A\right)  =\det\left(  tI_{n}-B\right)  $, and
Proposition \ref{prop.schurtri.similar.same} \textbf{(d)} is proven. \medskip

\textbf{(e)} The eigenvalues of a matrix, with their algebraic multiplicities,
are the roots of the characteristic polynomial. Thus, from Proposition
\ref{prop.schurtri.similar.same} \textbf{(d)}, we see that the matrices $A$
and $B$ have the same eigenvalues, with the same algebraic multiplicities. It
remains to show that the geometric multiplicities are also the same.

Let $\lambda$ be an eigenvalue of $A$ (and therefore also of $B$, as we have
just seen). The geometric multiplicity of $\lambda$ as an eigenvalue of $A$ is
$\dim\left(  \operatorname*{Ker}\left(  A-\lambda I_{n}\right)  \right)  $.
Likewise, the geometric multiplicity of $\lambda$ as an eigenvalue of $B$ is
$\dim\left(  \operatorname*{Ker}\left(  B-\lambda I_{n}\right)  \right)  $.
Hence, we must show that $\dim\left(  \operatorname*{Ker}\left(  A-\lambda
I_{n}\right)  \right)  =\dim\left(  \operatorname*{Ker}\left(  B-\lambda
I_{n}\right)  \right)  $.

We have%
\begin{align*}
\underbrace{B}_{=WAW^{-1}}-\lambda\underbrace{I_{n}}_{\substack{=WW^{-1}%
\\=WI_{n}W^{-1}}}  &  =WAW^{-1}-\underbrace{\lambda WI_{n}}_{=W\left(  \lambda
I_{n}\right)  }W^{-1}=WAW^{-1}-W\left(  \lambda I_{n}\right)  W^{-1}\\
&  =W\left(  A-\lambda I_{n}\right)  W^{-1}.
\end{align*}
This shows that the matrices $A-\lambda I_{n}$ and $B-\lambda I_{n}$ are
similar. Hence, Proposition \ref{prop.schurtri.similar.same} \textbf{(b)}
shows that these two matrices $A-\lambda I_{n}$ and $B-\lambda I_{n}$ have the
same nullity. In other words, $\dim\left(  \operatorname*{Ker}\left(
A-\lambda I_{n}\right)  \right)  =\dim\left(  \operatorname*{Ker}\left(
B-\lambda I_{n}\right)  \right)  $. This is exactly what we needed to show;
thus, Proposition \ref{prop.schurtri.similar.same} \textbf{(e)} is proven.
\medskip

\textbf{(f)} Let $k\in\mathbb{N}$. We claim that $B^{k}=WA^{k}W^{-1}$. Once
this is proved, it will clearly follow that $A^{k}$ is similar to $B^{k}$.

One way to prove $B^{k}=WA^{k}W^{-1}$ is as follows: From $B=WAW^{-1}$, we
obtain%
\begin{align*}
B^{k}  &  =\left(  WAW^{-1}\right)  ^{k}=WA\underbrace{W^{-1}\cdot W}_{=I_{n}%
}A\underbrace{W^{-1}\cdot W}_{=I_{n}}AW^{-1}\cdot\cdots\cdot
WA\underbrace{W^{-1}\cdot W}_{=I_{n}}AW^{-1}\\
&  =W\underbrace{AA\cdots A}_{k\text{ factors}}W^{-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since all the }W^{-1}\cdot W\text{'s in the
middle cancel out}\right) \\
&  =WA^{k}W^{-1}.
\end{align*}
(To be precise, this works for $k\geq1$; but the case $k=0$ is trivial.)

A less handwavy proof of $B^{k}=WA^{k}W^{-1}$ would proceed by induction on
$k$. As it is completely straightforward, I leave it to the reader. \medskip

\textbf{(g)} Let $\lambda\in\mathbb{F}$. Then,
\begin{align*}
\lambda\underbrace{I_{n}}_{\substack{=WW^{-1}\\=WI_{n}W^{-1}}}-\underbrace{B}%
_{=WAW^{-1}}  &  =\underbrace{\lambda WI_{n}}_{=W\left(  \lambda I_{n}\right)
}W^{-1}-WAW^{-1}=W\left(  \lambda I_{n}\right)  W^{-1}-WAW^{-1}\\
&  =W\left(  \lambda I_{n}-A\right)  W^{-1}.
\end{align*}
This shows that the matrices $\lambda I_{n}-A$ and $\lambda I_{n}-B$ are
similar. Thus, Proposition \ref{prop.schurtri.similar.same} \textbf{(g)} is
proven.\medskip

\textbf{(h)} This differs from part \textbf{(g)} only in that the subtrahend
and the minuend trade places. The proof is entirely analogous to part
\textbf{(g)}.
\end{proof}

Note that neither part \textbf{(a)}, nor part \textbf{(b)}, nor part
\textbf{(c)}, nor part \textbf{(d)}, nor part \textbf{(e)} of Proposition
\ref{prop.schurtri.similar.same} is an \textquotedblleft if and only
if\textquotedblright\ statement: One can find two $n\times n$-matrices (for
sufficiently large $n$) that have the same rank, nullity, determinant,
characteristic polynomial and eigenvalues but are not similar.\footnote{Some
of these examples are easy to find: For example, the matrices $\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & 0
\end{array}
\right)  $ have the same eigenvalues with the same algebraic multiplicities,
but are not similar.} Thus, proving the similarity of two matrices is not as
easy as comparing these data. We will later learn an algorithmic way to check
whether two matrices are similar.

\begin{exercise}
\label{exe.schurtri.similar.two-4x4s}\fbox{2} Prove that the two matrices
$\left(
\begin{array}
[c]{cccc}%
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cccc}%
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  $ are not similar.
\end{exercise}

\begin{exercise}
\label{exe.schurtri.similar.unitary-inv}\fbox{3} Let $A\in\mathbb{C}^{n\times
n}$ be a matrix that is similar to some unitary matrix. Prove that $A^{-1}$ is
similar to $A^{\ast}$.
\end{exercise}

\begin{remark}
If you are used to thinking of matrices as linear maps, then similarity is a
rather natural concept: Two $n\times n$-matrices $A\in\mathbb{F}^{n\times n}$
and $B\in\mathbb{F}^{n\times n}$ are similar if and only if they represent one
and the same endomorphism $f:\mathbb{F}^{n}\rightarrow\mathbb{F}^{n}$ of
$\mathbb{F}^{n}$ with respect to two (possibly different) bases of
$\mathbb{F}^{n}$. To be more precise, $A$ has to represent $f$ with respect to
some basis of $\mathbb{F}^{n}$, while $B$ has to represent $f$ with respect to
a further basis of $\mathbb{F}^{n}$ (possibly the same, but usually not).

This fact is not hard to prove. Indeed, if $A$ and $B$ represent the same
endomorphism $f$ with respect to two bases of $\mathbb{F}^{n}$, then we have
$B=WAW^{-1}$, where $W$ is the change-of-basis matrix between these two bases.
Conversely, if $A$ and $B$ are similar, then there exists some invertible
matrix $W$ satisfying $B=WAW^{-1}$, and then $A$ and $B$ represent the same
endomorphism $f$ with respect to two bases of $\mathbb{F}^{n}$ (namely, $B$
represents the endomorphism%
\begin{align*}
\mathbb{F}^{n}  &  \rightarrow\mathbb{F}^{n},\\
v  &  \mapsto Bv
\end{align*}
with respect to the standard basis $\left(  e_{1},e_{2},\ldots,e_{n}\right)
$, whereas $A$ represents the same endomorphism with respect to the basis
$\left(  We_{1},We_{2},\ldots,We_{n}\right)  $).

Knowing this fact, many properties of similar matrices -- including all parts
of Proposition \ref{prop.schurtri.similar.same} -- become essentially trivial:
One just needs to recall that things like rank, nullity, determinant,
eigenvalues etc. are properties of the endomorphism rather than properties of
the matrix.
\end{remark}

Two diagonal matrices are similar whenever they have the same diagonal entries
up to order. In other words:

\begin{proposition}
\label{prop.schurtri.similar.diag}Let $\mathbb{F}$ be a field. Let
$n\in\mathbb{N}$. Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\in
\mathbb{F}$. Let $\sigma$ be a permutation of $\left[  n\right]  $ (that is, a
bijective map from $\left[  n\right]  $ to $\left[  n\right]  $). Then,%
\[
\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)
\sim\operatorname*{diag}\left(  \lambda_{\sigma\left(  1\right)  }%
,\lambda_{\sigma\left(  2\right)  },\ldots,\lambda_{\sigma\left(  n\right)
}\right)  .
\]

\end{proposition}

\begin{example}
For $n=3$, Proposition \ref{prop.schurtri.similar.diag} claims that
$\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\lambda_{3}\right)
\sim\operatorname*{diag}\left(  \lambda_{\sigma\left(  1\right)  }%
,\lambda_{\sigma\left(  2\right)  },\lambda_{\sigma\left(  3\right)  }\right)
$. For example, if $\sigma$ is the permutation of $\left[  3\right]  $ that
sends $1,2,3$ to $2,3,1$, respectively, then this is saying that
$\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\lambda_{3}\right)
\sim\operatorname*{diag}\left(  \lambda_{2},\lambda_{3},\lambda_{1}\right)  $.
In other words,%
\[
\left(
\begin{array}
[c]{ccc}%
\lambda_{1} & 0 & 0\\
0 & \lambda_{2} & 0\\
0 & 0 & \lambda_{3}%
\end{array}
\right)  \sim\left(
\begin{array}
[c]{ccc}%
\lambda_{3} & 0 & 0\\
0 & \lambda_{1} & 0\\
0 & 0 & \lambda_{2}%
\end{array}
\right)  .
\]

\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.schurtri.similar.diag}.]Let $P_{\sigma}%
\in\mathbb{F}^{n\times n}$ be the permutation matrix of $\sigma$ (defined in
Example \ref{exa.unitary.unitary.exas} \textbf{(d)}). We recall that the
$\left(  i,j\right)  $-th entry of this matrix $P_{\sigma}$ is $%
\begin{cases}
\lambda_{i}, & \text{if }i=\sigma\left(  j\right)  ;\\
0, & \text{if }i\neq\sigma\left(  j\right)
\end{cases}
$ for any $i,j\in\left[  n\right]  $.

Now, it is easy to see that%
\begin{equation}
\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)
\cdot P_{\sigma}=P_{\sigma}\cdot\operatorname*{diag}\left(  \lambda
_{\sigma\left(  1\right)  },\lambda_{\sigma\left(  2\right)  },\ldots
,\lambda_{\sigma\left(  n\right)  }\right)  .
\label{pf.prop.schurtri.similar.diag.1}%
\end{equation}


[\textit{Proof of (\ref{pf.prop.schurtri.similar.diag.1}):} It is
straightforward to see that for any $i,j\in\left[  n\right]  $, both matrices
$\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\ldots,\lambda
_{n}\right)  \cdot P_{\sigma}$ and $P_{\sigma}\cdot\operatorname*{diag}\left(
\lambda_{\sigma\left(  1\right)  },\lambda_{\sigma\left(  2\right)  }%
,\ldots,\lambda_{\sigma\left(  n\right)  }\right)  $ have the same $\left(
i,j\right)  $-th entry, namely $%
\begin{cases}
\lambda_{i}, & \text{if }i=\sigma\left(  j\right)  ;\\
0, & \text{if }i\neq\sigma\left(  j\right)  .
\end{cases}
$\ \ Thus, these two matrices are equal. This proves
(\ref{pf.prop.schurtri.similar.diag.1}).] \medskip

Since the permutation matrix $P_{\sigma}$ is invertible, we can multiply both
sides of (\ref{pf.prop.schurtri.similar.diag.1}) by $P_{\sigma}^{-1}$ from the
right, and thus we obtain
\[
\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)
=P_{\sigma}\cdot\operatorname*{diag}\left(  \lambda_{\sigma\left(  1\right)
},\lambda_{\sigma\left(  2\right)  },\ldots,\lambda_{\sigma\left(  n\right)
}\right)  \cdot P_{\sigma}^{-1}.
\]
This shows that $\operatorname*{diag}\left(  \lambda_{1},\lambda_{2}%
,\ldots,\lambda_{n}\right)  \sim\operatorname*{diag}\left(  \lambda
_{\sigma\left(  1\right)  },\lambda_{\sigma\left(  2\right)  },\ldots
,\lambda_{\sigma\left(  n\right)  }\right)  $. Thus, Proposition
\ref{prop.schurtri.similar.diag} is proven.
\end{proof}

Proposition \ref{prop.schurtri.similar.diag} actually has a converse: If two
diagonal matrices are similar, then they have the same diagonal entries up to
order. This follows easily from Proposition \ref{prop.schurtri.similar.same}
\textbf{(e)}, because the diagonal entries of a diagonal matrix are its
eigenvalues (with their algebraic multiplicities).

An analogue of Proposition \ref{prop.schurtri.similar.diag} holds for
block-diagonal matrices:

\begin{proposition}
\label{prop.schurtri.similar.block-diag}Let $\mathbb{F}$ be a field. Let
$n\in\mathbb{N}$. For each $i\in\left[  n\right]  $, let $A_{i}$ be an
$n_{i}\times n_{i}$-matrix (for some $n_{i}\in\mathbb{N}$). Let $\sigma$ be a
permutation of $\left[  n\right]  $ (that is, a bijective map from $\left[
n\right]  $ to $\left[  n\right]  $). Then,%
\[
\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{n}%
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cccc}%
A_{\sigma\left(  1\right)  } & 0 & \cdots & 0\\
0 & A_{\sigma\left(  2\right)  } & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{\sigma\left(  n\right)  }%
\end{array}
\right)  .
\]

\end{proposition}

\begin{proof}
This can be proved similarly to how we proved Proposition
\ref{prop.schurtri.similar.diag}, except that now, instead of the permutation
matrix $P_{\sigma}$, we need to use a \textquotedblleft block permutation
matrix\textquotedblright\ $\mathbf{P}_{\sigma}$. This matrix $\mathbf{P}%
_{\sigma}$ is defined to be the matrix that is written as%
\[
\left(
\begin{array}
[c]{cccc}%
P\left(  1,1\right)  & P\left(  1,2\right)  & \cdots & P\left(  1,n\right) \\
P\left(  2,1\right)  & P\left(  2,2\right)  & \cdots & P\left(  2,n\right) \\
\vdots & \vdots & \ddots & \vdots\\
P\left(  n,1\right)  & P\left(  n,2\right)  & \cdots & P\left(  n,n\right)
\end{array}
\right)
\]
in block-matrix notation, where the $\left(  i,j\right)  $-th block $P\left(
i,j\right)  $ is defined by\footnote{We let $0_{u\times v}$ denote the zero
matrix of size $u\times v$.}%
\[
P\left(  i,j\right)  :=%
\begin{cases}
I_{n_{i}}, & \text{if }i=\sigma\left(  j\right)  ;\\
0_{n_{i}\times n_{\sigma\left(  j\right)  }}, & \text{if }i\neq\sigma\left(
j\right)  .
\end{cases}
\]
For example, if $n=2$ and if $\sigma$ is the permutation of $\left[  2\right]
$ that swaps $1$ with $2$, and if $n_{1}=1$ and $n_{2}=2$, then $\mathbf{P}%
_{\sigma}=\left(
\begin{array}
[c]{cc}%
0 & I_{1}\\
I_{2} & 0
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
0 & 0 & 1\\
1 & 0 & 0\\
0 & 1 & 0
\end{array}
\right)  $. The formula analogous to (\ref{pf.prop.schurtri.similar.diag.1})
is%
\[
\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{n}%
\end{array}
\right)  \cdot\mathbf{P}_{\sigma}=\mathbf{P}_{\sigma}\cdot\left(
\begin{array}
[c]{cccc}%
A_{\sigma\left(  1\right)  } & 0 & \cdots & 0\\
0 & A_{\sigma\left(  2\right)  } & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{\sigma\left(  n\right)  }%
\end{array}
\right)
\]
this time; its proof is easy with the help of Proposition
\ref{prop.blockmatrix.mult-uxv}.
\end{proof}

Similarity of block-diagonal matrices can also come from similarity of the
respective blocks:

\begin{proposition}
\label{prop.blockmatrix.simi-pres}Let $\mathbb{F}$ be a field. Let
$n\in\mathbb{N}$. For each $i\in\left[  n\right]  $, let $A_{i}$ and $B_{i}$
be two $n_{i}\times n_{i}$-matrices (for some $n_{i}\in\mathbb{N}$) satisfying
$A_{i}\sim B_{i}$. Then,%
\[
\left(
\begin{array}
[c]{cccc}%
A_{1} & 0 & \cdots & 0\\
0 & A_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & A_{n}%
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cccc}%
B_{1} & 0 & \cdots & 0\\
0 & B_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & B_{n}%
\end{array}
\right)  .
\]

\end{proposition}

\begin{exercise}
\label{exe.blockmatrix.simi-pres}\fbox{2} Prove Proposition
\ref{prop.blockmatrix.simi-pres}.
\end{exercise}

\subsection{Unitary similarity}

Unitary similarity is a more restrictive form of similarity, even though it is
not immediately obvious from its definition:

\begin{definition}
\label{def.schurtri.unisim.def}Let $A$ and $B$ be two matrices in
$\mathbb{C}^{n\times n}$. We say that $A$ is \emph{unitarily similar} to $B$
if there exists a unitary matrix $W\in\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  $ such that $B=WAW^{\ast}$.

We write \textquotedblleft$A\overset{\operatorname*{us}}{\sim}B$%
\textquotedblright\ for \textquotedblleft$A$ is unitarily similar to
$B$\textquotedblright.
\end{definition}

\begin{example}
The matrix $\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 1
\end{array}
\right)  $ is unitarily similar to the matrix $\left(
\begin{array}
[c]{cc}%
2 & 0\\
0 & 0
\end{array}
\right)  $, since $\left(
\begin{array}
[c]{cc}%
2 & 0\\
0 & 0
\end{array}
\right)  =W\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 1
\end{array}
\right)  W^{\ast}$ for the unitary matrix $W=\dfrac{1}{\sqrt{2}}\left(
\begin{array}
[c]{cc}%
1 & 1\\
-1 & 1
\end{array}
\right)  $.
\end{example}

\begin{example}
\label{exe.schurtri.unisim.two2x2}\fbox{2} Prove that the matrix $\left(
\begin{array}
[c]{cc}%
1 & 1\\
0 & 2
\end{array}
\right)  $ is similar to the matrix $\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 2
\end{array}
\right)  $, but not unitarily similar to it.
\end{example}

Just like the relation $\sim$, the relation $\overset{\operatorname*{us}%
}{\sim}$\ is an equivalence relation:

\begin{proposition}
\label{prop.schurtri.unisim.eqrel}\textbf{(a)} Any matrix $A\in\mathbb{C}%
^{n\times n}$ is unitarily similar to itself. \medskip

\textbf{(b)} If $A$ and $B$ are two matrices in $\mathbb{C}^{n\times n}$ such
that $A$ is unitarily similar to $B$, then $B$ is unitarily similar to $A$.
\medskip

\textbf{(c)} If $A$, $B$ and $C$ are three matrices in $\mathbb{C}^{n\times
n}$ such that $A$ is unitarily similar to $B$ and such that $B$ is unitarily
similar to $C$, then $A$ is unitarily similar to $C$.
\end{proposition}

\begin{proof}
This is very similar to the proof of Proposition
\ref{prop.schurtri.similar.eqrel}, and therefore left to the reader. (The only
new idea is to use Exercise \ref{exe.unitary.group}.)
\end{proof}

\begin{definition}
Let $A$ and $B$ be two matrices in $\mathbb{C}^{n\times n}$. We say that $A$
and $B$ are \emph{unitarily similar} if $A$ is unitarily similar to $B$ (or,
equivalently, $B$ is unitarily similar to $A$).
\end{definition}

As we promised, unitary similarity is a more restrictive version of similarity:

\begin{proposition}
\label{prop.schurtri.unisim.sim}Let $A$ and $B$ be two unitarily similar
matrices in $\mathbb{C}^{n\times n}$. Then, $A$ and $B$ are similar.
\end{proposition}

\begin{proof}
There exists a unitary matrix $W\in\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  $ such that $B=WAW^{\ast}$ (since $A$ is unitarily similar
to $B$). Consider this $W$. The matrix $W$ is unitary, and thus (by the
implication $\mathcal{A}\Longrightarrow\mathcal{D}$ in Theorem
\ref{thm.unitary.unitary.eqs}) must be square and invertible and satisfy
$W^{-1}=W^{\ast}$. Hence, $B=WA\underbrace{W^{\ast}}_{=W^{-1}}=WAW^{-1}$. But
this shows that $A$ is similar to $B$. Thus, Proposition
\ref{prop.schurtri.unisim.sim} is proven.
\end{proof}

%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 4 starts here.}\\\hline\hline
\end{tabular}
\]


\subsection{Schur triangularization}

We are now ready for one more matrix decomposition: the so-called \emph{Schur
triangularization} (aka \emph{Schur decomposition}):

\begin{theorem}
[Schur triangularization theorem]\label{thm.schurtri.schurtri}Let
$A\in\mathbb{C}^{n\times n}$. Then, there exists a unitary matrix
$U\in\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)  $ and an
upper-triangular matrix $T\in\mathbb{C}^{n\times n}$ such that $A=UTU^{\ast}$.
In other words, $A$ is unitarily similar to some upper-triangular matrix.
\end{theorem}

The factorization $A=UTU^{\ast}$ in Theorem \ref{thm.schurtri.schurtri} (or,
to be more precise, the pair $\left(  U,T\right)  $) is called a \emph{Schur
triangularization} of $A$. It is usually not unique.

\begin{example}
Let $A=\left(
\begin{array}
[c]{cc}%
1 & 3\\
-3 & 7
\end{array}
\right)  $. Then, a Schur triangularization of $A$ is $A=UTU^{\ast}$, where%
\[
U=\dfrac{1}{\sqrt{2}}\left(
\begin{array}
[c]{cc}%
1 & -1\\
1 & 1
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ T=\left(
\begin{array}
[c]{cc}%
4 & 6\\
0 & 4
\end{array}
\right)  .
\]
(We chose $A$ deliberately to obtain \textquotedblleft nice\textquotedblright%
\ matrices $U$ and $T$. The Schur triangularization of a typical $n\times
n$-matrix will be more complicated, involving roots of $n$-th degree polynomials.)
\end{example}

\begin{proof}
[Proof of Theorem \ref{thm.schurtri.schurtri}.]We proceed by induction on $n$:

\textit{Induction base:} For $n=0$, Theorem \ref{thm.schurtri.schurtri} holds trivially.

\textit{Induction step:} TODO: Scribe!
\end{proof}

\begin{exercise}
\label{exe.schurtri.schurtri.one2x2}\fbox{2} Find a Schur triangularization of
the matrix $\left(
\begin{array}
[c]{cc}%
1 & 0\\
i & 1
\end{array}
\right)  $.
\end{exercise}

\begin{exercise}
\label{exe.schurtri.schurtri.one3x3}\fbox{3} Find a Schur triangularization of
the matrix $\left(
\begin{array}
[c]{ccc}%
1 & 1 & 1\\
1 & 1 & 1\\
1 & 1 & 1
\end{array}
\right)  $.
\end{exercise}

One remark is in order about the $T$ in a Schur triangularization:

\begin{proposition}
\label{prop.schurtri.schurtri.T-diag}Let $A\in\mathbb{C}^{n\times n}$. Let
$\left(  U,T\right)  $ be a Schur triangularization of $A$. Then, the diagonal
entries of $T$ are the eigenvalues of $A$ (with their algebraic multiplicities).
\end{proposition}

Instead of proving this directly, let us show a more general result:

\begin{proposition}
\label{prop.schurtri.similar.T-diag}Let $\mathbb{F}$ be a field. Let
$A\in\mathbb{F}^{n\times n}$ and $T\in\mathbb{F}^{n\times n}$ be two similar
matrices. Assume that $T$ is upper-triangular. Then, the diagonal entries of
$T$ are the eigenvalues of $A$ (with their algebraic multiplicities).
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.schurtri.similar.T-diag}.]We have assumed that
$A$ and $T$ are similar. In other words, $T$ is similar to $A$. Thus,
Proposition \ref{prop.schurtri.similar.same} \textbf{(e)} shows that the
matrices $T$ and $A$ have the same eigenvalues with the same algebraic
multiplicities (and the same geometric multiplicities, but we don't need to
know this). In other words, the eigenvalues of $T$ are the eigenvalues of $A$
(with the same algebraic multiplicities).

However, the matrix $T$ is upper-triangular. Thus, the eigenvalues of $T$
(with algebraic multiplicities) are the diagonal entries of $T$ (this is a
well-known fact\footnote{Here is a \textit{proof:} The matrix $T$ is
upper-triangular; thus, it has the form
\[
T=\left(
\begin{array}
[c]{cccc}%
T_{1,1} & T_{1,2} & \cdots & T_{1,n}\\
0 & T_{2,2} & \cdots & T_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & T_{n,n}%
\end{array}
\right)  .
\]
Thus, if $t$ is an indeterminate, then
\begin{align*}
tI_{n}-T  &  =t\left(
\begin{array}
[c]{cccc}%
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  -\left(
\begin{array}
[c]{cccc}%
T_{1,1} & T_{1,2} & \cdots & T_{1,n}\\
0 & T_{2,2} & \cdots & T_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & T_{n,n}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
t-T_{1,1} & -T_{1,2} & \cdots & -T_{1,n}\\
0 & t-T_{2,2} & \cdots & -T_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & t-T_{n,n}%
\end{array}
\right)  .
\end{align*}
This is still an upper-triangular matrix; thus, its determinant is the product
of its diagonal entries. That is, we have%
\[
\det\left(  tI_{n}-T\right)  =\left(  t-T_{1,1}\right)  \left(  t-T_{2,2}%
\right)  \cdots\left(  t-T_{n,n}\right)  .
\]
Therefore, $T_{1,1},T_{2,2},\ldots,T_{n,n}$ are the roots of the polynomial
$\det\left(  tI_{n}-T\right)  $ (with multiplicities).
\par
However, $\det\left(  tI_{n}-T\right)  $ is the characteristic polynomial of
$T$. Thus, the roots of this polynomial $\det\left(  tI_{n}-T\right)  $ are
the eigenvalues of $T$ (with algebraic multiplicities). Since we know that the
roots of this polynomial are $T_{1,1},T_{2,2},\ldots,T_{n,n}$, we thus
conclude that $T_{1,1},T_{2,2},\ldots,T_{n,n}$ are the eigenvalues of $T$
(with algebraic multiplicities). In other words, the diagonal entries of $T$
are the eigenvalues of $T$ (with algebraic multiplicities). Qed.}). Since we
know that the eigenvalues of $T$ are the eigenvalues of $A$ (with the same
algebraic multiplicities), we thus conclude that the eigenvalues of $A$ (with
algebraic multiplicities) are the diagonal entries of $T$. This proves
Proposition \ref{prop.schurtri.similar.T-diag}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.schurtri.schurtri.T-diag}.]We assumed that
$\left(  U,T\right)  $ is a Schur triangularization of $A$. Hence, we have
$A=UTU^{\ast}$, and the matrix $U$ is unitary whereas the matrix $T$ is
upper-triangular. From $A=UTU^{\ast}$, we conclude that $T$ is unitarily
similar to $A$ (by Definition \ref{def.schurtri.unisim.def}, since $U$ is
unitary). Hence, $T$ is similar to $A$ (by Proposition
\ref{prop.schurtri.unisim.sim}). Therefore, Proposition
\ref{prop.schurtri.similar.T-diag} shows that the diagonal entries of $T$ are
the eigenvalues of $A$ (with their algebraic multiplicities). This proves
Proposition \ref{prop.schurtri.schurtri.T-diag}.
\end{proof}

We will see several applications of Theorem \ref{thm.schurtri.schurtri} in
this chapter. First, however, let us generalize this theorem from the case of
a single matrix to the case of several matrices that pairwise commute.

\subsection{Commuting matrices}

\begin{definition}
Two $n\times n$-matrices $A$ and $B$ are said to \emph{commute} if $AB=BA$.
\end{definition}

Examples of commuting matrices are easy to find; e.g., any two powers of a
single matrix commute (i.e., we have $A^{k}A^{\ell}=A^{\ell}A^{k}$ for any
$n\times n$-matrix $A$ and any $k,\ell\in\mathbb{N}$). Also, any two diagonal
matrices (of the same size) commute. But there are many more situations in
which matrices commute. In this section, we shall extend Schur
triangularization from a single matrix to a family of commuting matrices.

First, we need a lemma (\cite[Lemma 1.3.19]{HorJoh13}) which says that any
family of pairwise commuting matrices in $\mathbb{C}^{n\times n}$ has a common eigenvector:

\begin{lemma}
\label{lem.schurtri.commute.1}Let $n>0$. Let $\mathcal{F}$ be a subset of
$\mathbb{C}^{n\times n}$ such that any two matrices in $\mathcal{F}$ commute
(i.e., any $A\in\mathcal{F}$ and $B\in\mathcal{F}$ satisfy $AB=BA$).

Then, there exists a nonzero vector $x\in\mathbb{C}^{n}$ such that $x$ is an
eigenvector of each $A\in\mathcal{F}$.
\end{lemma}

\begin{proof}
TODO: Scribe!
\end{proof}

We can now generalize Theorem \ref{thm.schurtri.schurtri} to families of
commuting matrices:

\begin{theorem}
\label{thm.schurtri.commute.schurtri}Let $\mathcal{F}$ be a subset of
$\mathbb{C}^{n\times n}$ such that any two matrices in $\mathcal{F}$ commute
(i.e., any $A\in\mathcal{F}$ and $B\in\mathcal{F}$ satisfy $AB=BA$).

Then, there exists a unitary matrix $U\in\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  $ such that for each $A\in\mathcal{F}$, the matrix
$UAU^{\ast}$ is upper-triangular.
\end{theorem}

\begin{proof}
TODO: Scribe!
\end{proof}

%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 5 starts here.}\\\hline\hline
\end{tabular}
\]


\subsection{Normal matrices}

We next define a fairly wide class of matrices with complex entries that
contains several of our familiar classes as subsets:

\begin{definition}
\label{def.schurtri.normal.normal}A square matrix $A\in\mathbb{C}^{n\times n}$
is said to be \emph{normal} if $AA^{\ast}=A^{\ast}A$.
\end{definition}

In other words, a square matrix is normal if it commutes with its own
conjugate transpose. This is not the most intuitive notion (nor is the word
\textquotedblleft normal\textquotedblright\ particularly expressive), so we
shall give some examples:

\begin{example}
\textbf{(a)} Let $A=\left(
\begin{array}
[c]{cc}%
1 & -1\\
1 & 1
\end{array}
\right)  \in\mathbb{C}^{2\times2}$. Then, the matrix $A$ is normal. Indeed,
its conjugate transpose is $A^{\ast}=\left(
\begin{array}
[c]{cc}%
1 & 1\\
-1 & 1
\end{array}
\right)  $, and it is easily seen that $AA^{\ast}=A^{\ast}A=2I_{2}$. \medskip

\textbf{(b)} Let $B=\left(
\begin{array}
[c]{cc}%
0 & i\\
0 & 0
\end{array}
\right)  \in\mathbb{C}^{2\times2}$. Then, the matrix $B$ is not normal.
Indeed, $B^{\ast}=\left(
\begin{array}
[c]{cc}%
0 & 0\\
-i & 0
\end{array}
\right)  $ and thus $BB^{\ast}\neq B^{\ast}B$, as can easily be verified.
\medskip

\textbf{(c)} Let $a,b\in\mathbb{C}$ be arbitrary, and let $C=\left(
\begin{array}
[c]{cc}%
a & b\\
b & a
\end{array}
\right)  \in\mathbb{C}^{2\times2}$. Then, $C$ is normal. Indeed, $C^{\ast
}=\left(
\begin{array}
[c]{cc}%
\overline{a} & \overline{b}\\
\overline{b} & \overline{a}%
\end{array}
\right)  $, so that it is easy to check that both $CC^{\ast}$ and $C^{\ast}C$
equal $\left(
\begin{array}
[c]{cc}%
a & b\\
b & a
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
\overline{a} & \overline{b}\\
\overline{b} & \overline{a}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
a\overline{a}+b\overline{b} & a\overline{b}+b\overline{a}\\
a\overline{b}+b\overline{a} & a\overline{a}+b\overline{b}%
\end{array}
\right)  $.
\end{example}

As we promised, several familiar classes of matrices are normal. We recall a definition:

\begin{definition}
\label{def.schurtri.herm}A square matrix $H\in\mathbb{C}^{n\times n}$ is said
to be \emph{Hermitian} if and only if $H^{\ast}=H$.
\end{definition}

For example, the matrix $\left(
\begin{array}
[c]{cc}%
1 & i\\
-i & 2
\end{array}
\right)  $ is Hermitian.

In contrast, a square matrix $S\in\mathbb{C}^{n\times n}$ is skew-Hermitian if
and only if $S^{\ast}=-S$ (by Definition \ref{def.unitary.skew-herm}).
Finally, a square matrix $U\in\mathbb{C}^{n\times n}$ is unitary if and only
if $UU^{\ast}=U^{\ast}U=I_{n}$ (by Theorem \ref{thm.unitary.unitary.eqs},
equivalence $\mathcal{A}\Longleftrightarrow\mathcal{C}$). Having recalled all
these concepts, we can state the following:

\begin{proposition}
\label{prop.schurtri.normal.classes}\textbf{(a)} Every Hermitian matrix
$H\in\mathbb{C}^{n\times n}$ is normal. \medskip

\textbf{(b)} Every skew-Hermitian matrix $S\in\mathbb{C}^{n\times n}$ is
normal. \medskip

\textbf{(c)} Every unitary matrix $U\in\mathbb{C}^{n\times n}$ is normal.
\medskip

\textbf{(d)} Every diagonal matrix $D\in\mathbb{C}^{n\times n}$ is normal.
\end{proposition}

\begin{proof}
\textbf{(a)} Let $H\in\mathbb{C}^{n\times n}$ be a Hermitian matrix. Then,
$H^{\ast}=H$ (by the definition of \textquotedblleft
Hermitian\textquotedblright). Hence, $H\underbrace{H^{\ast}}_{=H}%
=\underbrace{H}_{=H^{\ast}}H=H^{\ast}H$. In other words, $H$ is normal. This
proves Proposition \ref{prop.schurtri.normal.classes} \textbf{(a)}. \medskip

\textbf{(b)} This is analogous to part \textbf{(a)}, except for a minus sign
that appears and disappears again. \medskip

\textbf{(c)} This is clear, since $UU^{\ast}=U^{\ast}U=I_{n}$ entails
$UU^{\ast}=U^{\ast}U$. \medskip

\textbf{(d)} Let $D\in\mathbb{C}^{n\times n}$ be a diagonal matrix. Write $D$
in the form
\[
D=\operatorname*{diag}\left(  \lambda_{1},\lambda_{2},\ldots,\lambda
_{n}\right)  \ \ \ \ \ \ \ \ \ \ \text{for some }\lambda_{1},\lambda
_{2},\ldots,\lambda_{n}\in\mathbb{C}.
\]
Then, $D^{\ast}=\operatorname*{diag}\left(  \overline{\lambda_{1}}%
,\overline{\lambda_{2}},\ldots,\overline{\lambda_{n}}\right)  $. Hence,
\begin{align*}
DD^{\ast}  &  =\operatorname*{diag}\left(  \lambda_{1}\overline{\lambda_{1}%
},\lambda_{2}\overline{\lambda_{2}},\ldots,\lambda_{n}\overline{\lambda_{n}%
}\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
D^{\ast}D  &  =\operatorname*{diag}\left(  \overline{\lambda_{1}}\lambda
_{1},\overline{\lambda_{2}}\lambda_{2},\ldots,\overline{\lambda_{n}}%
\lambda_{n}\right)  .
\end{align*}
The right hand sides of these two equalities are equal (since $\lambda
_{i}\overline{\lambda_{i}}=\overline{\lambda_{i}}\lambda_{i}$ for each
$i\in\left[  n\right]  $). Thus, the left hand sides must too be equal. In
other words, $DD^{\ast}=D^{\ast}D$. This means that $D$ is normal. This proves
Proposition \ref{prop.schurtri.normal.classes} \textbf{(d)}.
\end{proof}

Unlike the unitary matrices, the normal matrices are not closed under multiplication:

\begin{exercise}
\label{exe.schurtri.normal.not-additive}\fbox{2} Find two normal matrices
$A,B\in\mathbb{C}^{2\times2}$ such that neither $A+B$ nor $AB$ is normal.
\end{exercise}

Here are two more ways to construct normal matrices out of existing normal matrices:

\begin{proposition}
\label{prop.schurtri.normal.conj}Let $A\in\mathbb{C}^{n\times n}$ be a normal
matrix. \medskip

\textbf{(a)} If $\lambda\in\mathbb{C}$ is arbitrary, then the matrix $\lambda
I_{n}+A$ is normal. \medskip

\textbf{(b)} If $U\in\mathbb{C}^{n\times n}$ is a unitary matrix, then the
matrix $UAU^{\ast}$ is normal.
\end{proposition}

\begin{proof}
We have $AA^{\ast}=A^{\ast}A$ (since $A$ is normal). \medskip

\textbf{(a)} Let $\lambda\in\mathbb{C}$ be arbitrary. Then, Proposition
\ref{prop.unitary.(AB)*} \textbf{(a)} yields%
\[
\left(  \lambda I_{n}+A\right)  ^{\ast}=\underbrace{\left(  \lambda
I_{n}\right)  ^{\ast}}_{\substack{=\overline{\lambda}I_{n}\\\text{(this is
easily seen directly, or}\\\text{obtained from Proposition
\ref{prop.unitary.(AB)*} \textbf{(b)})}}}+A^{\ast}=\overline{\lambda}%
I_{n}+A^{\ast}.
\]
Hence,
\begin{align*}
\left(  \lambda I_{n}+A\right)  \underbrace{\left(  \lambda I_{n}+A\right)
^{\ast}}_{=\overline{\lambda}I_{n}+A^{\ast}}  &  =\left(  \lambda
I_{n}+A\right)  \left(  \overline{\lambda}I_{n}+A^{\ast}\right) \\
&  =\lambda\overline{\lambda}I_{n}+\lambda A^{\ast}+\overline{\lambda
}A+AA^{\ast}.
\end{align*}
A similar computation shows that%
\[
\left(  \lambda I_{n}+A\right)  ^{\ast}\left(  \lambda I_{n}+A\right)
=\overline{\lambda}\lambda I_{n}+\lambda A^{\ast}+\overline{\lambda}A+A^{\ast
}A.
\]
The right hand sides of these two equalities are equal (since $\lambda
\overline{\lambda}=\overline{\lambda}\lambda$ and $AA^{\ast}=A^{\ast}A$).
Hence, so are the left hand sides. In other words, $\left(  \lambda
I_{n}+A\right)  \left(  \lambda I_{n}+A\right)  ^{\ast}=\left(  \lambda
I_{n}+A\right)  ^{\ast}\left(  \lambda I_{n}+A\right)  $. In other words, the
matrix $\lambda I_{n}+A$ is normal. This proves Proposition
\ref{prop.schurtri.normal.conj} \textbf{(a)}. \medskip

\textbf{(b)} Let $U\in\mathbb{C}^{n\times n}$ be a unitary matrix. Thus,
$UU^{\ast}=U^{\ast}U=I_{n}$ (by the $\mathcal{A}\Longleftrightarrow
\mathcal{C}$ part of Theorem \ref{thm.unitary.unitary.eqs}). Now, applying
Proposition \ref{prop.unitary.(AB)*} \textbf{(c)} twice, we see that $\left(
XYZ\right)  ^{\ast}=Z^{\ast}Y^{\ast}X^{\ast}$ for any three $n\times
n$-matrices $X,Y,Z$. Hence,%
\[
\left(  UAU^{\ast}\right)  ^{\ast}=\underbrace{\left(  U^{\ast}\right)
^{\ast}}_{\substack{=U\\\text{(by Proposition \ref{prop.unitary.(AB)*}
\textbf{(d)})}}}A^{\ast}U^{\ast}=UA^{\ast}U^{\ast}.
\]
Hence,%
\[
\left(  UAU^{\ast}\right)  \underbrace{\left(  UAU^{\ast}\right)  ^{\ast}%
}_{=UA^{\ast}U^{\ast}}=\left(  UAU^{\ast}\right)  \left(  UA^{\ast}U^{\ast
}\right)  =UA\underbrace{U^{\ast}U}_{=I_{n}}A^{\ast}U^{\ast}=UAA^{\ast}%
U^{\ast}.
\]
A similar computation shows that%
\[
\left(  UAU^{\ast}\right)  ^{\ast}\left(  UAU^{\ast}\right)  =UA^{\ast
}AU^{\ast}.
\]
The right hand sides of these two equalities are equal (since $AA^{\ast
}=A^{\ast}A$). Hence, so are the left hand sides. In other words, $\left(
UAU^{\ast}\right)  \left(  UAU^{\ast}\right)  ^{\ast}=\left(  UAU^{\ast
}\right)  ^{\ast}\left(  UAU^{\ast}\right)  $. In other words, the matrix
$UAU^{\ast}$ is normal. This proves Proposition
\ref{prop.schurtri.normal.conj} \textbf{(b)}.
\end{proof}

Here is another normality-preserving way to transform matrices:

\begin{definition}
\label{def.schurtri.normal.p(A)}Let $\mathbb{F}$ be a field. Let
$A\in\mathbb{F}^{n\times n}$ be a square matrix. Let $p\left(  x\right)  $ be
a polynomial in a single indeterminate $x$ with coefficients in $\mathbb{F}$.
Write $p\left(  x\right)  $ in the form $p\left(  x\right)  =a_{0}x^{0}%
+a_{1}x^{1}+\cdots+a_{d}x^{d}$, where $a_{0},a_{1},\ldots,a_{d}\in\mathbb{F}$.

Then, $p\left(  A\right)  $ denotes the matrix $a_{0}A^{0}+a_{1}A^{1}%
+\cdots+a_{d}A^{d}\in\mathbb{F}^{n\times n}$.
\end{definition}

For instance, if $p\left(  x\right)  =x^{3}-2x^{2}+1$, then $p\left(
A\right)  =A^{3}-2A^{2}+A^{0}=A^{3}-2A^{2}+I_{n}$.

\begin{proposition}
\label{prop.schurtri.normal.p(A)nor}Let $A\in\mathbb{C}^{n\times n}$ be a
normal matrix. Let $p\left(  x\right)  $ be a polynomial in a single
indeterminate $x$ with coefficients in $\mathbb{C}$. Then, the matrix
$p\left(  A\right)  $ is normal.
\end{proposition}

\begin{exercise}
\label{exe.schurtri.normal.p(A)nor}\fbox{3} Prove Proposition
\ref{prop.schurtri.normal.p(A)nor}.
\end{exercise}

\begin{exercise}
\label{exe.schurtri.normal.isometry}\fbox{2} Generalizing Proposition
\ref{prop.schurtri.normal.conj} \textbf{(b)}, we might claim the following:

Let $A\in\mathbb{C}^{k\times k}$ be a normal matrix. Let $U\in\mathbb{C}%
^{n\times k}$ be an isometry. Then, the matrix $UAU^{\ast}$ is normal.

Is this generalization correct?
\end{exercise}

We will now prove an innocent-looking property of normal matrices that will
turn out crucial in characterizing them:

\begin{lemma}
\label{lem.schurtri.normal.tri}Let $T\in\mathbb{C}^{n\times n}$ be a
triangular matrix. Then, $T$ is normal if and only if $T$ is diagonal.
\end{lemma}

\begin{proof}
The \textquotedblleft if\textquotedblright\ direction follows from Proposition
\ref{prop.schurtri.normal.classes} \textbf{(d)}. Thus, it remains to prove the
\textquotedblleft only if\textquotedblright\ direction.

So let us assume that $T$ is normal. We shall show that $T$ is diagonal.

TODO:\ Scribe!
\end{proof}

For the next exercise, we recall the notion of a \emph{nilpotent matrix}:

\begin{definition}
Let $\mathbb{F}$ be a field. A square matrix $A\in\mathbb{F}^{n\times n}$ is
said to be \emph{nilpotent} if there exists some nonnegative integer $m$ such
that $A^{m}=0$.
\end{definition}

For example, the matrix $\left(
\begin{array}
[c]{cc}%
6 & 9\\
-4 & -6
\end{array}
\right)  $ is nilpotent, since $\left(
\begin{array}
[c]{cc}%
6 & 9\\
-4 & -6
\end{array}
\right)  ^{2}=0$. Also, every strictly upper-triangular matrix and every
strictly lower-triangular matrix is nilpotent.

\begin{exercise}
\label{exe.schurtri.normal.nilp}\fbox{2} Let $A\in\mathbb{C}^{n\times n}$ be a
normal matrix that is nilpotent. Prove that $A=0$.
\end{exercise}

\subsection{The spectral theorem}

We are now ready to state the main theorem about normal matrices, the
so-called \emph{spectral theorem}:

\begin{theorem}
[spectral theorem for normal matrices]\label{thm.schurtri.normal.spectral}Let
$A\in\mathbb{C}^{n\times n}$ be a normal matrix. Then: \medskip

\textbf{(a)} There exists a unitary matrix $U\in\operatorname*{U}%
\nolimits_{n}\left(  \mathbb{C}\right)  $ and a diagonal matrix $D\in
\mathbb{C}^{n\times n}$ such that%
\[
A=UDU^{\ast}.
\]
In other words, $A$ is unitarily similar to a diagonal matrix. \medskip

\textbf{(b)} Let $U\in\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)
$ be a unitary matrix, and $D\in\mathbb{C}^{n\times n}$ be a diagonal matrix
such that $A=UDU^{\ast}$. Then, the diagonal entries of $D$ are the
eigenvalues of $A$. Moreover, the columns of $U$ are the eigenvectors of $A$.
Thus, there exists an orthonormal basis of $\mathbb{C}^{n}$ consisting of
eigenvectors of $A$.
\end{theorem}

\begin{proof}
TODO: Scribe!
\end{proof}

The decomposition $A=UDU^{\ast}$ in Theorem
\ref{thm.schurtri.commute.schurtri} (or, to be more precise, the pair $\left(
U,D\right)  $) is called a \emph{spectral decomposition} of $A$. It is not
unique (e.g., we can replace $U$ by $\lambda U$ whenever $\lambda\in
\mathbb{C}$ satisfies $\left\vert \lambda\right\vert =1$; this does not change
$UDU^{\ast}$).

Note that Theorem \ref{thm.schurtri.normal.spectral} \textbf{(b)} has a
converse, which helps finding spectral decompositions in practice if one
doesn't want to go through the trouble of Schur triangularization:

\begin{proposition}
\label{prop.schurtri.normal.converse}Let $A\in\mathbb{C}^{n\times n}$. Let
$U\in\operatorname*{U}\nolimits_{n}\left(  \mathbb{C}\right)  $ be a unitary
matrix and $D\in\mathbb{C}^{n\times n}$ a diagonal matrix. Assume that for
each $i\in\left[  n\right]  $, we have $AU_{\bullet,i}=D_{i,i}U_{\bullet,i}$
(that is, the $i$-th column of $U$ is an eigenvector of $A$ for the eigenvalue
$D_{i,i}$). Then, $A=UDU^{\ast}$, so that $\left(  U,D\right)  $ is a spectral
decomposition of $A$.
\end{proposition}

\begin{exercise}
\fbox{2} Prove Proposition \ref{prop.schurtri.normal.converse}.
\end{exercise}

\begin{exercise}
\label{exe.schurtri.normal.examples}\fbox{5} \textbf{(a)} Find a spectral
decomposition of the normal matrix $\left(
\begin{array}
[c]{cc}%
1 & 1+i\\
1+i & 1
\end{array}
\right)  $. \medskip

\textbf{(b)} Find a spectral decomposition of the Hermitian matrix $\left(
\begin{array}
[c]{cc}%
0 & -i\\
i & 0
\end{array}
\right)  $. \medskip

\textbf{(c)} Find a spectral decomposition of the skew-Hermitian matrix
$\left(
\begin{array}
[c]{cc}%
0 & i\\
i & 0
\end{array}
\right)  $. \medskip

\textbf{(d)} Find a spectral decomposition of the unitary matrix $\dfrac
{1}{\sqrt{2}}\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & -1
\end{array}
\right)  $.
\end{exercise}

\begin{exercise}
\fbox{2} Describe all spectral decompositions of the $n\times n$ identity
matrix $I_{n}$.
\end{exercise}

Only normal matrices can have a spectral decomposition. Indeed, if some
$n\times n$-matrix $A\in\mathbb{C}^{n\times n}$ can be written as
$A=UDU^{\ast}$ for some unitary $U$ and some diagonal $D$, then $D$ is normal
(by Proposition \ref{prop.schurtri.normal.classes} \textbf{(d)}), and
therefore $A$ is normal (by Proposition \ref{prop.schurtri.normal.conj}
\textbf{(b)}, applied to $D$ instead of $A$). Thus, we obtain the following
characterization of normal matrices:

\begin{corollary}
\label{cor.schurtri.normal.normal-iff}An $n\times n$-matrix $A\in
\mathbb{C}^{n\times n}$ is normal if and only if it is unitarily similar to a
diagonal matrix.
\end{corollary}

\begin{proof}
TODO: Scribe!
\end{proof}

The spectral decomposition of a Hermitian matrix has a special property:

\begin{proposition}
\label{prop.schurtri.normal.hermitian-spec}Let $A\in\mathbb{C}^{n\times n}$ be
a Hermitian matrix, and let $\left(  U,D\right)  $ be a spectral decomposition
of $A$. Then, the diagonal entries of $D$ are real.
\end{proposition}

\begin{proof}
TODO: Scribe!
\end{proof}

This allows us to characterize Hermitian matrices in a similar way as normal
matrices were characterized by Corollary \ref{cor.schurtri.normal.normal-iff}:

\begin{corollary}
\label{cor.schurtri.normal.hermitian-iff}An $n\times n$-matrix $A\in
\mathbb{C}^{n\times n}$ is Hermitian if and only if it is unitarily similar to
a diagonal matrix with real entries.
\end{corollary}

\begin{proof}
TODO: Scribe!
\end{proof}

Similarly, we can handle skew-Hermitian matrices:

\begin{proposition}
\label{prop.schurtri.normal.skewherm-spec}Let $A\in\mathbb{C}^{n\times n}$ be
a skew-Hermitian matrix, and let $\left(  U,D\right)  $ be a spectral
decomposition of $A$. Then, the diagonal entries of $D$ are purely imaginary.
\end{proposition}

\begin{corollary}
\label{cor.schurtri.normal.skewherm-iff}An $n\times n$-matrix $A\in
\mathbb{C}^{n\times n}$ is skew-Hermitian if and only if it is unitarily
similar to a diagonal matrix with purely imaginary entries.
\end{corollary}

\begin{exercise}
\label{exe.schurtri.normal.skewherm}\fbox{3} Prove Proposition
\ref{prop.schurtri.normal.skewherm-spec} and Corollary
\ref{cor.schurtri.normal.skewherm-iff}.
\end{exercise}

Likewise, we can handle unitary matrices.

\begin{proposition}
\label{prop.schurtri.normal.unitary-spec}Let $A\in\mathbb{C}^{n\times n}$ be a
unitary matrix, and let $\left(  U,D\right)  $ be a spectral decomposition of
$A$. Then, each of the diagonal entries of $D$ has absolute value $1$.
\end{proposition}

\begin{corollary}
\label{cor.schurtri.normal.unitary-iff}An $n\times n$-matrix $A\in
\mathbb{C}^{n\times n}$ is unitary if and only if it is unitarily similar to a
diagonal matrix whose all diagonal entries have absolute value $1$.
\end{corollary}

\begin{exercise}
\label{exe.schurtri.normal.unitary}\fbox{2} Prove Proposition
\ref{prop.schurtri.normal.unitary-spec} and Corollary
\ref{cor.schurtri.normal.unitary-iff}.
\end{exercise}

\begin{exercise}
\fbox{2} Prove the following generalization of Theorem
\ref{thm.schurtri.normal.spectral}:

Let $\mathcal{F}$ be a subset of $\mathbb{C}^{n\times n}$ such that any matrix
in $\mathcal{F}$ is normal, and such that any two matrices in $\mathcal{F}$
commute (i.e., any $A\in\mathcal{F}$ and $B\in\mathcal{F}$ satisfy $AB=BA$).

Then, there exists a unitary matrix $U\in\operatorname*{U}\nolimits_{n}\left(
\mathbb{C}\right)  $ such that for each $A\in\mathcal{F}$, the matrix
$UAU^{\ast}$ is diagonal.
\end{exercise}

%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 6 starts here.}\\\hline\hline
\end{tabular}
\]


\subsection{The Cayley--Hamilton theorem}

We will now state the famous Cayley--Hamilton theorem, and to prove it at
least for matrices with complex entries. This will serve as a reminder of an
important theorem (which will soon be used), and also as an illustration of
how Schur triangularization can be applied.

In Definition \ref{def.schurtri.normal.p(A)}, we have learnt how to substitute
a square matrix into a polynomial. Something peculiar happens when a matrix is
substituted into its own characteristic polynomial:

\begin{theorem}
[Cayley--Hamilton theorem]\label{thm.schurtri.ch.ch}Let $\mathbb{F}$ be a
field. Let $A\in\mathbb{F}^{n\times n}$ be an $n\times n$-matrix. Then,%
\[
p_{A}\left(  A\right)  =0.
\]
(The \textquotedblleft$0$\textquotedblright\ on the right hand side here means
the zero matrix $0_{2\times2}$.)
\end{theorem}

\begin{example}
Let $n=2$ and $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $. Then, as we know from Example \ref{exa.schurtri.ch.pA.2x2}, we
have%
\[
p_{A}=t^{2}-\left(  a+d\right)  t+\left(  ad-bc\right)  .
\]
Thus,%
\begin{align*}
p_{A}\left(  A\right)   &  =A^{2}-\left(  a+d\right)  A+\left(  ad-bc\right)
I_{2}\\
&  =\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  ^{2}-\left(  a+d\right)  \left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  +\left(  ad-bc\right)  \left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & 0
\end{array}
\right)  =0.
\end{align*}
Thus, we have verified Theorem \ref{thm.schurtri.ch.ch} for $n=2$.
\end{example}

\begin{remark}
\label{rmk.schurtri.ch.not-that-easy}It is tempting to \textquotedblleft
prove\textquotedblright\ Theorem \ref{thm.schurtri.ch.ch} by arguing that
$p_{A}\left(  A\right)  =\det\left(  AI_{n}-A\right)  $ holds
\textquotedblleft by substituting $A$ for $t$ into $p_{A}=\det\left(
tI_{n}-A\right)  $\textquotedblright. Unfortunately, such an argument is
unjustified. Indeed, $tI_{n}-A$ is a matrix whose entries are polynomials in
$t$. If you substitute $A$ for $t$ into it, it will become a matrix whose
entries are matrices. This poses two problems: First, it is unclear how to
take the determinant of such a matrix; second, this matrix is not $AI_{n}-A$.
For example, for $n=2$, substituting $A$ for $t$ in $tI_{n}-A$ gives%
\[
\left(
\begin{array}
[c]{cc}%
\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  -a & -b\\
-c & \left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  -d
\end{array}
\right)  ,
\]
which can be made sense of (if we treat the $a,b,c,d$ as multiples of $I_{2}%
$), but which is certainly not the same as $AI_{n}-A$ (which is the zero
matrix). There \textbf{is} a correct proof of the Cayley--Hamilton theorem
along the lines of \textquotedblleft substituting $A$ for $t$%
\textquotedblright, but it requires a lot of additional work (see
\url{https://math.stackexchange.com/questions/1141648/} for some discussion of this).
\end{remark}

Various proofs of Theorem \ref{thm.schurtri.ch.ch} are found across the
literature; see \cite[after Theorem 2.6]{trach} for a list of references
(Theorem \ref{thm.schurtri.ch.ch} is \cite[Theorem 2.5]{trach}). I can
particularly recommend the algebraic proofs given in \cite[Chapter Five,
Section IV, Lemma 1.9]{Heffer20}, \cite[\S 4, Theorem 1]{Mate16} and
\cite{Shurma15}, and the combinatorial proof shown in \cite{Straub83} and
\cite[\S 3]{Zeilbe}. Here, however, I will show a proof of Theorem
\ref{thm.schurtri.ch.ch} in the particular case when $\mathbb{F}=\mathbb{C}$:

\begin{proof}
[Proof of Theorem \ref{thm.schurtri.ch.ch} for $\mathbb{F}=\mathbb{C}$.]Assume
that $\mathbb{F}=\mathbb{C}$. The Schur triangularization theorem (Theorem
\ref{thm.schurtri.schurtri}) shows that $A$ is unitarily similar to an
upper-triangular matrix. Hence, $A$ is similar to an upper-triangular matrix
(because unitarily similar matrices always are similar). In other words, there
exist an invertible matrix $U$ and an upper-triangular matrix $T$ such that
$A=UTU^{-1}$. Consider these $U$ and $T$.

Now, let $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ be the diagonal entries
of $T$. Then, by Proposition \ref{prop.schurtri.similar.T-diag}, these
diagonal entries $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ are the
eigenvalues of $A$ (with algebraic multipliticies). Hence,%
\[
p_{A}=\left(  t-\lambda_{1}\right)  \left(  t-\lambda_{2}\right)
\cdots\left(  t-\lambda_{n}\right)
\]
(since $p_{A}$ is monic, and the roots of $p_{A}$ are precisely the
eigenvalues of $A$ with algebraic multiplicities).

TODO: Scribe!
\end{proof}

The Cayley--Hamilton theorem has an interesting consequence: it yields that
the inverse of an invertible matrix can be written as a polynomial applied to
this matrix. (However, the specific polynomial that needs to be applied
depends on this matrix.) In more detail:

\begin{exercise}
\label{exe.schurtri.ch.inverse-poly}\fbox{3} Let $\mathbb{F}$ be a field. Let
$n$ be a positive integer. Let $A\in\mathbb{F}^{n\times n}$ be an invertible
matrix with entries in $\mathbb{F}$. Prove that there exists a polynomial $f$
of degree $n-1$ in the single indeterminate $t$ over $\mathbb{F}$ such that
$A^{-1}=f\left(  A\right)  $.
\end{exercise}

For example, for $n=2$, we have $A^{-1}=uI_{2}-vA$ with $u=\dfrac
{\operatorname*{Tr}A}{\det A}$ and $v=\dfrac{1}{\det A}$. \medskip

Another consequence of Cayley--Hamilton is that the powers of a given square
matrix $A\in\mathbb{F}^{n\times n}$ span a vector space of dimension $\leq n$:

\begin{exercise}
\label{exe.schurtri.ch.powers-span}\fbox{3} Let $\mathbb{F}$ be a field. Let
$A\in\mathbb{F}^{n\times n}$ be a square matrix with entries in $\mathbb{F}$.
Prove that for any nonnegative integer $k$, the power $A^{k}$ can be written
as an $\mathbb{F}$-linear combination of the first $n$ powers $A^{0}%
,A^{1},\ldots,A^{n-1}$.
\end{exercise}

Yet another rather curious consequence is an application to linearly recurrent
sequences. We recall what these are:

\begin{definition}
Let $a_{1},a_{2},\ldots,a_{k}$ be $k$ numbers. A sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $ of numbers is said to be $\left(  a_{1}%
,a_{2},\ldots,a_{k}\right)  $\emph{-recurrent} if each integer $i\geq k$
satisfies%
\[
x_{i}=a_{1}x_{i-1}+a_{2}x_{i-2}+\cdots+a_{k}x_{i-k}.
\]

\end{definition}

For instance, the famous Fibonacci sequence $\left(  f_{0},f_{1},f_{2}%
,\ldots\right)  $ (defined by the starting values $f_{0}=0$ and $f_{1}=1$ and
the recurrence $f_{i}=f_{i-1}+f_{i-2}$) is $\left(  1,1\right)  $-recurrent
(by its very definition). Now, it is a simple exercise to check that the
\textquotedblleft even-indexed Fibonacci sequence\textquotedblright\ $\left(
f_{0},f_{2},f_{4},f_{6},\ldots\right)  $ and the \textquotedblleft odd-indexed
Fibonacci sequence\textquotedblright\ $\left(  f_{1},f_{3},f_{5},f_{7}%
,\ldots\right)  $ themselves follow a simple recursion; to wit, they are both
$\left(  3,-1\right)  $-recurrent (check this!). Likewise, the
\textquotedblleft multiples-of-$3$-indexed Fibonacci
sequence\textquotedblright\ $\left(  f_{0},f_{3},f_{6},f_{9},\ldots\right)  $
as well as its companions $\left(  f_{1},f_{4},f_{7},f_{10},\ldots\right)  $
and $\left(  f_{2},f_{5},f_{8},f_{11},\ldots\right)  $ are $\left(
4,1\right)  $-recurrent. This generalizes:

\begin{exercise}
\label{exe.schurtri.ch.lin-rec-kd}\fbox{5} Let $a_{1},a_{2},\ldots,a_{k}$ be
$k$ numbers. Let $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ be any $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $-recurrent sequence of numbers. Let $d$ be a
positive integer. Show that there exist $k$ integers $b_{1},b_{2},\ldots
,b_{k}$ such that each $i\geq kd$ satisfies%
\[
x_{i}=b_{1}x_{i-d}+b_{2}x_{i-2d}+\cdots+b_{k}x_{i-kd}.
\]
(This means that the sequences $\left(  x_{0+u},x_{d+u},x_{2d+u}%
,x_{3d+u},\ldots\right)  $ are $\left(  b_{1},b_{2},\ldots,b_{k}\right)
$-recurrent for all $u\geq0$.)

[\textbf{Hint:} For each $j\geq0$, define the column vector $v_{j}$ by
$v_{j}=\left(
\begin{array}
[c]{c}%
x_{j}\\
x_{j+1}\\
\vdots\\
x_{j+k-1}%
\end{array}
\right)  \in\mathbb{R}^{k}$. Let $A$ be the $k\times k$-matrix $\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 1\\
a_{k} & a_{k-1} & a_{k-2} & \cdots & a_{1}%
\end{array}
\right)  \in\mathbb{R}^{k\times k}$. Start by showing that $Av_{j}=v_{j+1}$
for each $j\geq0$.]
\end{exercise}

\subsection{Sylvester's equation}

We shall next see another application of the Cayley--Hamilton theorem. First,
a notation:

\begin{definition}
\label{def.schurtri.syl.spec}Let $A\in\mathbb{C}^{n\times n}$. Then, the
\emph{spectrum} of $A$ is defined to be the set of all eigenvalues of $A$.
This spectrum is denoted by $\sigma\left(  A\right)  $.
\end{definition}

Some authors write $\operatorname*{spec}A$ instead of $\sigma\left(  A\right)
$. (Some also define it to be a multiset rather than a set; however, the set
suffices for our purposes.)

We now claim the following:

\begin{theorem}
\label{thm.schurtri.syl.equivalence}Let $A\in\mathbb{C}^{n\times n}$ be an
$n\times n$-matrix, and let $B\in\mathbb{C}^{m\times m}$ be an $m\times
m$-matrix (both with complex entries). Let $C\in\mathbb{C}^{n\times m}$ be an
$n\times m$-matrix. Then, the following two statements are equivalent:

\begin{itemize}
\item $\mathcal{U}$: There is a \textbf{unique} matrix $X\in\mathbb{C}%
^{n\times m}$ such that $AX-XB=C$.

\item $\mathcal{V}$: We have $\sigma\left(  A\right)  \cap\sigma\left(
B\right)  =\varnothing$.
\end{itemize}
\end{theorem}

\begin{example}
Let us take $n=1$ and $m=1$, and see what Theorem
\ref{thm.schurtri.syl.equivalence} becomes. In this case, the matrices $A$,
$B$ and $C$ are $1\times1$-matrices, so we can view them as scalars. Let us
therefore write $a$, $b$ and $c$ for them. Then, Theorem
\ref{thm.schurtri.syl.equivalence} says that the following two statements are equivalent:

\begin{itemize}
\item $\mathcal{U}$: There is a \textbf{unique} complex number $x$ such that
$ax-xb=c$.

\item $\mathcal{V}$: We have $\left\{  a\right\}  \cap\left\{  b\right\}
=\varnothing$ (that is, $a\neq b$).
\end{itemize}

This is not surprising, because the linear equation $ax-xb=c$ has a unique
solution (namely, $x=\dfrac{c}{a-b}$) when $a\neq b$, and otherwise has either
none or infinitely many solutions.
\end{example}

The equation $AX-XB=C$ in Theorem \ref{thm.schurtri.syl.equivalence} is known
as \emph{Sylvester's equation}. It is much harder than the superficially
similar equations $AX-BX=C$ and $XA-XB=C$ (see Exercise
\ref{exe.schurtri.syl.AX-BX} for the first of these). In fact, since the $X$
is on different sides in $AX$ and in $XB$, it cannot be factored out from
$AX-XB$ (matrices do not generally commute).

\begin{exercise}
\label{exe.schurtri.syl.AX-BX}\fbox{2} Let $A\in\mathbb{C}^{n\times m}$,
$B\in\mathbb{C}^{n\times m}$ and $C\in\mathbb{C}^{n\times p}$ be three complex
matrices. Prove that there exists a unique matrix $X\in\mathbb{C}^{m\times p}$
such that $AX-BX=C$ if and only if each column of $C$ belongs to the image (=
column space) of $A-B$.
\end{exercise}

We shall prove only the $\mathcal{V}\Longrightarrow\mathcal{U}$ part of
Theorem \ref{thm.schurtri.syl.equivalence}; the opposite direction will be
left as an exercise (Exercise \ref{exe.schurtri.syl.sigA-B} \textbf{(b)}). Our
proof of $\mathcal{V}\Longrightarrow\mathcal{U}$ will rely on the following lemma:

\begin{lemma}
\label{lem.schurtri.syl.AX=XB}Let $\mathbb{F}$ be a field. Let $A\in
\mathbb{F}^{n\times n}$, $B\in\mathbb{F}^{m\times m}$ and $X\in\mathbb{F}%
^{n\times m}$ be three matrices such that $AX=XB$. Then: \medskip

\textbf{(a)} We have $A^{k}X=XB^{k}$ for each $k\in\mathbb{N}$. \medskip

\textbf{(b)} Let $p$ be a polynomial in a single indeterminate $x$ with
coefficients in $\mathbb{F}$. Then, $p\left(  A\right)  X=Xp\left(  B\right)
$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.schurtri.syl.AX=XB}.]\textbf{(a)} Intuitively, this
is easy: For instance, if $k=4$, then this is saying that $A^{4}X=XB^{4}$, but
this follows from
\[
A^{4}B=AAA\underbrace{AB}_{=BA}=AA\underbrace{AB}_{=BA}A=A\underbrace{AB}%
_{=BA}AA=\underbrace{AB}_{=BA}AAA=BAAAA=BA^{4}.
\]


Formally, Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(a)} is proved by
induction on $k$:

\textit{Induction base:} We have $A^{0}X=XB^{0}$, since both sides of this
equation equal $X$. Thus, Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(a)}
holds for $k=0$.

\textit{Induction step:} Let $\ell\in\mathbb{N}$. Assume (as the induction
hypothesis) that Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(a)} holds for
$k=\ell$. We must prove that Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(a)}
holds for $k=\ell+1$.

We have assumed that Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(a)} holds for
$k=\ell$. In other words, $A^{\ell}X=XB^{\ell}$. Thus,%
\[
\underbrace{A^{\ell+1}}_{=AA^{\ell}}X=A\underbrace{A^{\ell}X}_{=XB^{\ell}%
}=\underbrace{AX}_{=XB}B^{\ell}=X\underbrace{BB^{\ell}}_{=B^{\ell+1}}%
=XB^{\ell+1}.
\]
In other words, Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(a)} holds for
$k=\ell+1$. This completes the induction step; thus, Lemma
\ref{lem.schurtri.syl.AX=XB} \textbf{(a)} is proven. \medskip

\textbf{(b)} Write the polynomial $p$ in the form $p\left(  x\right)
=\sum_{k=0}^{d}p_{k}x^{k}$ for some coefficients $p_{0},p_{1},\ldots,p_{d}%
\in\mathbb{F}$. Then, Definition \ref{def.schurtri.normal.p(A)} yields%
\[
p\left(  A\right)  =\sum_{k=0}^{d}p_{k}A^{k}\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ p\left(  B\right)  =\sum_{k=0}^{d}p_{k}B^{k}.
\]
Hence,%
\begin{align*}
p\left(  A\right)  X  &  =\left(  \sum_{k=0}^{d}p_{k}A^{k}\right)
X=\sum_{k=0}^{d}p_{k}\underbrace{A^{k}X}_{\substack{=XB^{k}\\\text{(by Lemma
\ref{lem.schurtri.syl.AX=XB} \textbf{(a)})}}}=\sum_{k=0}^{d}p_{k}%
XB^{k}\ \ \ \ \ \ \ \ \ \ \text{and}\\
Xp\left(  B\right)   &  =X\sum_{k=0}^{d}p_{k}B^{k}=\sum_{k=0}^{d}p_{k}XB^{k}.
\end{align*}
Comparing these two equalities, we find $p\left(  A\right)  X=Xp\left(
B\right)  $. Thus, Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(b)} is proven.
\end{proof}

\begin{proof}
[Proof of the $\mathcal{V}\Longrightarrow\mathcal{U}$ part of Theorem
\ref{thm.schurtri.syl.equivalence}.]First, we observe that the matrix space
$\mathbb{C}^{n\times m}$ is itself a $\mathbb{C}$-vector space of dimension
$nm$.

Consider the map%
\begin{align*}
L:\mathbb{C}^{n\times m}  &  \rightarrow\mathbb{C}^{n\times m},\\
X  &  \mapsto AX-XB.
\end{align*}
This map $L$ is linear, because for any $\alpha,\beta\in\mathbb{C}$ and any
$X,Y\in\mathbb{C}^{n\times m}$, we have%
\begin{align*}
L\left(  \alpha X+\beta Y\right)   &  =A\left(  \alpha X+\beta Y\right)
-\left(  \alpha X+\beta Y\right)  B\\
&  =\alpha AX+\beta AY-\alpha XB-\beta YB\\
&  =\alpha\underbrace{\left(  AX-XB\right)  }_{=L\left(  X\right)  }%
+\beta\underbrace{\left(  AY-YB\right)  }_{=L\left(  Y\right)  }=\alpha
L\left(  X\right)  +\beta L\left(  Y\right)  .
\end{align*}


Now, assume that statement $\mathcal{V}$ holds. That is, we have
$\sigma\left(  A\right)  \cap\sigma\left(  B\right)  =\varnothing$. We shall
now show that $\operatorname*{Ker}L=0$. This will then yield that $L$ is bijective.

Indeed, let $X\in\operatorname*{Ker}L$. Thus, $X\in\mathbb{C}^{n\times m}$ and
$L\left(  X\right)  =0$. However, the definition of $L$ yields $L\left(
X\right)  =AX-XB$. Therefore, $AX-XB=L\left(  X\right)  =0$. In other words,
$AX=XB$. Hence, we can apply Lemma \ref{lem.schurtri.syl.AX=XB}.

Thus, Lemma \ref{lem.schurtri.syl.AX=XB} \textbf{(b)} (applied to $p=p_{A}$)
yields $p_{A}\left(  A\right)  X=Xp_{A}\left(  B\right)  $. However, Theorem
\ref{thm.schurtri.ch.ch} \textbf{(a)} yields $p_{A}\left(  A\right)  =0$, so
that $p_{A}\left(  A\right)  X=0X=0$. Comparing this with $p_{A}\left(
A\right)  X=Xp_{A}\left(  B\right)  $, we obtain $Xp_{A}\left(  B\right)  =0$.

We shall show that the matrix $p_{A}\left(  B\right)  $ is invertible. Indeed,
Theorem \ref{thm.schurtri.ch.fta-cons} \textbf{(a)} shows that the polynomial
$p_{A}$ factors into $n$ linear terms:%
\begin{equation}
p_{A}=\left(  t-\lambda_{1}\right)  \left(  t-\lambda_{2}\right)
\cdots\left(  t-\lambda_{n}\right)  ,
\label{pf.thm.schurtri.syl.equivalence.pA=prod}%
\end{equation}
where $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\in\mathbb{C}$ are its roots.
Moreover, these roots $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ are the
eigenvalues of $A$ (by Theorem \ref{thm.schurtri.ch.fta-cons} \textbf{(b)});
thus, $\left\{  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right\}
=\sigma\left(  A\right)  $.

Substituting the matrix $B$ for $t$ on both sides of the equality
(\ref{pf.thm.schurtri.syl.equivalence.pA=prod}), we obtain%
\begin{equation}
p_{A}\left(  B\right)  =\left(  B-\lambda_{1}I_{n}\right)  \left(
B-\lambda_{2}I_{n}\right)  \cdots\left(  B-\lambda_{n}I_{n}\right)  .
\label{pf.thm.schurtri.syl.equivalence.pAB=prod}%
\end{equation}
(Note that we have tacitly using a not-completely-trivial fact here: that if
$f_{1},f_{2},\ldots,f_{k}$ are several polynomials and $C$ is an $n\times
n$-matrix, then $\left(  f_{1}f_{2}\cdots f_{k}\right)  \left(  C\right)
=f_{1}\left(  C\right)  \cdot f_{2}\left(  C\right)  \cdot\cdots\cdot
f_{k}\left(  C\right)  $. We have applied this fact to $C=B$ and $k=n$ and
$f_{i}=t-\lambda_{i}$ here. This fact is fairly straightforward to show:
Induct on $k$ to reduce it to the case of $k=2$; in that case, write
everything out explicitly in terms of coefficients, like in our proof of Lemma
\ref{lem.schurtri.syl.AX=XB} \textbf{(b)}.)

Now, let $i\in\left[  n\right]  $. Then, $\lambda_{i}\in\left\{  \lambda
_{1},\lambda_{2},\ldots,\lambda_{n}\right\}  =\sigma\left(  A\right)  $.
Therefore, $\lambda_{i}\notin\sigma\left(  B\right)  $ (since having
$\lambda_{i}\in\sigma\left(  B\right)  $ would yield $\lambda_{i}\in
\sigma\left(  A\right)  \cap\sigma\left(  B\right)  $, which would contradict
$\sigma\left(  A\right)  \cap\sigma\left(  B\right)  =\varnothing$). In other
words, $\lambda_{i}$ is not an eigenvalue of $B$. In other words, $\det\left(
\lambda_{i}I_{n}-B\right)  \neq0$ (by the definition of an eigenvalue). Hence,
the matrix $\lambda_{i}I_{n}-B$ is invertible. In other words, the matrix
$B-\lambda_{i}I_{n}$ is invertible (since $B-\lambda_{i}I_{n}=-\left(
\lambda_{i}I_{n}-B\right)  $).

Forget that we fixed $i$. We thus have shown that the matrix $B-\lambda
_{i}I_{n}$ is invertible for each $i\in\left[  n\right]  $. In other words,
the $n$ matrices $B-\lambda_{1}I_{n},B-\lambda_{2}I_{n},\ldots,B-\lambda
_{n}I_{n}$ are invertible. Hence, their product $\left(  B-\lambda_{1}%
I_{n}\right)  \left(  B-\lambda_{2}I_{n}\right)  \cdots\left(  B-\lambda
_{n}I_{n}\right)  $ is invertible as well. In view of
(\ref{pf.thm.schurtri.syl.equivalence.pAB=prod}), this shows that
$p_{A}\left(  B\right)  $ is invertible. Hence, from $Xp_{A}\left(  B\right)
=0$, we conclude that $X=0$.

Now, forget that we fixed $X$. We thus have shown that $X=0$ for each
$X\in\operatorname*{Ker}L$. In other words, $\operatorname*{Ker}L=0$. Hence,
the linear map $L$ is injective.

However, it is well-known that an injective linear map between two
finite-dimensional vector spaces of the same dimension is necessarily
bijective\footnote{\textit{Proof.} Let $f:U\rightarrow V$ be an injective
linear map between two finite-dimensional vector spaces of the same dimension.
We must show that $f$ is bijective. We have $\operatorname*{Ker}f=0$ (since
$f$ is injective) and thus $\dim\left(  \operatorname*{Ker}f\right)  =0$. The
rank-nullity theorem yields%
\[
\dim U=\underbrace{\dim\left(  \operatorname*{Ker}f\right)  }_{\substack{=0}%
}+\dim\left(  \operatorname{Im}f\right)  =\dim\left(  \operatorname{Im}%
f\right)  ,
\]
so that $\dim\left(  \operatorname{Im}f\right)  =\dim U=\dim V$ (since $U$ and
$V$ have the same dimension), and therefore $\operatorname{Im}f=V$ (because
$\operatorname{Im}f$ is a subspace of $V$). This shows that $f$ is surjective.
Since $f$ is also injective, we thus conclude that $f$ is bijective.}. Hence,
$L$ is bijective (since $L$ is an injective linear map between $\mathbb{C}%
^{n\times m}$ and $\mathbb{C}^{n\times m}$). Therefore, there exists a
\textbf{unique} matrix $X\in\mathbb{C}^{n\times m}$ such that $L\left(
X\right)  =C$. In other words, there is a \textbf{unique} matrix
$X\in\mathbb{C}^{n\times m}$ such that $AX-XB=C$ (since $L\left(  X\right)
=AX-XB$). In other words, statement $\mathcal{U}$ holds. Thus, the implication
$\mathcal{V}\Longrightarrow\mathcal{U}$ is proven.
\end{proof}

\begin{exercise}
\label{exe.schurtri.syl.sigA-B}\fbox{5} Let $A$, $B$ and $C$ be as in Theorem
\ref{thm.schurtri.syl.equivalence}. \medskip

\textbf{(a)} Let the linear map $L$ be as in the above proof of the
$\mathcal{V}\Longrightarrow\mathcal{U}$ part of Theorem
\ref{thm.schurtri.syl.equivalence}. Prove that if $\lambda\in\sigma\left(
A\right)  $ and $\mu\in\sigma\left(  B\right)  $, then $\lambda-\mu$ is an
eigenvalue of $L$ (that is, there exists a nonzero matrix $X\in\mathbb{C}%
^{n\times m}$ satisfying $L\left(  X\right)  =\left(  \lambda-\mu\right)  X$).
\medskip

\textbf{(b)} Prove the implication $\mathcal{U}\Longrightarrow\mathcal{V}$ in
Theorem \ref{thm.schurtri.syl.equivalence} (thus completing the proof of the theorem).
\end{exercise}

We note that more can be said: If $A$, $B$ and $C$ are as in Theorem
\ref{thm.schurtri.syl.equivalence}, and if $L$ is as in the above proof, then
\textbf{all} eigenvalues of $L$ have the form $\lambda-\mu$ for $\lambda
\in\sigma\left(  A\right)  $ and $\mu\in\sigma\left(  B\right)  $. But this
seems harder to prove at this point. \medskip%

\[%
\begin{tabular}
[c]{||l||}\hline\hline
\textbf{Lecture 7 starts here.}\\\hline\hline
\end{tabular}
\]


We shall next prove a somewhat surprising consequence of Theorem
\ref{thm.schurtri.syl.equivalence}: a similarity criterion for certain block matrices:

\begin{corollary}
\label{cor.schurtri.block-sim}Let $A\in\mathbb{C}^{n\times n}$, $B\in
\mathbb{C}^{m\times m}$ and $C\in\mathbb{C}^{n\times m}$ be three matrices
such that $\sigma\left(  A\right)  \cap\sigma\left(  B\right)  =\varnothing$.
Then, the two $\left(  n+m\right)  \times\left(  n+m\right)  $-matrices%
\[
\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)
\]
(written in block matrix notation) are similar.
\end{corollary}

\begin{example}
Let $A=\left(
\begin{array}
[c]{cc}%
1 & 3\\
0 & 1
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{c}%
2
\end{array}
\right)  $ and $C=\left(
\begin{array}
[c]{c}%
7\\
9
\end{array}
\right)  $. Then, Corollary \ref{cor.schurtri.block-sim} says that the
matrices%
\[
\left(
\begin{array}
[c]{ccc}%
1 & 3 & 7\\
0 & 1 & 9\\
0 & 0 & 2
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{ccc}%
1 & 3 & 0\\
0 & 1 & 0\\
0 & 0 & 2
\end{array}
\right)
\]
are similar.
\end{example}

\begin{proof}
[Proof of Corollary \ref{cor.schurtri.block-sim}.]Theorem
\ref{thm.schurtri.syl.equivalence} (specifically, its $\mathcal{V}%
\Longrightarrow\mathcal{U}$ direction) shows that there is a unique matrix
$X\in\mathbb{C}^{n\times m}$ such that $AX-XB=C$. Consider this $X$.

Now, let $S=\left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  $. (This is an $\left(  n+m\right)  \times\left(  n+m\right)
$-matrix written in block matrix notation.) Now, I claim that this matrix $S$
is invertible and that%
\[
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  =S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  S^{-1}.
\]
Once this claim is proved, the claim of Corollary \ref{cor.schurtri.block-sim}
will follow (by the definition of \textquotedblleft similar\textquotedblright).

To see that $S$ is invertible, we construct an inverse. Namely, we set
$S^{\prime}=\left(
\begin{array}
[c]{cc}%
I_{n} & -X\\
0 & I_{m}%
\end{array}
\right)  $ (again an $\left(  n+m\right)  \times\left(  n+m\right)  $-matrix).
Then, the definitions of $S$ and $S^{\prime}$ yield%
\begin{align*}
SS^{\prime}  &  =\left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
I_{n} & -X\\
0 & I_{m}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
I_{n}I_{n}+X\cdot0 & I_{n}\left(  -X\right)  +XI_{m}\\
0I_{n}+I_{m}\cdot0 & 0\left(  -X\right)  +I_{m}I_{m}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.blockmatrix.mult-2x2}}\right) \\
&  =\left(
\begin{array}
[c]{cc}%
I_{n} & 0\\
0 & I_{m}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }I_{n}I_{n}+X\cdot0=I_{n}\text{ and }I_{n}\left(  -X\right)
+XI_{m}=-X+X=0\\
\text{and }0I_{n}+I_{m}\cdot0=0\text{ and }0\left(  -X\right)  +I_{m}%
I_{m}=I_{m}%
\end{array}
\right) \\
&  =I_{n+m}%
\end{align*}
and similarly $S^{\prime}S=I_{n+m}$. Thus, the matrices $S$ and $S^{\prime}$
are mutually inverse. Hence, $S$ is invertible.

It remains to check that%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  =S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  S^{-1}. \label{pf.cor.schurtri.block-sim.4}%
\end{equation}
To do so, it suffices to check the equivalent identity%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  S=S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  \label{pf.cor.schurtri.block-sim.5}%
\end{equation}
(indeed, these two identities are equivalent, since $S$ is invertible). This
we do by computing both sides and comparing: Namely, the definition of $S$
yields%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  S  &  =\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
AI_{n}+0\cdot0 & A\cdot X+0\cdot I_{m}\\
0I_{n}+B\cdot0 & 0X+BI_{m}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.blockmatrix.mult-2x2}}\right) \\
&  =\left(
\begin{array}
[c]{cc}%
A & AX\\
0 & B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the obvious simplifications}%
\right)
\end{align*}
and%
\begin{align*}
S\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cc}%
I_{n} & X\\
0 & I_{m}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
I_{n}A+X\cdot0 & I_{n}C+XB\\
0A+I_{m}\cdot0 & 0C+I_{m}\cdot B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.blockmatrix.mult-2x2}}\right) \\
&  =\left(
\begin{array}
[c]{cc}%
A & C+XB\\
0 & B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the obvious simplifications}%
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
A & AX\\
0 & B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \ \left(  \text{since }AX-XB=C\text{ entails
}C+XB=AX\right)  .
\end{align*}
Comparing these two equalities yields (\ref{pf.cor.schurtri.block-sim.5}).
Thus, we obtain (\ref{pf.cor.schurtri.block-sim.4}) (by multiplying both sides
of (\ref{pf.cor.schurtri.block-sim.5}) with $S^{-1}$ from the right). But this
shows that the two matrices $\left(
\begin{array}
[c]{cc}%
A & C\\
0 & B
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cc}%
A & 0\\
0 & B
\end{array}
\right)  $ are similar. This proves Corollary \ref{cor.schurtri.block-sim}.
\end{proof}

\section{The Jordan canonical form (\cite[Chapter 3]{HorJoh13})}

This chapter is devoted to the \emph{Jordan canonical form} (and some of its
variants), which is a normal form for $n\times n$-matrices over $\mathbb{C}$
with respect to similarity. This means that each $n\times n$-matrix is similar
to a more-or-less unique matrix of a certain kind (namely, a block-diagonal
matrix made of a specific type of blocks), called its \textquotedblleft Jordan
canonical form\textquotedblright.

We recall that the notation \textquotedblleft$A\sim B$\textquotedblright%
\ (where $A$ and $B$ are two $n\times n$-matrices) means that the matrices $A$
and $B$ are similar.

\subsection{Jordan cells}

The building blocks for the Jordan canonical form are the so-called Jordan
cells. Let us define them:

\begin{definition}
\label{def.jnf.jcell}Let $\mathbb{F}$ be a field. A \emph{Jordan cell} is an
$m\times m$-matrix of the form%
\[
\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 & 0 & \cdots & 0\\
0 & \lambda & 1 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{for some }m>0\text{ and some }\lambda
\in\mathbb{F}.
\]
In other words, it is an $m\times m$-matrix

\begin{itemize}
\item whose diagonal entries are $\lambda$,

\item whose entries directly above the diagonal (i.e., just one step upwards
from a diagonal entry) are $1$, and

\item whose all remaining entries are $0$.
\end{itemize}

\noindent In formal terms, it is the $m\times m$-matrix $A$ whose entries are
given by the rule%
\[
A_{i,j}=%
\begin{cases}
\lambda, & \text{if }i=j;\\
1, & \text{if }i=j-1;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left[  m\right]  \text{ and }%
j\in\left[  m\right]  .
\]


To be specific, this matrix is called the \emph{Jordan cell of size }$m$\emph{
at eigenvalue }$\lambda$. It is denoted by $J_{m}\left(  \lambda\right)  $.
\end{definition}

\begin{example}
\textbf{(a)} The Jordan cell of size $3$ at eigenvalue $-5$ is%
\[
J_{3}\left(  -5\right)  =\left(
\begin{array}
[c]{ccc}%
-5 & 1 & 0\\
0 & -5 & 1\\
0 & 0 & -5
\end{array}
\right)  .
\]


\textbf{(b)} The Jordan cell of size $2$ at eigenvalue $\pi$ is%
\[
J_{2}\left(  \pi\right)  =\left(
\begin{array}
[c]{cc}%
\pi & 1\\
0 & \pi
\end{array}
\right)  .
\]


\textbf{(c)} For any $\lambda\in\mathbb{F}$, the Jordan cell of size $1$ at
eigenvalue $\lambda$ is the $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
\lambda
\end{array}
\right)  $.
\end{example}

\begin{remark}
We will chiefly use Jordan cells as building blocks for the Jordan normal
form. However, they are of some independent interest. In particular, they
serve as matrix representations for several useful linear maps.

For example, fix $m\in\mathbb{N}$, and let $P_{m}$ be the $\mathbb{C}$-vector
space of all polynomials in a single variable $t$ of degree $<m$ (with complex
coefficients). Then, $P_{m}$ has a basis $\left(  t^{0},t^{1},\ldots
,t^{m-1}\right)  $. The derivative operator $\dfrac{d}{dt}:P_{m}\rightarrow
P_{m}$ (which sends each polynomial $f\in P_{m}$ to its derivative $f^{\prime
}$) is a $\mathbb{C}$-linear map that is represented by the matrix%
\[
\left(
\begin{array}
[c]{cccccc}%
0 & 1 & 0 & 0 & \cdots & 0\\
0 & 0 & 2 & 0 & \cdots & 0\\
0 & 0 & 0 & 3 & \cdots & 0\\
0 & 0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & \cdots & 0
\end{array}
\right)
\]
with respect to this basis. This matrix has the numbers $1,2,\ldots,m-1$ in
the cells directly above the main diagonal, and $0$s everywhere else. It is
not quite a Jordan cell. However, if we instead use the basis $\left(
\dfrac{t^{0}}{0!},\dfrac{t^{1}}{1!},\ldots,\dfrac{t^{m-1}}{\left(  m-1\right)
!}\right)  $, then the operator $\dfrac{d}{dt}$ is represented by the matrix
\[
\left(
\begin{array}
[c]{cccccc}%
0 & 1 & 0 & 0 & \cdots & 0\\
0 & 0 & 1 & 0 & \cdots & 0\\
0 & 0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & \cdots & 0
\end{array}
\right)
\]
which is precisely the Jordan cell $J_{m}\left(  0\right)  $. (This basis is
just a rescaled version of the basis $\left(  t^{0},t^{1},\ldots
,t^{m-1}\right)  $, where the rescaling factors have been chosen to
\textquotedblleft normalize\textquotedblright\ the $1,2,\ldots,m-1$ entries to
be $1$s.)
\end{remark}

While the Jordan cell $J_{m}\left(  \lambda\right)  $ depends on both $m$ and
$\lambda$, its dependence on $k$ is not very substantial:

\begin{proposition}
\label{prop.jnf.jcell-lambda}Let $\mathbb{F}$ be a field. Let $m$ be a
positive integer, and let $\lambda\in\mathbb{F}$. Then,%
\[
J_{m}\left(  \lambda\right)  =J_{m}\left(  0\right)  +\lambda I_{m}.
\]

\end{proposition}

\begin{proof}
Definition \ref{def.jnf.jcell} yields%
\begin{equation}
J_{m}\left(  \lambda\right)  =\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 & 0 & \cdots & 0\\
0 & \lambda & 1 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  \label{pf.prop.jnf.jcell-lambda.1}%
\end{equation}
and%
\begin{equation}
J_{m}\left(  0\right)  =\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  . \label{pf.prop.jnf.jcell-lambda.2}%
\end{equation}
On the other hand,%
\[
\lambda I_{m}=\left(
\begin{array}
[c]{ccccc}%
\lambda & 0 & 0 & \cdots & 0\\
0 & \lambda & 0 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  .
\]
Adding this equality to (\ref{pf.prop.jnf.jcell-lambda.2}), we obtain%
\begin{align*}
J_{m}\left(  0\right)  +\lambda I_{m}  &  =\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  +\left(
\begin{array}
[c]{ccccc}%
\lambda & 0 & 0 & \cdots & 0\\
0 & \lambda & 0 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 & 0 & \cdots & 0\\
0 & \lambda & 1 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  =J_{m}\left(  \lambda\right)
\end{align*}
(by (\ref{pf.prop.jnf.jcell-lambda.1})). This proves Proposition
\ref{prop.jnf.jcell-lambda}.
\end{proof}

Thanks to Proposition \ref{prop.jnf.jcell-lambda}, we can reduce many
questions about $J_{m}\left(  \lambda\right)  $ to the corresponding questions
about $J_{m}\left(  0\right)  $. Let us compute the powers of $J_{m}\left(
0\right)  $:

\begin{proposition}
\label{prop.jnf.jcell0-powers}Let $\mathbb{F}$ be a field. Let $m$ be a
positive integer. Let $B=J_{m}\left(  0\right)  $. Let $p\in\mathbb{N}$. Then:
\medskip

\textbf{(a)} The entries of the $m\times m$-matrix $B^{p}$ are given by%
\[
\left(  B^{p}\right)  _{i,j}=%
\begin{cases}
1, & \text{if }i=j-p;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left[  m\right]  .
\]


\textbf{(b)} We have $B^{p}=0$ if $p\geq m$. \medskip

\textbf{(c)} We have $\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)
\right)  =p$ if $p\leq m$. \medskip

\textbf{(d)} We have $\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)
\right)  =m$ if $p\geq m$.
\end{proposition}

\begin{example}
For $m=4$, the matrix $B=J_{4}\left(  0\right)  $ from Proposition
\ref{prop.jnf.jcell0-powers} satisfies%
\begin{align*}
B  &  =\left(
\begin{array}
[c]{cccc}%
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ B^{2}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  ,\\
B^{3}  &  =\left(
\begin{array}
[c]{cccc}%
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ B^{4}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  =0.
\end{align*}
Thus, as we go from $B$ to $B^{2}$ to $B^{3}$ to $B^{4}$, the $1$s in the
cells directly above the main diagonal recede further and further upwards,
until they eventually disappear beyond the borders of the matrix. (It is
actually better to start this sequence with $B^{0}$ rather than $B$, so that
the $1$s start on the main diagonal.) Proposition \ref{prop.jnf.jcell0-powers}
\textbf{(a)} is merely a formal way of stating this phenomenon. Parts
\textbf{(b)}, \textbf{(c)} and \textbf{(d)} of Proposition
\ref{prop.jnf.jcell0-powers} follow easily from part \textbf{(a)}.
\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.jnf.jcell0-powers}.]We have%
\begin{equation}
B=J_{m}\left(  0\right)  =\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  \label{pf.prop.jnf.jcell0-powers.B=}%
\end{equation}
(by the definition of $J_{m}\left(  0\right)  $). \medskip

\textbf{(a)} We can prove Proposition \ref{prop.jnf.jcell0-powers}
\textbf{(a)} by induction on $p$, using the definition of matrix
multiplication (and the fact that $B^{q+1}=B^{q}B$ for each $q\in\mathbb{N}$).
However, there is a more elegant proof using the action of $B$ on basis
vectors: \medskip

Forget that we fixed $p$. For each $i\in\left[  m\right]  $, let $e_{i}$ be
the column vector in $\mathbb{F}^{m}$ whose $i$-th entry is $1$ while all its
other entries are $0$. That is,
\[
e_{i}=\left(
\begin{array}
[c]{ccccccccc}%
0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0
\end{array}
\right)  ^{T},
\]
where the $1$ is in the $i$-th position. The vectors $e_{1},e_{2},\ldots
,e_{m}$ are the standard basis vectors of $\mathbb{F}^{m}$. It is well-known
that every $m\times m$-matrix $C\in\mathbb{F}^{m\times m}$ satisfies%
\begin{equation}
C_{\bullet,i}=Ce_{i}\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  m\right]
. \label{pf.prop.jnf.jcell0-powers.a.coli}%
\end{equation}
(Recall that $C_{\bullet,i}$ denotes the $i$-th column of $C$.)

We have so far defined the vectors $e_{i}$ only for $i\in\left[  m\right]  $.
Now, for each integer $i\notin\left[  m\right]  $, we define $e_{i}$ to be the
zero vector $0\in\mathbb{F}^{m}$. Thus, we have defined a vector $e_{i}%
\in\mathbb{F}^{m}$ for each $i\in\mathbb{Z}$ (although it is nonzero only when
$i\in\left[  m\right]  $). In particular, $e_{0}=0$ (since $0\notin\left[
m\right]  $). Note that we have%
\begin{equation}
\left(  \text{the }k\text{-th entry of the column vector }e_{i}\right)  =%
\begin{cases}
1, & \text{if }k=i;\\
0, & \text{otherwise}%
\end{cases}
\label{pf.prop.jnf.jcell0-powers.a.ient}%
\end{equation}
for each $i\in\mathbb{Z}$ and each $k\in\left[  m\right]  $%
\ \ \ \ \footnote{\textit{Proof.} If $i\in\left[  m\right]  $, then the
equality (\ref{pf.prop.jnf.jcell0-powers.a.ient}) follows from the definition
of $e_{i}$. On the other hand, if $i\notin\left[  m\right]  $, then $e_{i}=0$
(by definition), so that both sides of the equality
(\ref{pf.prop.jnf.jcell0-powers.a.ient}) are $0$ (since we don't have $k=i$
(because $k\in\left[  m\right]  $ and $i\notin\left[  m\right]  $), and thus
we have $%
\begin{cases}
1, & \text{if }k=i;\\
0, & \text{otherwise}%
\end{cases}
\ \ =0$). Thus, the equality (\ref{pf.prop.jnf.jcell0-powers.a.ient}) holds in
either case.}.

Now, from (\ref{pf.prop.jnf.jcell0-powers.B=}), we see that the columns of the
matrix $B$ are $0,e_{1},e_{2},\ldots,e_{m-1}$ in this order. In other words,
the columns of $B$ are $e_{0},e_{1},e_{2},\ldots,e_{m-1}$ in this order (since
$e_{0}=0$). In other words, we have%
\[
B_{\bullet,i}=e_{i-1}\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  m\right]
\]
(since $B_{\bullet,i}$ denotes the $i$-th column of $B$). However,
(\ref{pf.prop.jnf.jcell0-powers.a.coli}) (applied to $C=B$) shows that we have%
\[
B_{\bullet,i}=Be_{i}\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  m\right]
.
\]
Comparing these two equalities, we obtain%
\begin{equation}
Be_{i}=e_{i-1}\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  m\right]  .
\label{pf.prop.jnf.jcell0-powers.a.Bei}%
\end{equation}
However, this equality also holds for all $i\leq0$ (because if $i\leq0$, then
both $e_{i}$ and $e_{i-1}$ equal the zero vector $0$ (since $i\notin\left[
m\right]  $ and $i-1\notin\left[  m\right]  $), and therefore this equality
boils down to $B\cdot0=0$). Thus,
\begin{equation}
Be_{i}=e_{i-1}\ \ \ \ \ \ \ \ \ \ \text{for each integer }i\leq m.
\label{pf.prop.jnf.jcell0-powers.a.Bei2}%
\end{equation}


Now, we claim that%
\begin{equation}
B^{p}e_{i}=e_{i-p}\ \ \ \ \ \ \ \ \ \ \text{for each }p\in\mathbb{N}\text{ and
}i\in\left[  m\right]  . \label{pf.prop.jnf.jcell0-powers.a.Bpei}%
\end{equation}


[\textit{Proof of (\ref{pf.prop.jnf.jcell0-powers.a.Bpei}):} We induct on $p$:

\textit{Induction base:} We have $B^{0}=I_{m}$ and thus $B^{0}e_{i}=I_{m}%
e_{i}=e_{i}=e_{i-0}$ for each $i\in\left[  m\right]  $. In other words,
(\ref{pf.prop.jnf.jcell0-powers.a.Bpei}) holds for $p=0$.

\textit{Induction step:} Let $q\in\mathbb{N}$. Assume that
(\ref{pf.prop.jnf.jcell0-powers.a.Bpei}) holds for $p=q$. We must prove that
(\ref{pf.prop.jnf.jcell0-powers.a.Bpei}) holds for $p=q+1$.

We have assumed that (\ref{pf.prop.jnf.jcell0-powers.a.Bpei}) holds for $p=q$.
In other words, we have $B^{q}e_{i}=e_{i-q}$ for each $i\in\left[  m\right]
$. Now, let $i\in\left[  m\right]  $ be arbitrary. As we have just seen, we
have $B^{q}e_{i}=e_{i-q}$. However, from $q\geq0$, we obtain $i-q\leq i\leq m$
(since $i\in\left[  m\right]  $). Thus,
(\ref{pf.prop.jnf.jcell0-powers.a.Bei2}) (applied to $i-q$ instead of $i$)
yields $Be_{i-q}=e_{i-q-1}$. Now,%
\[
\underbrace{B^{q+1}}_{=BB^{q}}e_{i}=B\underbrace{B^{q}e_{i}}_{=e_{i-q}%
}=Be_{i-q}=e_{i-q-1}=e_{i-\left(  q+1\right)  }.
\]


Forget that we fixed $i$. We thus have shown that $B^{q+1}e_{i}=e_{i-\left(
q+1\right)  }$ for each $i\in\left[  m\right]  $. In other words,
(\ref{pf.prop.jnf.jcell0-powers.a.Bpei}) holds for $p=q+1$. This completes the
induction step. Thus, (\ref{pf.prop.jnf.jcell0-powers.a.Bpei}) is proved.]
\medskip

Now, let $p\in\mathbb{N}$ and let $i,j\in\left[  m\right]  $. Then,%
\begin{align*}
\left(  B^{p}\right)  _{\bullet,j}  &  =B^{p}e_{j}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.prop.jnf.jcell0-powers.a.coli}), applied to }B^{p}\text{ and
}j\text{ instead of }C\text{ and }i\right) \\
&  =e_{j-p}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.jnf.jcell0-powers.a.Bpei}), applied to }j\text{ instead of
}i\right)  .
\end{align*}
Furthermore,%
\begin{align*}
\left(  B^{p}\right)  _{i,j}  &  =\left(  \text{the }i\text{-th entry of the
column vector }\underbrace{\left(  B^{p}\right)  _{\bullet,j}}_{=e_{j-p}%
}\right) \\
&  =\left(  \text{the }i\text{-th entry of the column vector }e_{j-p}\right)
\\
&  =%
\begin{cases}
1, & \text{if }i=j-p;\\
0, & \text{otherwise}%
\end{cases}
\end{align*}
(by (\ref{pf.prop.jnf.jcell0-powers.a.ient}), applied to $i$ and $j-p$ instead
of $k$ and $i$). This proves Proposition \ref{prop.jnf.jcell0-powers}
\textbf{(a)}. \medskip

\textbf{(b)} Assume that $p\geq m$. Let $i,j\in\left[  m\right]  $ be
arbitrary. Then, $i\geq1$ and $j\leq m$. Hence, $\underbrace{j}_{\leq m}-p\leq
m-p\leq0$ (since $p\geq m$), so that $0\geq j-p$. On the other hand,
$i\geq1>0\geq j-p$. Therefore, $i\neq j-p$. However, Proposition
\ref{prop.jnf.jcell0-powers} \textbf{(a)} yields
\[
\left(  B^{p}\right)  _{i,j}=%
\begin{cases}
1, & \text{if }i=j-p;\\
0, & \text{otherwise}%
\end{cases}
\ \ =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq j-p\right)  .
\]


Forget that we fixed $i$ and $j$. We thus have shown that $\left(
B^{p}\right)  _{i,j}=0$ for all $i,j\in\left[  m\right]  $. In other words,
all entries of the matrix $B^{p}$ equal $0$. Thus, $B^{p}=0$. This proves
Proposition \ref{prop.jnf.jcell0-powers} \textbf{(b)}. \medskip

\textbf{(c)} Assume that $p\leq m$. We shall use the column vectors $e_{i}%
\in\mathbb{F}^{m}$ that were defined for all $i\in\mathbb{Z}$ in our above
proof of Proposition \ref{prop.jnf.jcell0-powers} \textbf{(a)}.

Let $v=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{m}%
\end{array}
\right)  \in\mathbb{F}^{m}$ be a column vector. Thus,
\[
v=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{m}%
\end{array}
\right)  =v_{1}e_{1}+v_{2}e_{2}+\cdots+v_{m}e_{m}=\sum_{i=1}^{m}v_{i}e_{i}.
\]
Hence,%
\begin{align*}
B^{p}v  &  =B^{p}\cdot\sum_{i=1}^{m}v_{i}e_{i}=\sum_{i=1}^{m}v_{i}%
\underbrace{B^{p}e_{i}}_{\substack{=e_{i-p}\\\text{(by
(\ref{pf.prop.jnf.jcell0-powers.a.Bpei}))}}}=\sum_{i=1}^{m}v_{i}e_{i-p}\\
&  =\sum_{i=1}^{p}v_{i}\underbrace{e_{i-p}}_{\substack{=0\\\text{(since
}i-p\notin\left[  m\right]  \\\text{(because }i\leq p\text{ and thus }%
i-p\leq0\text{))}}}+\sum_{i=p+1}^{m}v_{i}e_{i-p}=\underbrace{\sum_{i=1}%
^{p}v_{i}0}_{=0}+\sum_{i=p+1}^{m}v_{i}e_{i-p}\\
&  =\sum_{i=p+1}^{m}v_{i}e_{i-p}=\sum_{j=1}^{m-p}v_{j+p}e_{j}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }j+p\text{ for }i\\
\text{in the sum}%
\end{array}
\right) \\
&  =v_{p+1}e_{1}+v_{p+2}e_{2}+\cdots+v_{m}e_{m-p}=\left(
\begin{array}
[c]{c}%
v_{p+1}\\
v_{p+2}\\
\vdots\\
v_{m}\\
0\\
0\\
\vdots\\
0
\end{array}
\right)  .
\end{align*}
Thus, $B^{p}v=0$ holds if and only if $v_{p+1}=v_{p+2}=\cdots=v_{m}=0$. In
other words, $B^{p}v=0$ holds if and only if $v\in\operatorname*{span}\left(
e_{1},e_{2},\ldots,e_{p}\right)  $ (because $v_{p+1}=v_{p+2}=\cdots=v_{m}=0$
is equivalent to $v\in\operatorname*{span}\left(  e_{1},e_{2},\ldots
,e_{p}\right)  $).

Now, forget that we fixed $v$. We thus have shown that a vector $v\in
\mathbb{F}^{m}$ satisfies $B^{p}v=0$ if and only if $v\in\operatorname*{span}%
\left(  e_{1},e_{2},\ldots,e_{p}\right)  $. Thus,
\[
\operatorname*{Ker}\left(  B^{p}\right)  =\operatorname*{span}\left(
e_{1},e_{2},\ldots,e_{p}\right)
\]
(since $\operatorname*{Ker}\left(  B^{p}\right)  $ is defined as the set of
all vectors $v\in\mathbb{F}^{m}$ satisfying $B^{p}v=0$). Therefore,%
\[
\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)  \right)  =\dim\left(
\operatorname*{span}\left(  e_{1},e_{2},\ldots,e_{p}\right)  \right)  =p
\]
(since $\operatorname*{span}\left(  e_{1},e_{2},\ldots,e_{p}\right)  $ is
clearly a $p$-dimensional subspace of $\mathbb{F}^{m}$). This proves
Proposition \ref{prop.jnf.jcell0-powers} \textbf{(c)}. \medskip

\textbf{(d)} Assume that $p\geq m$. Then, Proposition
\ref{prop.jnf.jcell0-powers} \textbf{(b)} yields $B^{p}=0$. Hence,
$\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)  \right)  =\dim
\underbrace{\left(  \operatorname*{Ker}0\right)  }_{=\mathbb{F}^{m}}%
=\dim\left(  \mathbb{F}^{m}\right)  =0$. Thus, Proposition
\ref{prop.jnf.jcell0-powers} \textbf{(d)} is proven.
\end{proof}

\begin{proposition}
\label{prop.jnf.jcell.multiplicity}Let $m$ be a positive integer, and let
$\lambda\in\mathbb{C}$. The only eigenvalue of the matrix $J_{m}\left(
\lambda\right)  $ is $\lambda$. This eigenvalue has algebraic multiplicity $m$
and geometric multiplicity $1$.
\end{proposition}

\begin{proof}
The definition of $J_{m}\left(  \lambda\right)  $ yields%
\[
J_{m}\left(  \lambda\right)  =\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 & 0 & \cdots & 0\\
0 & \lambda & 1 & \cdots & 0\\
0 & 0 & \lambda & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & \lambda
\end{array}
\right)  .
\]
This matrix is upper-triangular, so its characteristic polynomial is%
\[
p_{J_{m}\left(  \lambda\right)  }=\left(  t-\lambda\right)  \left(
t-\lambda\right)  \cdots\left(  t-\lambda\right)  =\left(  t-\lambda\right)
^{m}.
\]
Thus, the only root of this polynomial is $\lambda$. In other words, the only
eigenvalue of this matrix $J_{m}\left(  \lambda\right)  $ is $\lambda$. Its
algebraic multiplicity is $m$ (since this is its multiplicity as a root of
$p_{J_{m}\left(  \lambda\right)  }$). It remains to show that its geometric
multiplicity is $1$.

Since the geometric multiplicity of $\lambda$ is defined as $\dim\left(
\operatorname*{Ker}\left(  J_{m}\left(  \lambda\right)  -\lambda I_{m}\right)
\right)  $, this means that it remains to show that $\dim\left(
\operatorname*{Ker}\left(  J_{m}\left(  \lambda\right)  -\lambda I_{m}\right)
\right)  =1$.

Let $B=J_{m}\left(  0\right)  $. Proposition \ref{prop.jnf.jcell0-powers}
\textbf{(c)} (applied to $p=1$) yields $\dim\left(  \operatorname*{Ker}\left(
B^{1}\right)  \right)  =1$ (since $1\leq m$). In other words, $\dim\left(
\operatorname*{Ker}B\right)  =1$.

Proposition \ref{prop.jnf.jcell-lambda} yields%
\[
J_{m}\left(  \lambda\right)  =\underbrace{J_{m}\left(  0\right)  }%
_{=B}+\lambda I_{m}=B+\lambda I_{m},
\]
so that $J_{m}\left(  \lambda\right)  -\lambda I_{m}=B$. Hence,
\[
\dim\left(  \operatorname*{Ker}\underbrace{\left(  J_{m}\left(  \lambda
\right)  -\lambda I_{m}\right)  }_{=B}\right)  =\dim\left(
\operatorname*{Ker}B\right)  =1.
\]
This completes our proof of Proposition \ref{prop.jnf.jcell.multiplicity}.
\end{proof}

\subsection{Jordan canonical form: the theorem}

Let us now build larger matrices out of Jordan cells:

\begin{definition}
\label{def.jnf.jmat}Let $\mathbb{F}$ be a field. A \emph{Jordan matrix} means
a block-diagonal matrix whose diagonal blocks are Jordan cells. In other
words, it is a matrix of the form%
\[
\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  ,
\]
where $n_{1},n_{2},\ldots,n_{k}$ are positive integers and $\lambda
_{1},\lambda_{2},\ldots,\lambda_{k}$ are scalars in $\mathbb{F}$ (not
necessarily distinct, but not necessarily equal either).
\end{definition}

We note that any Jordan matrix is upper-triangular. \medskip

We claim the following:\footnote{Recall that \textquotedblleft$A\sim
B$\textquotedblright\ means that the matrix $A$ is similar to the matrix $B$.}

\begin{theorem}
[Jordan canonical form theorem]\label{thm.jnf.jnf}Let $A\in\mathbb{C}^{n\times
n}$ be an $n\times n$-matrix over $\mathbb{C}$. Then:

\textbf{(a)} There exists a Jordan matrix $J$ such that $A\sim J$.

\textbf{(b)} This Jordan matrix $J$ is unique up to the order of the diagonal blocks.
\end{theorem}

This theorem is useful partly (but not only) because it allows to reduce
questions about general square matrices to questions about Jordan matrices.
And the latter can usually further be reduced to questions about Jordan cells,
because a block-diagonal matrix \textquotedblleft behaves like its diagonal
blocks are separate\textquotedblright\ (see, e.g., the discussion before
Proposition \ref{prop.blockmatrix.nullity-diag}).

Note that we cannot hope for the matrix $J$ in Theorem \ref{thm.jnf.jnf} to be
fully unique, unless it has only one diagonal block (i.e., unless $k=1$).
Indeed, Proposition \ref{prop.blockmatrix.mult-uxv} shows that if we permute
the diagonal blocks $J_{n_{1}}\left(  \lambda_{1}\right)  ,J_{n_{2}}\left(
\lambda_{2}\right)  ,\ldots,J_{n_{k}}\left(  \lambda_{k}\right)  $, then the
matrix stays similar to $A$. Thus, the order of these diagonal blocks can be
chosen arbitrary.

\begin{definition}
\label{def.jnf.jnf}Let $A$ be an $n\times n$-matrix over $\mathbb{C}$. Theorem
\ref{thm.jnf.jnf} \textbf{(a)} says that there exists a Jordan matrix $J$ such
that $A\sim J$. Such a matrix $J$ is called a \emph{Jordan canonical form} of
$A$ (or a \emph{Jordan normal form} of $A$).

We often use the definite article (\textquotedblleft the Jordan canonical form
of $A$\textquotedblright), because Theorem \ref{thm.jnf.jnf} \textbf{(b)} says
that $J$ is \textquotedblleft more or less unique\textquotedblright. (Strictly
speaking, of course, it is not entirely appropriate.)

The diagonal blocks $J_{n_{1}}\left(  \lambda_{1}\right)  ,J_{n_{2}}\left(
\lambda_{2}\right)  ,\ldots,J_{n_{k}}\left(  \lambda_{k}\right)  $ of $J$ are
called the \emph{Jordan blocks} (or \emph{Jordan cells}) of $A$.

We often abbreviate \textquotedblleft Jordan canonical form\textquotedblright%
\ as \textquotedblleft JCF\textquotedblright.
\end{definition}

\begin{example}
A Jordan canonical form of $\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
0 & 2 & 5\\
0 & 0 & 1
\end{array}
\right)  $ is $\left(
\begin{array}
[c]{ccc}%
2 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  $. Indeed, $\left(
\begin{array}
[c]{ccc}%
2 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  $ is a Jordan matrix (it can be written as $\left(
\begin{array}
[c]{cc}%
J_{1}\left(  2\right)  & 0\\
0 & J_{2}\left(  1\right)
\end{array}
\right)  $) and it can be checked that%
\[
\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
0 & 2 & 5\\
0 & 0 & 1
\end{array}
\right)  \sim\left(
\begin{array}
[c]{ccc}%
2 & 0 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  .
\]
We can swap the two Jordan cells in this Jordan matrix, and obtain another
Jordan canonical form of the same matrix: $\left(
\begin{array}
[c]{ccc}%
1 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 2
\end{array}
\right)  $.
\end{example}

\begin{example}
If $D$ is a diagonal matrix, then $D$ itself is a Jordan canonical form of
$D$. Indeed, each diagonal entry $\lambda$ of $D$ can be viewed as a Jordan
cell of size $1$ (namely, $J_{1}\left(  \lambda\right)  $), so that $D$ is a
Jordan matrix.
\end{example}

\subsection{Jordan canonical form: proof of uniqueness}

We shall approach the proof of Theorem \ref{thm.jnf.jnf} slowly, making sure
to record all auxiliary results obtained on the way (as they are themselves
rather useful). We first try to explore how much of the structure of the
Jordan normal form $J$ can be read off the matrix $A$. This will lead us to
the proof of the uniqueness part (i.e., part \textbf{(b)}) of Theorem
\ref{thm.jnf.jnf}.

We start with an example:

\begin{example}
\label{exa.jnf.unique.exa1}Let $A\in\mathbb{C}^{7\times7}$. Suppose that
$A\sim J$ with%
\[
J=\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  8\right)  & 0 & 0\\
0 & J_{3}\left(  8\right)  & 0\\
0 & 0 & J_{2}\left(  9\right)
\end{array}
\right)  =\left(
\begin{array}
[c]{ccccccc}%
8 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 8 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 8 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 8 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 8 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 9 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 9
\end{array}
\right)  .
\]
How are the structural elements of this matrix $J$ (that is, the diagonal
entries $8$ and $9$ and the sizes $2,3,2$ of the diagonal blocks) reflected in
the matrix $A$ ?

First, the matrix $J$ is a Jordan matrix, and thus upper-triangular. Hence,
Proposition \ref{prop.schurtri.similar.T-diag} (applied to $T=J$) shows that
the diagonal entries of $J$ are the eigenvalues of $A$ (with their algebraic
multiplicities). Thus, the eigenvalues of $A$ are $8$ and $9$, with respective
algebraic multiplicities $5$ and $2$.

Now, what about the geometric multiplicities? That is, what are $\dim\left(
\operatorname*{Ker}\left(  A-8I_{7}\right)  \right)  $ and $\dim\left(
\operatorname*{Ker}\left(  A-9I_{7}\right)  \right)  $ ?

From $A\sim J$, we obtain $A-8I_{7}\sim J-8I_{7}$ (by Proposition
\ref{prop.schurtri.similar.same} \textbf{(h)}, applied to $B=J$ and
$\lambda=8$ and $n=7$). Thus, Proposition \ref{prop.schurtri.similar.same}
\textbf{(b)} (applied to $A-8I_{7}$, $J-8I_{7}$ and $7$ instead of $A$, $B$
and $8$) yields that the matrices $A-8I_{7}$ and $J-8I_{7}$ have the same
nullity. In other words,%
\begin{align*}
\dim\left(  \operatorname*{Ker}\left(  A-8I_{7}\right)  \right)   &
=\dim\left(  \operatorname*{Ker}\left(  J-8I_{7}\right)  \right) \\
&  =\dim\left(  \operatorname*{Ker}\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  8\right)  -8I_{2} & 0 & 0\\
0 & J_{3}\left(  8\right)  -8I_{3} & 0\\
0 & 0 & J_{2}\left(  9\right)  -8I_{2}%
\end{array}
\right)  \right)
\end{align*}
(since%
\begin{align*}
J-8I_{7}  &  =\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  8\right)  & 0 & 0\\
0 & J_{3}\left(  8\right)  & 0\\
0 & 0 & J_{2}\left(  9\right)
\end{array}
\right)  -8I_{7}\\
&  =\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  8\right)  -8I_{2} & 0 & 0\\
0 & J_{3}\left(  8\right)  -8I_{3} & 0\\
0 & 0 & J_{2}\left(  9\right)  -8I_{2}%
\end{array}
\right)
\end{align*}
). Thus,%
\begin{align*}
&  \dim\left(  \operatorname*{Ker}\left(  A-8I_{7}\right)  \right) \\
&  =\dim\left(  \operatorname*{Ker}\left(
\begin{array}
[c]{ccc}%
J_{2}\left(  8\right)  -8I_{2} & 0 & 0\\
0 & J_{3}\left(  8\right)  -8I_{3} & 0\\
0 & 0 & J_{2}\left(  9\right)  -8I_{2}%
\end{array}
\right)  \right) \\
&  =\dim\left(  \operatorname*{Ker}\left(  J_{2}\left(  8\right)
-8I_{2}\right)  \right)  +\dim\left(  \operatorname*{Ker}\left(  J_{3}\left(
8\right)  -8I_{3}\right)  \right)  +\dim\left(  \operatorname*{Ker}\left(
J_{2}\left(  9\right)  -8I_{2}\right)  \right)
\end{align*}
(by Proposition \ref{prop.blockmatrix.nullity-diag}). Now, let us find the
three dimensions on the right hand side.

Proposition \ref{prop.jnf.jcell-lambda} yields $J_{2}\left(  8\right)
=J_{2}\left(  0\right)  +2I_{2}$, so that $J_{2}\left(  8\right)
-8I_{2}=J_{2}\left(  0\right)  $. Hence,%
\[
\dim\left(  \operatorname*{Ker}\left(  J_{2}\left(  8\right)  -8I_{2}\right)
\right)  =\dim\left(  \operatorname*{Ker}\left(  J_{2}\left(  0\right)
\right)  \right)  =\dim\left(  \operatorname*{Ker}\left(  \left(  J_{2}\left(
0\right)  \right)  ^{1}\right)  \right)  =1
\]
(by Proposition \ref{prop.jnf.jcell0-powers} \textbf{(c)}, applied to $m=2$
and $p=1$, because the matrix $J_{2}\left(  0\right)  $ is what is called $B$
in this proposition). Similarly, $\dim\left(  \operatorname*{Ker}\left(
J_{3}\left(  8\right)  -8I_{3}\right)  \right)  =1$. On the other hand, the
matrix $J_{2}\left(  9\right)  -8I_{2}=\left(
\begin{array}
[c]{cc}%
1 & 1\\
0 & 1
\end{array}
\right)  $ is an upper-triangular matrix with $1$'s on its main diagonal;
thus, its determinant is $1\cdot1=1\neq0$, so that it is nonsingular. Hence,
$\operatorname*{Ker}\left(  J_{2}\left(  9\right)  -8I_{2}\right)  =0$, so
that $\dim\left(  \operatorname*{Ker}\left(  J_{2}\left(  9\right)
-8I_{2}\right)  \right)  =0$. Thus, our above computation of $\dim\left(
\operatorname*{Ker}\left(  A-8I_{7}\right)  \right)  $ becomes%
\begin{align*}
&  \dim\left(  \operatorname*{Ker}\left(  A-8I_{7}\right)  \right) \\
&  =\underbrace{\dim\left(  \operatorname*{Ker}\left(  J_{2}\left(  8\right)
-8I_{2}\right)  \right)  }_{=1}+\underbrace{\dim\left(  \operatorname*{Ker}%
\left(  J_{3}\left(  8\right)  -8I_{3}\right)  \right)  }_{=1}%
+\underbrace{\dim\left(  \operatorname*{Ker}\left(  J_{2}\left(  9\right)
-8I_{2}\right)  \right)  }_{=0}\\
&  =1+1+0=2.
\end{align*}
Looking back, we see that this comes from the fact that exactly $2$ of the
diagonal blocks in the Jordan canonical form $J$ are Jordan cells at
eigenvalue $8$.
\end{example}

Generalizing this reasoning, we obtain the following:

\begin{proposition}
\label{prop.jnf.unique.mults}Let $A$ be an $n\times n$-matrix, and let
$J=\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  $ be its Jordan canonical form. Then: \medskip

\textbf{(a)} We have $\sigma\left(  A\right)  =\left\{  \lambda_{1}%
,\lambda_{2},\ldots,\lambda_{k}\right\}  $. \medskip

\textbf{(b)} The geometric multiplicity of a number $\lambda\in\mathbb{C}$ as
an eigenvalue of $A$ is the number of Jordan cells of $A$ at eigenvalue
$\lambda$. In other words, it is the number of $i\in\left[  k\right]  $
satisfying $\lambda_{i}=\lambda$. \medskip

\textbf{(c)} The algebraic multiplicity of a number $\lambda\in\mathbb{C}$ as
an eigenvalue of $A$ is the \textbf{sum} of the sizes of all Jordan cells of
$A$ at eigenvalue $\lambda$. In other words, it is $\sum
\limits_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}n_{i}$.
\end{proposition}

\begin{proof}
TODO: Scribe?
\end{proof}

With some more effort, we can obtain a more precise result:

\begin{proposition}
\label{prop.jnf.unique.cellsizes}Let $A$ be an $n\times n$-matrix, and let
$J=\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  $ be its Jordan canonical form. Let $\lambda\in\mathbb{C}$. Let $p$
be a positive integer. Then,%
\begin{align*}
&  \left(  \text{the number of }i\in\left[  k\right]  \text{ such that
}\lambda_{i}=\lambda\text{ and }n_{i}\geq p\right) \\
&  =\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  -\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p-1}\right)  \right)  .
\end{align*}

\end{proposition}

\begin{proof}
We have $A\sim J$, so that $A-\lambda I_{n}\sim J-\lambda I_{n}$ (by
Proposition \ref{prop.schurtri.similar.same} \textbf{(h)}, applied to $B=J$),
and therefore $\left(  A-\lambda I_{n}\right)  ^{p}\sim\left(  J-\lambda
I_{n}\right)  ^{p}$ (by Proposition \ref{prop.schurtri.similar.same}
\textbf{(f)}, applied to $A-\lambda I_{n}$ and $B-\lambda I_{n}$ and $p$
instead of $A$, $B$ and $k$). Hence,%
\begin{equation}
\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  =\dim\left(  \operatorname*{Ker}\left(  \left(
J-\lambda I_{n}\right)  ^{p}\right)  \right)  .
\label{pf.prop.jnf.unique.cellsizes.dimKer1}%
\end{equation}


For each $i\in\left[  k\right]  $, we set
\begin{equation}
M_{i}:=J_{n_{i}}\left(  \lambda_{i}-\lambda\right)  .
\label{pf.prop.jnf.unique.cellsizes.Mi=}%
\end{equation}


However, for each $i\in\left[  k\right]  $, we have%
\begin{equation}
J_{n_{i}}\left(  \lambda_{i}\right)  -\lambda I_{n_{i}}=J_{n_{i}}\left(
\lambda_{i}-\lambda\right)  \label{pf.prop.jnf.unique.cellsizes.J-lam}%
\end{equation}
(because the two Jordan cells $J_{n_{i}}\left(  \lambda_{i}\right)  $ and
$J_{n_{i}}\left(  \lambda_{i}-\lambda\right)  $ differ only in their diagonal
entries, which are $\lambda_{i}$ in the former matrix and $\lambda_{i}%
-\lambda$ in the latter). Comparing this with
(\ref{pf.prop.jnf.unique.cellsizes.Mi=}), we obtain%
\begin{equation}
J_{n_{i}}\left(  \lambda_{i}\right)  -\lambda I_{n_{i}}=M_{i}
\label{pf.prop.jnf.unique.cellsizes.J-lam2}%
\end{equation}
for each $i\in\left[  k\right]  $.

Now, we have%
\begin{align*}
J  &  =\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
\lambda I_{n}  &  =\left(
\begin{array}
[c]{cccc}%
\lambda I_{n_{1}} & 0 & \cdots & 0\\
0 & \lambda I_{n_{2}} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda I_{n_{k}}%
\end{array}
\right)  .
\end{align*}
Subtracting these two equalities from one another, we obtain%
\begin{align*}
J-\lambda I_{n}  &  =\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  -\lambda I_{n_{1}} & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  -\lambda I_{n_{2}} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)  -\lambda I_{n_{k}}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
M_{1} & 0 & \cdots & 0\\
0 & M_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & M_{k}%
\end{array}
\right)
\end{align*}
(by (\ref{pf.prop.jnf.unique.cellsizes.J-lam})). Hence,%
\[
\left(  J-\lambda I_{n}\right)  ^{p}=\left(
\begin{array}
[c]{cccc}%
M_{1} & 0 & \cdots & 0\\
0 & M_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & M_{k}%
\end{array}
\right)  ^{p}=\left(
\begin{array}
[c]{cccc}%
M_{1}^{p} & 0 & \cdots & 0\\
0 & M_{2}^{p} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & M_{k}^{p}%
\end{array}
\right)
\]
(by Corollary \ref{cor.blockmatrix.powt-diag}). Thus,%
\begin{align}
&  \dim\left(  \operatorname*{Ker}\left(  \left(  J-\lambda I_{n}\right)
^{p}\right)  \right) \nonumber\\
&  =\dim\left(  \operatorname*{Ker}\left(
\begin{array}
[c]{cccc}%
M_{1}^{p} & 0 & \cdots & 0\\
0 & M_{2}^{p} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & M_{k}^{p}%
\end{array}
\right)  \right) \nonumber\\
&  =\dim\left(  \operatorname*{Ker}\left(  M_{1}^{p}\right)  \right)
+\dim\left(  \operatorname*{Ker}\left(  M_{2}^{p}\right)  \right)
+\cdots+\dim\left(  \operatorname*{Ker}\left(  M_{k}^{p}\right)  \right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.blockmatrix.nullity-diag}}\right) \nonumber\\
&  =\sum_{i=1}^{k}\dim\left(  \operatorname*{Ker}\left(  M_{i}^{p}\right)
\right)  \label{pf.prop.jnf.unique.cellsizes.1}%
\end{align}


Now, fix an $i\in\left[  k\right]  $ satisfying $\lambda_{i}\neq\lambda$.
Thus, $\lambda_{i}-\lambda\neq0$. The matrix $J_{n_{i}}\left(  \lambda
_{i}-\lambda\right)  $ is upper-triangular, and its diagonal entries are all
$\lambda_{i}-\lambda$. Hence, its determinant is $\det\left(  J_{n_{i}}\left(
\lambda_{i}-\lambda\right)  \right)  =\left(  \lambda_{i}-\lambda\right)
^{n_{i}}\neq0$ (since $\lambda_{i}-\lambda\neq0$). Therefore, this matrix
$J_{n_{i}}\left(  \lambda_{i}-\lambda\right)  $ is invertible. In other words,
the matrix $M_{i}$ is invertible (since $M_{i}=J_{n_{i}}\left(  \lambda
_{i}-\lambda\right)  $). Hence, its $p$-th power $M_{i}^{p}$ is also
invertible, and therefore has nullity $0$. In other words, $\dim\left(
\operatorname*{Ker}\left(  M_{i}^{p}\right)  \right)  =0$.

Forget that we fixed $i$. We thus have shown that if $i\in\left[  k\right]  $
satisfies $\lambda_{i}\neq\lambda$, then
\begin{equation}
\dim\left(  \operatorname*{Ker}\left(  M_{i}^{p}\right)  \right)  =0\text{.}
\label{pf.prop.jnf.unique.cellsizes.nullity0}%
\end{equation}
Hence, (\ref{pf.prop.jnf.unique.cellsizes.1}) becomes%
\begin{align}
&  \dim\left(  \operatorname*{Ker}\left(  \left(  J-\lambda I_{n}\right)
^{p}\right)  \right) \nonumber\\
&  =\sum_{i=1}^{k}\dim\left(  \operatorname*{Ker}\left(  M_{i}^{p}\right)
\right) \nonumber\\
&  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}\neq\lambda
}}\underbrace{\dim\left(  \operatorname*{Ker}\left(  M_{i}^{p}\right)
\right)  }_{\substack{=0\\\text{(by
(\ref{pf.prop.jnf.unique.cellsizes.nullity0}))}}}+\sum_{\substack{i\in\left[
k\right]  ;\\\lambda_{i}=\lambda}}\dim\left(  \operatorname*{Ker}\left(
M_{i}^{p}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  \text{since each }%
i\in\left[  k\right]  \text{ satisfies either }\lambda_{i}\neq\lambda\text{ or
}\lambda_{i}=\lambda\right) \nonumber\\
&  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}\dim\left(
\operatorname*{Ker}\left(  M_{i}^{p}\right)  \right)  .
\label{pf.prop.jnf.unique.cellsizes.2}%
\end{align}


Now, let us fix some $i\in\left[  k\right]  $ satisfying $\lambda_{i}=\lambda
$. Then, $\lambda_{i}-\lambda=0$. Hence, $J_{n_{i}}\left(  \lambda_{i}%
-\lambda\right)  =J_{n_{i}}\left(  0\right)  =\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $. Let us denote this matrix $J_{n_{i}}\left(  0\right)  $ by $B$.
Then, Proposition \ref{prop.jnf.jcell0-powers} \textbf{(c)} (applied to
$m=n_{i}$) shows that we have
\[
\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)  \right)
=p\ \ \ \ \ \ \ \ \ \ \text{if }p\leq n_{i}.
\]
On the other hand, Proposition \ref{prop.jnf.jcell0-powers} \textbf{(c)}
(applied to $m=n_{i}$) shows that we have
\[
\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)  \right)  =n_{i}%
\ \ \ \ \ \ \ \ \ \ \text{if }p>n_{i}.
\]
Combining these two equalities, we obtain
\[
\dim\left(  \operatorname*{Ker}\left(  B^{p}\right)  \right)  =%
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p\geq n_{i}.
\end{cases}
\]
\footnote{Note that the two cases $p\leq n_{i}$ and $p\geq n_{i}$ are not
mutually exclusive: They overlap when $p=n_{i}$. (But the answers in this case
are identical.)} In other words,%
\begin{equation}
\dim\left(  \operatorname*{Ker}\left(  M_{i}^{p}\right)  \right)  =%
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p\geq n_{i}%
\end{cases}
\label{pf.prop.jnf.unique.cellsizes.nullity1}%
\end{equation}
(since $B=J_{n_{i}}\left(  0\right)  =J_{n_{i}}\left(  \lambda_{i}%
-\lambda\right)  =M_{i}$ (by (\ref{pf.prop.jnf.unique.cellsizes.Mi=}))).

Now, forget that we fixed $i$. We thus have proved
(\ref{pf.prop.jnf.unique.cellsizes.nullity0}) for each $i\in\left[  k\right]
$ satisfying $\lambda_{i}=\lambda$. Therefore,
(\ref{pf.prop.jnf.unique.cellsizes.2}) becomes%
\[
\dim\left(  \operatorname*{Ker}\left(  \left(  J-\lambda I_{n}\right)
^{p}\right)  \right)  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda
_{i}=\lambda}}\underbrace{\dim\left(  \operatorname*{Ker}\left(  M_{i}%
^{p}\right)  \right)  }_{\substack{=%
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p\geq n_{i}%
\end{cases}
\\\text{(by (\ref{pf.prop.jnf.unique.cellsizes.nullity1}))}}}=\sum
_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}%
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p\geq n_{i}.
\end{cases}
\]
Thus, (\ref{pf.prop.jnf.unique.cellsizes.dimKer1}) becomes%
\[
\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  =\dim\left(  \operatorname*{Ker}\left(  \left(
J-\lambda I_{n}\right)  ^{p}\right)  \right)  =\sum_{\substack{i\in\left[
k\right]  ;\\\lambda_{i}=\lambda}}%
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p>n_{i}.
\end{cases}
\]
However, we can also apply the same argument to $p-1$ instead of $p$ (since
$p-1\in\mathbb{N}$). Thus, we obtain%
\[
\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p-1}\right)  \right)  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda
_{i}=\lambda}}%
\begin{cases}
p-1; & \text{if }p-1\leq n_{i};\\
n_{i}, & \text{if }p-1\geq n_{i}.
\end{cases}
\]
Subtracting these two equalities, we obtain%
\begin{align*}
&  \dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  -\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p-1}\right)  \right) \\
&  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}%
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p>n_{i}%
\end{cases}
-\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}%
\begin{cases}
p-1; & \text{if }p-1\leq n_{i};\\
n_{i}, & \text{if }p-1\geq n_{i}%
\end{cases}
\\
&  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda
}}\underbrace{\left(
\begin{cases}
p, & \text{if }p\leq n_{i};\\
n_{i}, & \text{if }p>n_{i}%
\end{cases}
-%
\begin{cases}
p-1; & \text{if }p-1\leq n_{i};\\
n_{i}, & \text{if }p-1\geq n_{i}%
\end{cases}
\right)  }_{\substack{=%
\begin{cases}
1, & \text{if }p\leq n_{i};\\
0, & \text{if }p>n_{i}%
\end{cases}
\\\text{(this can be directly checked in each}\\\text{of the two cases }p\leq
n_{i}\text{ and }p>n_{i}\text{)}}}\\
&  =\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}%
\begin{cases}
1, & \text{if }p\leq n_{i};\\
0, & \text{if }p>n_{i}%
\end{cases}
=\sum_{\substack{i\in\left[  k\right]  ;\\\lambda_{i}=\lambda}}%
\begin{cases}
1, & \text{if }n_{i}\geq p;\\
0, & \text{if }n_{i}<p
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the condition }p\leq n_{i}\text{ is equivalent to }n_{i}\geq
p\text{,}\\
\text{and since the condition }p>n_{i}\text{ is equivalent to }n_{i}<p
\end{array}
\right) \\
&  =\left(  \text{the number of }i\in\left[  k\right]  \text{ such that
}\lambda_{i}=\lambda\text{ and }n_{i}\geq p\right)
\end{align*}
(because the sum has an addend equal to $1$ for each $i\in\left[  k\right]  $
satisfying $\lambda_{i}=\lambda$ and $n_{i}\geq p$, whereas all remaining
addends of this sum are $0$). Thus, Proposition
\ref{prop.jnf.unique.cellsizes} is proved.
\end{proof}

\begin{corollary}
\label{cor.jnf.unique.cellsizes=}Let $A$ be an $n\times n$-matrix, and let
$J=\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  $ be its Jordan canonical form. Let $\lambda\in\mathbb{C}$. Let $p$
be a positive integer. Then,%
\begin{align*}
&  \left(  \text{the number of }i\in\left[  k\right]  \text{ such that
}\lambda_{i}=\lambda\text{ and }n_{i}=p\right) \\
&  =2\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ -\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p-1}\right)  \right)  -\dim\left(
\operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)  ^{p+1}\right)
\right)  .
\end{align*}

\end{corollary}

\begin{proof}
An integer $z$ equals $p$ if and only if it satisfies $z\geq p$ but does not
satisfy $z\geq p+1$. Hence, an $i\in\left[  k\right]  $ satisfies $n_{i}=p$ if
and only if it satisfies $n_{i}\geq p$ but does not satisfy $n_{i}\geq p+1$.
Thus,%
\begin{align*}
&  \left(  \text{the number of }i\in\left[  k\right]  \text{ such that
}\lambda_{i}=\lambda\text{ and }n_{i}=p\right) \\
&  =\underbrace{\left(  \text{the number of }i\in\left[  k\right]  \text{ such
that }\lambda_{i}=\lambda\text{ and }n_{i}\geq p\right)  }_{\substack{=\dim
\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  -\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p-1}\right)  \right)  \\\text{(by Proposition
\ref{prop.jnf.unique.cellsizes})}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  \text{the number of }i\in\left[
k\right]  \text{ such that }\lambda_{i}=\lambda\text{ and }n_{i}\geq
p+1\right)  }_{\substack{=\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p+1}\right)  \right)  -\dim\left(
\operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)  ^{p}\right)
\right)  \\\text{(by Proposition \ref{prop.jnf.unique.cellsizes}%
,}\\\text{applied to }p+1\text{ instead of }p\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  \text{because any }%
i\in\left[  k\right]  \text{ satisfying }n_{i}\geq p+1\text{ must also satisfy
}n_{i}\geq p\right) \\
&  =\left(  \dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda
I_{n}\right)  ^{p}\right)  \right)  -\dim\left(  \operatorname*{Ker}\left(
\left(  A-\lambda I_{n}\right)  ^{p-1}\right)  \right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ -\left(  \dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p+1}\right)  \right)  -\dim\left(
\operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)  ^{p}\right)
\right)  \right) \\
&  =2\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  -\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p-1}\right)  \right)  -\dim\left(
\operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)  ^{p+1}\right)
\right)  .
\end{align*}
This proves Corollary \ref{cor.jnf.unique.cellsizes=}.
\end{proof}

Now, we can easily prove Theorem \ref{thm.jnf.jnf} \textbf{(b)}:

\begin{proof}
[Proof of Theorem \ref{thm.jnf.jnf} \textbf{(b)}.]Let $A\in\mathbb{C}^{n\times
n}$ be an $n\times n$-matrix. Let $J$ be a Jordan matrix such that $A\sim J$.
Write $J$ as $J=\left(
\begin{array}
[c]{cccc}%
J_{n_{1}}\left(  \lambda_{1}\right)  & 0 & \cdots & 0\\
0 & J_{n_{2}}\left(  \lambda_{2}\right)  & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & J_{n_{k}}\left(  \lambda_{k}\right)
\end{array}
\right)  $ as in Corollary \ref{cor.jnf.unique.cellsizes=}. Then, the Jordan
blocks of $J$ are $J_{n_{1}}\left(  \lambda_{1}\right)  ,J_{n_{2}}\left(
\lambda_{2}\right)  ,\ldots,J_{n_{k}}\left(  \lambda_{k}\right)  $. Hence, for
any $\lambda\in\mathbb{C}$ and any positive integer $p$, we have%
\begin{align*}
&  \left(  \text{the number of Jordan blocks of }J\text{ of size }p\text{ at
eigenvalue }\lambda\right) \\
&  =\left(  \text{the number of }i\in\left[  k\right]  \text{ such that
}\lambda_{i}=\lambda\text{ and }n_{i}=p\right) \\
&  =2\dim\left(  \operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)
^{p}\right)  \right)  -\dim\left(  \operatorname*{Ker}\left(  \left(
A-\lambda I_{n}\right)  ^{p-1}\right)  \right)  -\dim\left(
\operatorname*{Ker}\left(  \left(  A-\lambda I_{n}\right)  ^{p+1}\right)
\right)  .
\end{align*}
Therefore, this number is uniquely determined by $A$, $\lambda$ and $p$.
Hence, the whole structure of $J$ is determined uniquely by $A$, up to the
order of the Jordan blocks. This proves Theorem \ref{thm.jnf.jnf} \textbf{(b)}.
\end{proof}

\begin{example}
Let $A$ be an $8\times8$-matrix. Assume that we know that the Jordan canonical
form of $A$ has

\begin{itemize}
\item $1$ Jordan block of size $1$ at eigenvalue $17$;

\item $2$ Jordan blocks of size $1$ at eigenvalue $35$;

\item $1$ Jordan block of size $2$ at eigenvalue $35$;

\item $1$ Jordan block of size $3$ at eigenvalue $59$;

\item no Jordan blocks of other sizes or at other eigenvalues.
\end{itemize}

Then, the Jordan canonical form of $A$ must be the block-diagonal matrix%
\[
\left(
\begin{array}
[c]{ccccc}%
J_{1}\left(  17\right)  & 0 & 0 & 0 & 0\\
0 & J_{2}\left(  35\right)  & 0 & 0 & 0\\
0 & 0 & J_{1}\left(  35\right)  & 0 & 0\\
0 & 0 & 0 & J_{1}\left(  35\right)  & 0\\
0 & 0 & 0 & 0 & J_{3}\left(  59\right)
\end{array}
\right)
\]
or one that is obtained from it by permuting the diagonal blocks.
\end{example}

Let us now approach the existence of the Jordan canonical form (Theorem
\ref{thm.jnf.jnf} \textbf{(a)}).

\subsection{\label{sect.jnf.exist}Jordan canonical form: proof of existence}

We will prove the existence of the Jordan canonical form in several steps,
each of which will bring our matrix $A$ \textquotedblleft
closer\textquotedblright\ to a Jordan matrix. Along the way, we will obtain
several results of independent interest.

\subsubsection{Step 1: Schur triangularization}

Our first step will be a slightly stronger version of Schur triangularization.
As we recall, the Schur triangularization theorem (Theorem
\ref{thm.schurtri.schurtri}) tells us that if $A\in\mathbb{C}^{n\times n}$ is
an $n\times n$-matrix, then $A$ is unitarily similar to an upper-triangular
matrix $T$. The diagonal entries of the latter matrix $T$ will be the
eigenvalues of $A$ in some order (by Proposition
\ref{prop.schurtri.schurtri.T-diag}). However, let us now be a bit pickier. To
wit, we now want the triangular matrix $T$ to have the property that equal
eigenvalues come in contiguous runs on the main diagonal. In other words, we
want $T$ to have the property that if two diagonal entries of $T$ are equal,
then all the diagonal entries between them are also equal to them. For
instance, if the eigenvalues of $A$ are $1,1,2,2$, we don't want\footnote{In
the following equation, an empty cell of the matrix must be filled with a $0$,
whereas a \textquotedblleft$\ast$\textquotedblright\ in a cell means that any
arbitrary value can go into that cell.}%
\[
T=\left(
\begin{array}
[c]{cccc}%
1 & \ast & \ast & \ast\\
& 2 & \ast & \ast\\
&  & 1 & \ast\\
&  &  & 2
\end{array}
\right)  ,
\]
but instead we want%
\[
T=\left(
\begin{array}
[c]{cccc}%
1 & \ast & \ast & \ast\\
& 1 & \ast & \ast\\
&  & 2 & \ast\\
&  &  & 2
\end{array}
\right)  .
\]
To this purpose, we need a stronger version of Schur triangularization:

\begin{theorem}
[Schur triangularization with perscribed diagonal]%
\label{thm.jnf.schurtri-diag}Let $A\in\mathbb{C}^{n\times n}$ be an $n\times
n$-matrix. Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$ be its eigenvalues
(listed with their algebraic multiplicities). Then, there exists an
upper-triangular matrix $T$ such that $A\overset{\operatorname*{us}}{\sim}T$
(this means \textquotedblleft$A$ is unitarily similar to $T$\textquotedblright%
, as we recall) and such that the diagonal entries of $T$ are $\lambda
_{1},\lambda_{2},\ldots,\lambda_{n}$ in this order.
\end{theorem}

\begin{proof}
TODO: Scribe!

We proceed as in the proof of Theorem \ref{thm.schurtri.schurtri}, but we pay
some attention to the eigenvectors that we pick. That proof constructed $T$
recursively, starting by picking an eigenvalue $\lambda$ of $A$ and a
corresponding $\lambda$-eigenvector $x\neq0$, and then finding a unitary
matrix $U$ such that%
\[
U^{\ast}AU=\left(
\begin{array}
[c]{cc}%
\lambda & p\\
0 & B
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{for some }p\in\mathbb{C}^{1\times\left(
n-1\right)  }\text{ and }B\in\mathbb{C}^{\left(  n-1\right)  \times\left(
n-1\right)  }.
\]
Then, the algorithm was applied recursively to $B$.

Now, we perform this algorithm, but we make sure to pick $\lambda=\lambda_{1}%
$. Thus,%
\[
U^{\ast}AU=\left(
\begin{array}
[c]{cc}%
\lambda_{1} & p\\
0 & B
\end{array}
\right)  ,
\]
which is a good start. Now we want to apply the IH (= induction hypothesis) to
$B$. To that purpose, we need to know that the eigenvalues of $B$ are
$\lambda_{2},\lambda_{3},\ldots,\lambda_{n}$ (with algebraic multiplicities).
In other words, we need to know that%
\[
p_{B}\left(  t\right)  =\left(  t-\lambda_{2}\right)  \left(  t-\lambda
_{3}\right)  \cdots\left(  t-\lambda_{n}\right)  .
\]


Now, since $U$ is unitary, we have $A\overset{\operatorname*{us}}{\sim}%
U^{\ast}AU$ and thus $A\sim U^{\ast}AU$. Thus,%
\begin{align*}
p_{A}  &  =p_{U^{\ast}AU}=\det\left(  tI_{n}-U^{\ast}AU\right)  =\det\left(
tI_{n}-\left(
\begin{array}
[c]{cc}%
\lambda_{1} & p\\
0 & B
\end{array}
\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U^{\ast}AU=\left(
\begin{array}
[c]{cc}%
\lambda_{1} & p\\
0 & B
\end{array}
\right)  \right) \\
&  =\det\left(
\begin{array}
[c]{cc}%
t-\lambda_{1} & -p\\
0 & tI_{n-1}-B
\end{array}
\right) \\
&  =\left(  t-\lambda_{1}\right)  \cdot\underbrace{\det\left(  tI_{n-1}%
-B\right)  }_{=p_{B}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we applied Laplace expansion along the}\\
\text{first column to compute the determinant}\\
\text{(noticing that this column has only one nonzero entry)}%
\end{array}
\right) \\
&  =\left(  t-\lambda_{1}\right)  \cdot p_{B},
\end{align*}
so that%
\[
\left(  t-\lambda_{1}\right)  \cdot p_{B}=p_{A}=\left(  t-\lambda_{1}\right)
\left(  t-\lambda_{2}\right)  \cdots\left(  t-\lambda_{n}\right)  .
\]
Now, we can cancel $t-\lambda_{1}$ (since this is a nonzero polynomial), and
this becomes%
\[
p_{B}=\left(  t-\lambda_{2}\right)  \left(  t-\lambda_{3}\right)
\cdots\left(  t-\lambda_{n}\right)  ,
\]
exactly as we wanted to show.
\end{proof}

\begin{noncompile}
The following lemma was originally used in the above proof:

\begin{lemma}
Let $\mathbb{F}$ be a field. Let $X\in\mathbb{F}^{n\times n}$, $Y\in
\mathbb{F}^{n\times m}$ and $Z\in\mathbb{F}^{m\times m}$ be three matrices.
Then,$\det\left(
\begin{array}
[c]{cc}%
X & Y\\
0 & Z
\end{array}
\right)  =\det X\cdot\det Z.$
\end{lemma}

For example,%
\[
\det\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
a^{\prime} & b^{\prime} & c^{\prime} & d^{\prime}\\
0 & 0 & c^{\prime\prime} & d^{\prime\prime}\\
0 & 0 & c^{\prime\prime\prime} & d^{\prime\prime\prime}%
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
a & b\\
a^{\prime} & b^{\prime}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
c^{\prime\prime} & d^{\prime\prime}\\
c^{\prime\prime\prime} & d^{\prime\prime\prime}%
\end{array}
\right)  .
\]

\end{noncompile}

\subsubsection{Step 2: Separating distinct eigenvalues}

Theorem \ref{thm.jnf.schurtri-diag} brings any $n\times n$-matrix
$A\in\mathbb{C}^{n\times n}$ to a certain simplified form (upper-triangular
with eigenvalues placed contiguously on the diagonal) that is not yet a Jordan
canonical form, but already has some of its aspects. We will now transform it
further to get a bit closer to a Jordan canonical form. To wit, we will get
rid of some of the entries above the diagonal (or, to be more precise, we will
turn them into $0$). Let us demonstrate this on an example:

\begin{example}
\label{exa.jnf.step2}Let $a,b,c,\ldots,p\in\mathbb{C}$ be any numbers. We
shall now show that%
\begin{equation}
\left(
\begin{array}
[c]{cccccc}%
1 & a & b & c & d & e\\
& 1 & f & g & h & i\\
&  & 2 & j & k & \ell\\
&  &  & 2 & m & n\\
&  &  &  & 2 & p\\
&  &  &  &  & 3
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cccccc}%
1 & a &  &  &  & \\
& 1 &  &  &  & \\
&  & 2 & j & k & \\
&  &  & 2 & m & \\
&  &  &  & 2 & \\
&  &  &  &  & 3
\end{array}
\right)  \label{eq.exa.jnf.step2.1}%
\end{equation}
(where the entries in the empty cells are understood to be $0$s).

Indeed, the triangular matrices $\left(
\begin{array}
[c]{cc}%
1 & a\\
& 1
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cccc}%
2 & j & k & \ell\\
& 2 & m & n\\
&  & 2 & p\\
&  &  & 3
\end{array}
\right)  $ have disjoint spectra (i.e., they have no eigenvalues in common),
because their diagonals have no entries in common. So, by Corollary
\ref{cor.schurtri.block-sim}, we have%
\begin{equation}
\left(
\begin{array}
[c]{cccccc}%
1 & a & b & c & d & e\\
& 1 & f & g & h & i\\
&  & 2 & j & k & \ell\\
&  &  & 2 & m & n\\
&  &  &  & 2 & p\\
&  &  &  &  & 3
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cccccc}%
1 & a &  &  &  & \\
& 1 &  &  &  & \\
&  & 2 & j & k & \ell\\
&  &  & 2 & m & n\\
&  &  &  & 2 & p\\
&  &  &  &  & 3
\end{array}
\right)  . \label{eq.exa.jnf.step2.2}%
\end{equation}
Furthermore, the triangular matrices $\left(
\begin{array}
[c]{ccccc}%
1 & a &  &  & \\
& 1 &  &  & \\
&  & 2 & j & k\\
&  &  & 2 & m\\
&  &  &  & 2
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{c}%
3
\end{array}
\right)  $ have disjoint spectra, so Corollary \ref{cor.schurtri.block-sim}
yields%
\begin{equation}
\left(
\begin{array}
[c]{cccccc}%
1 & a &  &  &  & \\
& 1 &  &  &  & \\
&  & 2 & j & k & \ell\\
&  &  & 2 & m & n\\
&  &  &  & 2 & p\\
&  &  &  &  & 3
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cccccc}%
1 & a &  &  &  & \\
& 1 &  &  &  & \\
&  & 2 & j & k & \\
&  &  & 2 & m & \\
&  &  &  & 2 & \\
&  &  &  &  & 3
\end{array}
\right)  . \label{eq.exa.jnf.step2.3}%
\end{equation}
Since $\sim$ is an equivalence relation, we can combine the two similarities
(\ref{eq.exa.jnf.step2.2}) and (\ref{eq.exa.jnf.step2.3}), and we conclude
that the claim (\ref{eq.exa.jnf.step2.1}) holds.
\end{example}

This example generalizes:

\begin{theorem}
\label{thm.jnf.step2-T}Let $T\in\mathbb{C}^{n\times n}$ be an upper-triangular
matrix. Assume that the diagonal entries of $T$ come in contiguous runs (i.e.,
if $i,j\in\left[  n\right]  $ satisfy $i<j$ and $T_{i,i}=T_{j,j}$, then
$T_{i,i}=T_{i+1,i+1}=T_{i+2,i+2}=\cdots=T_{j,j}$). Let $S$ be the matrix
obtained from $T$ by setting all entries $T_{i,j}$ with $T_{i,i}\neq T_{j,j}$
to $0$. In other words, let $S\in\mathbb{C}^{n\times n}$ be the $n\times
n$-matrix defined by setting%
\[
S_{i,j}=%
\begin{cases}
T_{i,j}, & \text{if }T_{i,i}=T_{j,j};\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left[  n\right]  .
\]
Then, $T\sim S$.
\end{theorem}

\begin{proof}
TODO: Scribe!
\end{proof}

Roughly speaking, Theorem \ref{thm.jnf.step2-T} says that whenever we have an
upper-triangular matrix $T$ whose diagonal has no interlaced values (i.e.,
there is never a $\mu$ between two $\lambda$'s on the diagonal when $\mu
\neq\lambda$), we can \textquotedblleft clean out\textquotedblright\ all the
above-diagonal entries that correspond to different diagonal entries (i.e.,
all above-diagonal entries $T_{i,j}$ with $T_{i,i}\neq T_{j,j}$) by a
similarity (i.e., if we set all these entries to $0$, the resulting matrix
will be similar to $T$).

Now, combining this with Theorem \ref{thm.jnf.schurtri-diag}, we obtain the following:

\begin{proposition}
\label{prop.jnf.step2}Let $A\in\mathbb{C}^{n\times n}$ be an $n\times
n$-matrix. Then, $A$ is similar to a block-diagonal matrix of the form%
\[
\left(
\begin{array}
[c]{cccc}%
B_{1} &  &  & \\
& B_{2} &  & \\
&  & \ddots & \\
&  &  & B_{k}%
\end{array}
\right)  ,
\]
where each $B_{i}$ is an upper-triangular matrix such that all entries on the
diagonal of $B_{i}$ are equal. (Here, the cells that we left empty are
understood to be filled with zero matrices.)
\end{proposition}

\begin{proof}
TODO: Scribe!
\end{proof}

Note that we have given up unitary similarity at this point: The word
\textquotedblleft similar\textquotedblright\ in Proposition
\ref{prop.jnf.step2} cannot be replaced by \textquotedblleft unitarily
similar\textquotedblright. (A counterexample is easily obtained from Exercise
\ref{exe.schurtri.unisim.two2x2}.)

\subsubsection{\label{subsect.jnf.exist.step3}Step 3: Strictly
upper-triangular matrices}

The block-diagonal matrix in Proposition \ref{prop.jnf.step2} is not yet a
Jordan canonical form, but it is already somewhat close. At least, we have
separated out all the distinct eigenvalues of $A$ and \textquotedblleft
cleaned out the space between them\textquotedblright. We now can work with the
matrices $B_{1},B_{2},\ldots,B_{k}$ separately; each of these matrices has one
distinct eigenvalue. Our next goal is to show that each of these matrices
$B_{1},B_{2},\ldots,B_{k}$ is similar to a Jordan matrix. (This will easily
yield that the total block-diagonal matrix $\left(
\begin{array}
[c]{cccc}%
B_{1} &  &  & \\
& B_{2} &  & \\
&  & \ddots & \\
&  &  & B_{k}%
\end{array}
\right)  $ is similar to a Jordan matrix, and therefore the same holds for $A$.)

For each $i\in\left[  k\right]  $, the matrix $B_{i}$ has all its diagonal
entries equal. Let us say these diagonal entries all equal $\mu_{i}$. Thus,
$B_{i}-\mu_{i}I$ is a strictly upper-triangular matrix. (Recall: a
\emph{strictly upper-triangular} matrix is an upper-triangular matrix whose
diagonal entries are $0$.) We want to show that $B_{i}$ is similar to a Jordan
matrix. Because of Proposition \ref{prop.schurtri.similar.same} \textbf{(g)},
it will suffice to show that the strictly upper-triangular matrix $B_{i}%
-\mu_{i}I$ is similar to a Jordan matrix (because adding $\mu_{i}I$ to a
Jordan matrix always gives a Jordan matrix again).

Thus, our goal is now to show that every strictly upper-triangular matrix $A$
is similar to a Jordan matrix. Before we approach this goal in general, let us
convince ourselves that it is achievable for $2\times2$-matrices.

\begin{example}
A strictly upper-triangular $2\times2$-matrix $A\in\mathbb{C}^{2\times2}$ must
have the form $\left(
\begin{array}
[c]{cc}%
0 & a\\
0 & 0
\end{array}
\right)  $ for some $a\in\mathbb{C}$.

\begin{itemize}
\item If $a=0$, then $A$ is the Jordan matrix $\left(
\begin{array}
[c]{cc}%
J_{1}\left(  0\right)  & \\
& J_{1}\left(  0\right)
\end{array}
\right)  $.

\item If $a\neq0$, then $A$ is similar to the Jordan matrix $J_{2}\left(
0\right)  =\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $. Indeed,
\[
A=\left(
\begin{array}
[c]{cc}%
a & 0\\
0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
a & 0\\
0 & 1
\end{array}
\right)  ^{-1}.
\]

\end{itemize}
\end{example}

Now, we return to the general case. Let $\mathbb{F}$ be any field. Let
$A\in\mathbb{F}^{n\times n}$ be any strictly upper-triangular $n\times
n$-matrix. (We don't need to restrict ourselves to the case $\mathbb{F}%
=\mathbb{C}$ here.) We want to prove that $A$ is similar to a Jordan matrix.

The key to this proof will be to restate the question in terms of certain
bases of $\mathbb{F}^{n}$, and then to construct these bases by an iterative
process. We begin with a few notions:

\begin{convention}
We fix a nonnegative integer $n\in\mathbb{N}$, a field $\mathbb{F}$ and a
strictly upper-triangular matrix $A\in\mathbb{F}^{n\times n}$ for the rest of
Subsection \ref{subsect.jnf.exist.step3}.
\end{convention}

\begin{definition}
\label{def.jnf.exist.step3.orbits-etc}\textbf{(a)} An \emph{orbit} shall mean
a tuple of the form $\left(  A^{0}v,\ A^{1}v,\ \ldots,\ A^{k}v\right)  $,
where $v\in\mathbb{F}^{n}$ is a vector and $k\in\mathbb{N}$ is an integer
satisfying $A^{k+1}v=0$. (We can also write this tuple as $\left(
v,\ Av,\ A^{2}v,\ \ldots,\ A^{k}v\right)  $.) \medskip

\textbf{(b)} The \emph{concatenation} of two tuples $\left(  a_{1}%
,a_{2},\ldots,a_{k}\right)  $ and $\left(  b_{1},b_{2},\ldots,b_{\ell}\right)
$ is defined to be the tuple $\left(  a_{1},a_{2},\ldots,a_{k},b_{1}%
,b_{2},\ldots,b_{\ell}\right)  $. Thus, concatenation is a binary operation on
the set of tuples. Since this operation is associative, we thus obtain the
notion of concatenation of several tuples. \medskip

\textbf{(c)} A tuple $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ of vectors in
$\mathbb{F}^{n}$ will be called \emph{forwarded} if each $i\in\left[
m\right]  $ satisfies $Av_{i}=v_{i+1}$ or $Av_{i}=0$. (Here, $v_{m+1}$ is
understood to be $0$.) \medskip

\textbf{(d)} A tuple $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ of vectors in
$\mathbb{F}^{n}$ will be called \emph{backwarded} if each $i\in\left[
m\right]  $ satisfies $Av_{i}=v_{i-1}$ or $Av_{i}=0$. (Here, $v_{0}$ is
understood to be $0$.) \medskip

Note that the notions of \textquotedblleft orbit\textquotedblright,
\textquotedblleft forwarded\textquotedblright\ and \textquotedblleft
backwarded\textquotedblright\ depend on $A$, but we do not mention $A$ since
$A$ is fixed.
\end{definition}

\begin{example}
Let $p,q,r$ be three vectors in $\mathbb{F}^{n}$ satisfying $A^{3}p=0$ and
$A^{2}q=0$ and $A^{4}r=0$. Then, the $9$-tuple
\[
\left(  p,\ Ap,\ A^{2}p,\ q,\ Aq,\ r,\ Ar,\ A^{2}r,\ A^{3}r\right)
\]
is forwarded. Indeed, if we rename this tuple as $\left(  v_{1},v_{2}%
,\ldots,v_{9}\right)  $, then each $i\in\left\{  1,2,4,6,7,8\right\}  $
satisfies $Av_{i}=v_{i+1}$, whereas each $i\in\left\{  3,5\right\}  $
satisfies $Av_{i}=0$. This $9$-tuple is furthermore the concatenation of the
orbits $\left(  p,\ Ap,\ A^{2}p\right)  $, $\left(  q,\ Aq\right)  $ and
$\left(  r,\ Ar,\ A^{2}r,\ A^{3}r\right)  $. Reversing this $9$-tuple yields a
new $9$-tuple%
\[
\left(  A^{3}r,\ A^{2}r,\ Ar,\ r,\ Aq,\ q,\ A^{2}p,\ Ap,\ p\right)  ,
\]
which is backwarded.
\end{example}

What we have seen in this example can be generalized:

\begin{proposition}
\label{prop.jnf.exist.step3.bwd}\textbf{(a)} A tuple $\left(  v_{1}%
,v_{2},\ldots,v_{m}\right)  $ of vectors in $\mathbb{F}^{n}$ is forwarded if
and only if it is a concatenation of finitely many orbits. \medskip

\textbf{(b)} A tuple $\left(  v_{1},v_{2},\ldots,v_{m}\right)  $ of vectors in
$\mathbb{F}^{n}$ is backwarded if and only if the tuple $\left(  v_{m}%
,v_{m-1},\ldots,v_{1}\right)  $ is forwarded.
\end{proposition}

\begin{proof}
TODO: Scribe?
\end{proof}

More importantly, backwarded tuples are closely related to Jordan forms. To wit:

\begin{proposition}
\label{prop.jnf.exist.step3.basis}Let $\left(  s_{1},s_{2},\ldots
,s_{n}\right)  $ be a basis of $\mathbb{F}^{n}$. Let $S\in\mathbb{F}^{n\times
n}$ be the $n\times n$-matrix with columns $s_{1},s_{2},\ldots,s_{n}$. Then,
$S^{-1}AS$ is a Jordan matrix if and only if the $n$-tuple $\left(
s_{1},s_{2},\ldots,s_{n}\right)  $ is backwarded.
\end{proposition}

\begin{proof}
TODO: Scribe?

We shall only prove the \textquotedblleft if\textquotedblright\ part, since
this is the only part that will be used; however, the proof of the
\textquotedblleft only if\textquotedblright\ part can essentially be obtained
from the proof of the \textquotedblleft if\textquotedblright\ part by reading
it in reverse.

Thus, we assume that the $n$-tuple $\left(  s_{1},s_{2},\ldots,s_{n}\right)  $
is backwarded. In other words, each $i\in\left[  n-1\right]  $ satisfies
\begin{equation}
As_{i+1}=s_{i}\text{ or }As_{i+1}=0. \label{pf.prop.jnf.exist.step3.basis.fwd}%
\end{equation}
Our goal is to show that $S^{-1}AS$ is a Jordan matrix.

First, we observe that the matrix $S$ is invertible, since its columns
$s_{1},s_{2},\ldots,s_{n}$ form a basis of $\mathbb{F}^{n}$. For each
$i\in\left[  n\right]  $, the $i$-th column of the matrix $S$ is $s_{i}$.
Hence, for each $i\in\left[  n\right]  $, the $i$-th column of the matrix $AS$
is $As_{i}$ (by the rules for multiplying matrices).

We want to find a basis $\left(  s_{1},s_{2},\ldots,s_{n}\right)  $ of
$\mathbb{F}^{n}$ such that for each $i\in\left[  n\right]  $, the vector
$As_{i}$ is either $s_{i-1}$ or $0$. (When $i=1$, this vector has to be $0$,
since there is no $s_{0}$.) In fact, if $\left(  s_{1},s_{2},\ldots
,s_{n}\right)  $ is such a basis, then the matrix $S:=\left(
\begin{array}
[c]{cccc}%
s_{1} & s_{2} & \cdots & s_{n}%
\end{array}
\right)  \in\mathbb{F}^{n\times n}$ is invertible and satisfies%
\begin{align*}
AS  &  =\left(  \text{a matrix whose }i\text{-th column is either }%
s_{i-1}\text{ or }0\text{ for each }i\right) \\
&  =S\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{for example}\right)  ,
\end{align*}
so that%
\[
S^{-1}AS=\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\end{array}
\right)  =\left(  \text{a Jordan matrix}\right)  .
\]
So $A$ is similar to a Jordan matrix.
\end{proof}

Recall that our goal is to show that $A$ is similar to a Jordan matrix.
Proposition \ref{prop.jnf.exist.step3.basis} shows us a way to this goal: We
just need to find a basis for $\mathbb{F}^{n}$ that is forwarded. In view of
Proposition \ref{prop.jnf.exist.step3.bwd} \textbf{(b)}, this is tantamount to
finding a basis for $\mathbb{F}^{n}$ that is backwarded. Let us first see how
to do so on examples:

[...]

TODO: Polish from here!

TODO: Empty cells = $0$ entries.

\begin{example}
Let $n=4$ and $A=$... (where the cells we leave empty are understood to
contain zeroes). Then, ... find some interesting orbits and bases

TODO: Scribe?
\end{example}

We begin by finding forwarded bases in some examples:

\begin{example}
Let $n=2$. Then, $A=\left(
\begin{array}
[c]{cc}%
0 & a\\
0 & 0
\end{array}
\right)  $ for some $a\in\mathbb{F}$.

We are looking for an invertible matrix $S\in\mathbb{F}^{2\times2}$ such that
$S^{-1}AS$ is a Jordan matrix.

If $a=0$, then this is obvious (just take $S=I_{2}$), since $A=\left(
\begin{array}
[c]{cc}%
J_{1}\left(  0\right)  & \\
& J_{1}\left(  0\right)
\end{array}
\right)  $ is already a Jordan matrix.

Now assume $a\neq0$.

Consider our unknown invertible matrix $S$. Let $s_{1}$ and $s_{2}$ be its
columns. Then, $s_{1}$ and $s_{2}$ are linearly independent (since $S$ is
invertible). Moreover, we want $S^{-1}AS=\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $. In other words, we want $AS=S\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $. However, $S=\left(
\begin{array}
[c]{cc}%
s_{1} & s_{2}%
\end{array}
\right)  $ (in block-matrix notation), so $S\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
0 & s_{1}%
\end{array}
\right)  $. Thus our equation $AS=S\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $ is equivalent to%
\[
\left(
\begin{array}
[c]{cc}%
As_{1} & As_{2}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
0 & s_{1}%
\end{array}
\right)  .
\]
In other words, $As_{1}=0$ and $As_{2}=s_{1}$.

So we are looking for two linearly independent vectors $s_{1},s_{2}%
\in\mathbb{F}^{2}$ such that $As_{1}=0$ and $As_{2}=s_{1}$.

One way to do so is to pick some nonzero vector $s_{1}\in\operatorname*{Ker}%
A$, and then define $s_{2}$ to be some preimage of $s_{1}$ under $A$. (It can
be shown that such preimage exists.) This way, however, does not generalize to
higher $n$.

Another (better) way is to start by picking $s_{2}\in\mathbb{F}^{2}%
\setminus\operatorname*{Ker}A$ and then setting $s_{1}=As_{2}$. We claim that
$s_{1}$ and $s_{2}$ are linearly independent, and that $As_{1}=0$.

To show that $As_{1}=0$, we just observe that $As_{1}=\underbrace{AA}%
_{=A^{2}=0}s_{2}=0$.

To show that $s_{1}$ and $s_{2}$ are linearly independent, we argue as
follows: Let $\lambda_{1},\lambda_{2}\in\mathbb{F}$ be such that $\lambda
_{1}s_{1}+\lambda_{2}s_{2}=0$. Applying $A$ to this, we obtain $A\cdot\left(
\lambda_{1}s_{1}+\lambda_{2}s_{2}\right)  =A\cdot0=0$. However,%
\[
A\cdot\left(  \lambda_{1}s_{1}+\lambda_{2}s_{2}\right)  =\lambda
_{1}\underbrace{As_{1}}_{=0}+\lambda_{2}\underbrace{As_{2}}_{=s_{1}}%
=\lambda_{2}s_{1},
\]
so this becomes $\lambda_{2}s_{1}=0$. However, $s_{1}\neq0$ (because
$s_{1}=As_{2}$ but $s_{2}\notin\operatorname*{Ker}A$). Hence, $\lambda_{2}=0$.
Now, $\lambda_{1}s_{1}+\lambda_{2}s_{2}=0$ becomes $\lambda_{1}s_{1}=0$. Since
$s_{1}\neq0$, this yields $\lambda_{1}=0$. Now both $\lambda_{i}$s are $0$, qed.
\end{example}

\begin{example}
Let $n=3$ and $A=\left(
\begin{array}
[c]{ccc}
& 1 & 1\\
&  & 0\\
&  &
\end{array}
\right)  $.

Our first method above doesn't work, because most vectors in
$\operatorname*{Ker}A$ do not have preimages under $A$.

However, our second method can be made to work:

We pick a vector $s_{3}\notin\operatorname*{Ker}A$. To wit, we pick
$s_{3}=e_{3}=\left(
\begin{array}
[c]{c}%
0\\
0\\
1
\end{array}
\right)  $. Then, $As_{3}=e_{1}$. Set $s_{2}=As_{3}=e_{1}$. Note that
$s_{2}\in\operatorname*{Ker}A$. Let $s_{1}$ be another nonzero vector in
$\operatorname*{Ker}A$, namely $e_{2}-e_{3}$.{} These three vectors
$s_{1},s_{2},s_{3}$ are linearly independent and satisfy $As_{1}=0$ and
$As_{2}=0$ and $As_{3}=s_{2}$.

So $S=\left(
\begin{array}
[c]{ccc}%
0 & 1 & 0\\
1 & 0 & 0\\
-1 & 0 & 1
\end{array}
\right)  $. And indeed, $S^{-1}AS=\allowbreak\left(
\begin{array}
[c]{ccc}%
0 & 0 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{array}
\right)  $ is a Jordan matrix.
\end{example}

So what is the general algorithm here? Can we always find $n$ linearly
independent vectors $s_{1},s_{2},\ldots,s_{n}$ such that each $As_{i}$ is
either $0$ or $s_{i-1}$ ?

\bigskip

Now, we return to the general case: How do we find a backwarded basis $\left(
s_{1},s_{2},\ldots,s_{n}\right)  $ of $\mathbb{F}^{n}$ ?

(The following proof is due to Terence Tao \cite{Tao07}.)

We define an \textbf{orbit} to be a tuple of the form $\left(  v,Av,A^{2}%
v,\ldots,A^{k}v\right)  $, where $v\in\mathbb{F}^{n}$ satisfies $A^{k+1}v=0$.
Note that for each $v\in\mathbb{F}^{n}$, there is an orbit that starts with
$v$, since $A^{n}=0$.

The \textbf{concatenation} of some tuples $\left(  a_{1},a_{2},\ldots
,a_{k}\right)  $ and $\left(  b_{1},b_{2},\ldots,b_{\ell}\right)  $ and
$\left(  c_{1},c_{2},\ldots,c_{m}\right)  $ is $\left(  a_{1},a_{2}%
,\ldots,a_{k},b_{1},b_{2},\ldots,b_{\ell},c_{1},c_{2},\ldots,c_{m}\right)  $.

Now, I claim:

\begin{lemma}
[orbit basis lemma]There exists a basis of $\mathbb{F}^{n}$ that is a
concatenation of orbits.
\end{lemma}

Once this lemma is proved, we will be done, because reading such a basis
backwards gives us exactly the basis $\left(  s_{1},s_{2},\ldots,s_{n}\right)
$ we are looking for. For example, if our basis that is a concatenation of
orbits is%
\[
\left(  u,Au,A^{2}u,\ \ v,Av,A^{2}v,A^{3}v,\ \ w,Aw\right)
\]
(with $A^{3}u=0$ and $A^{4}v=0$ and $A^{2}w=0$), then reading it backwards
gives
\[
\left(  Aw,w,\ \ A^{3}v,A^{2}v,Av,v,\ \ A^{2}u,Au,u\right)  ,
\]
which is a basis $\left(  s_{1},s_{2},\ldots,s_{n}\right)  $ of $\mathbb{F}%
^{n}$ such that for each $i\in\left[  n\right]  $, the vector $As_{i}$ is
either $s_{i-1}$ or $0$.

\begin{proof}
[Proof of the Lemma.]It is easy to find a finite \textbf{spanning set} of
$\mathbb{F}^{n}$ that is a concatenation of orbits. Indeed, we can start with
the standard basis $\left(  e_{1},e_{2},\ldots,e_{n}\right)  $, and extend it
to the list%
\begin{align*}
&  (e_{1},Ae_{1},A^{2}e_{1},\ldots,A^{n-1}e_{1},\\
&  \ e_{2},Ae_{2},A^{2}e_{2},\ldots,A^{n-1}e_{2},\\
&  \ \ldots,\\
&  \ e_{n},Ae_{n},A^{2}e_{n},\ldots,A^{n-1}e_{n}).
\end{align*}
This is clearly a spanning set of $\mathbb{F}^{n}$ (since $e_{1},e_{2}%
,\ldots,e_{n}$ already span $\mathbb{F}^{n}$), and also a concatenation of
orbits (since $A^{n}=0$).

Now, we will gradually shorten this spanning set (i.e., replace it by smaller
ones) until we get a basis. We have to do this in such a way that it remains a
spanning set throughout the process, and that it remains a concatenation of
orbits throughout the process.

For the sake of concreteness, let us assume that our spanning set is%
\[
\left(  x,Ax,\ \ y,Ay,A^{2}y,\ \ z,Az,A^{2}z,A^{3}z,\ \ w\right)  ,
\]
with $A^{2}x=0$ and $A^{3}y=0$ and $A^{4}z=0$ and $Aw=0$. If this spanning set
is linearly independent, then it is already a basis, and we are done. So
assume that it isn't. Thus, there exists some linear dependence relation --
say,%
\[
3x+4Ax+5Ay+6A^{2}y+7A^{2}z+8w=0.
\]
Apply $A$ to this relation:%
\begin{align*}
3Ax+4A^{2}x+5A^{2}y+6A^{3}y+7A^{3}z+8Aw  &  =0,\ \ \ \ \ \ \ \ \ \ \text{i.e.}%
\\
3Ax+5A^{2}y+7A^{3}z  &  =0.
\end{align*}
Apply $A$ to this relation:%
\begin{align*}
3A^{2}x+5A^{3}y+7A^{4}z  &  =0,\ \ \ \ \ \ \ \ \ \ \text{i.e.}\\
0  &  =0.
\end{align*}
We have gone too far, so let us revert to the previous equation:%
\[
3Ax+5A^{2}y+7A^{3}z=0.
\]
So this is a linear dependence relation between the \textbf{final} vectors of
the orbits in our spanning set. (\textquotedblleft Final\textquotedblright%
\ means the last vector in the orbit.) Factoring out an $A$ in this relation,
we obtain%
\[
A\left(  3x+5Ay+7A^{2}z\right)  =0.
\]
So the $1$-tuple $\left(  3x+5Ay+7A^{2}z\right)  $ is an orbit.

Now, let us replace the orbit $\left(  x,Ax\right)  $ in our spanning set
$\left(  x,Ax,\ \ y,Ay,A^{2}y,\ \ z,Az,A^{2}z,A^{3}z,\ \ w\right)  $ by the
orbit $\left(  3x+5Ay+7A^{2}z\right)  $. We get
\[
\left(  3x+5Ay+7A^{2}z,\ \ y,Ay,A^{2}y,\ \ z,Az,A^{2}z,A^{3}z,\ \ w\right)  .
\]
This is still a concatenation of orbits, since the $1$-tuple $\left(
3x+5Ay+7A^{2}z\right)  $ is an orbit. Furthermore, this is still a spanning
set of $\mathbb{F}^{n}$; why? Because we removed the dependent vector $Ax$
(this is a combination of the other vectors, because $3Ax+5A^{2}y+7A^{3}z=0$)
and we replaced $x$ by $3x+5Ay+7A^{2}z$ (which does not change the span,
because $Ay$ and $A^{2}z$ are still in the spanning set).

This example generalizes. In the general case, you have a spanning set that is
a concatenation of orbits:%
\[
\left(  v_{1},Av_{1},\ldots,A^{m_{1}}v_{1},\ v_{2},Av_{2},\ldots,A^{m_{2}%
}v_{2},\ \ldots,\ v_{k},Av_{k},\ldots,A^{m_{k}}v_{k}\right)  .
\]
If it is a basis, you are done. If not, you pick a linear dependence relation:%
\[
\sum_{i,j}\lambda_{i,j}A^{j}v_{i}=0.
\]
By multiplying this by $A$ an appropriate amount of times (namely, you keep
multiplying until it becomes $0=0$, and then you take a step back), you obtain
a linear dependence relation that involves only the \textbf{final} vectors of
the orbits (i.e., the vectors $A^{m_{1}}v_{1},\ A^{m_{2}}v_{2},\ \ldots
,\ A^{m_{k}}v_{k}$). So it will look like this:%
\[
\mu_{1}A^{m_{1}}v_{1}+\mu_{2}A^{m_{2}}v_{2}+\cdots+\mu_{k}A^{m_{k}}v_{k}=0.
\]
Assume WLOG that the first $p$ of the $\mu_{1},\mu_{2},\ldots,\mu_{k}$ are
nonzero, while the remaining $k-p$ are $0$. So the relation becomes%
\[
\mu_{1}A^{m_{1}}v_{1}+\mu_{2}A^{m_{2}}v_{2}+\cdots+\mu_{p}A^{m_{p}}v_{p}=0,
\]
with $\mu_{1},\mu_{2},\ldots,\mu_{p}$ being nonzero. Assume WLOG that
$m_{1}=\min\left\{  m_{1},m_{2},\ldots,m_{p}\right\}  $, and factor out
$A^{m_{1}}$ from this relation. This yields%
\[
A^{m_{1}}\left(  \mu_{1}v_{1}+\mu_{2}A^{m_{2}-m_{1}}v_{2}+\cdots+\mu
_{p}A^{m_{p}-m_{1}}v_{p}\right)  =0.
\]
Now, set $w_{1}=\mu_{1}v_{1}+\mu_{2}A^{m_{2}-m_{1}}v_{2}+\cdots+\mu
_{p}A^{m_{p}-m_{1}}v_{p}$. Thus, $A^{m_{1}}w_{1}=0$. Hence, $\left(
w_{1},Aw_{1},A^{2}w_{1},\ldots,A^{m_{1}-1}w_{1}\right)  $ is an orbit of
length $m_{1}$. Now, replace the orbit $\left(  v_{1},Av_{1},\ldots,A^{m_{1}%
}v_{1}\right)  $ in the spanning set%
\[
\left(  v_{1},Av_{1},\ldots,A^{m_{1}}v_{1},\ v_{2},Av_{2},\ldots,A^{m_{2}%
}v_{2},\ \ldots,\ v_{k},Av_{k},\ldots,A^{m_{k}}v_{k}\right)
\]
by the shorter orbit $\left(  w_{1},Aw_{1},A^{2}w_{1},\ldots,A^{m_{1}-1}%
w_{1}\right)  $. The resulting list%
\[
\left(  w_{1},Aw_{1},A^{2}w_{1},\ldots,A^{m_{1}-1}w_{1},\ v_{2},Av_{2}%
,\ldots,A^{m_{2}}v_{2},\ \ldots,\ v_{k},Av_{k},\ldots,A^{m_{k}}v_{k}\right)
\]
is still a concatenation of orbits. Also, it still spans $\mathbb{F}^{n}$,
because%
\begin{align*}
w_{1}  &  =\underbrace{\mu_{1}}_{\neq0}v_{1}+\mu_{2}A^{m_{2}-m_{1}}%
v_{2}+\cdots+\mu_{p}A^{m_{p}-m_{1}}v_{p};\\
Aw_{1}  &  =\underbrace{\mu_{1}}_{\neq0}Av_{1}+\mu_{2}A^{m_{2}-m_{1}+1}%
v_{2}+\cdots+\mu_{p}A^{m_{p}-m_{1}+1}v_{p};\\
&  \ldots;\\
A^{m_{1}}v_{1}  &  =-\left(  \mu_{2}A^{m_{2}}v_{2}+\cdots+\mu_{p}A^{m_{p}%
}v_{p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mu_{1}A^{m_{1}}v_{1}+\mu
_{2}A^{m_{2}}v_{2}+\cdots+\mu_{p}A^{m_{p}}v_{p}=0\right)  .
\end{align*}


So we have found a spanning set of $\mathbb{F}^{n}$ that is still a
concatenation of orbits, but is shorter (it has one less vector). Doing this
repeatedly, we will eventually obtain a basis (since we cannot keep making a
finite list shorter and shorter indefinitely). This proves the lemma.
\end{proof}

As we said, the lemma gives us a basis $\left(  s_{1},s_{2},\ldots
,s_{n}\right)  $ such that $As_{i}$ is either $s_{i-1}$ or $0$; and that shows
that $A$ is similar to a Jordan matrix. This completes the proof of the
existence part of the Jordan canonical form.

\begin{example}
Let $\mathbb{F}=\mathbb{C}$ and
\[
A=\left(
\begin{array}
[c]{cccccc}%
0 & 1 & 0 & -1 & 1 & -1\\
0 & 1 & 1 & -2 & 2 & -2\\
0 & 1 & 0 & -1 & 2 & -2\\
0 & 1 & 0 & -1 & 2 & -2\\
0 & 1 & 0 & -1 & 1 & -1\\
0 & 1 & 0 & -1 & 1 & -1
\end{array}
\right)  .
\]
This is not strictly upper-triangular, but it is nilpotent, with $A^{3}=0$, so
the above argument goes equally well with this $A$.

Let us try to find a basis of $\mathbb{F}^{6}$ that is a concatenation of orbits.

We begin with the spanning set%
\[
\left(  e_{1},Ae_{1},A^{2}e_{1},\ \ e_{2},Ae_{2},A^{2}e_{2},\ \ \ldots
,\ \ e_{6},Ae_{6},A^{2}e_{6}\right)  .
\]
It has lots of linear dependencies. For one, $Ae_{1}=0$. Multiplying it by $A$
gives $A^{2}e_{1}=0$, so we can replace $\left(  e_{1},Ae_{1},A^{2}%
e_{1}\right)  $ by $\left(  e_{1},Ae_{1}\right)  $. So our spanning set
becomes%
\[
\left(  e_{1},Ae_{1},\ \ e_{2},Ae_{2},A^{2}e_{2},\ \ \ldots,\ \ e_{6}%
,Ae_{6},A^{2}e_{6}\right)  .
\]
One more step of the same form gives%
\[
\left(  e_{1},\ \ e_{2},Ae_{2},A^{2}e_{2},\ \ \ldots,\ \ e_{6},Ae_{6}%
,A^{2}e_{6}\right)  .
\]


Now, observe that $Ae_{3}=e_{2}$. That is, $e_{2}-Ae_{3}=0$. Multiplying it by
$A^{2}$, we obtain $A^{2}e_{2}=0$ (since $A^{2}\cdot Ae_{3}=A^{3}e_{3}=0$). So
we replace the orbit $\left(  e_{2},Ae_{2},A^{2}e_{2}\right)  $ by $\left(
e_{2},Ae_{2}\right)  $. So we get the spanning set
\[
\left(  e_{1},\ \ e_{2},Ae_{2},\ \ e_{3},Ae_{3},A^{2}e_{3},\ \ e_{4}%
,Ae_{4},A^{2}e_{4},\ \ e_{5},Ae_{5},A^{2}e_{5},\ \ e_{6},Ae_{6},A^{2}%
e_{6}\right)  .
\]


We observe that%
\[
Ae_{2}=e_{1}+e_{2}+e_{3}+e_{4}+e_{5}+e_{6}.
\]
In other words,%
\[
Ae_{2}-e_{1}-e_{2}-e_{3}-e_{4}-e_{5}-e_{6}=0.
\]
Multiplying this by $A^{2}$, we obtain%
\[
-A^{2}e_{3}-A^{2}e_{4}-A^{2}e_{5}-A^{2}e_{6}=0.
\]
In other words,%
\[
A^{2}\left(  -e_{3}-e_{4}-e_{5}-e_{6}\right)  =0.
\]
Thus, we set $w_{1}:=-e_{3}-e_{4}-e_{5}-e_{6}$, and we replace $\left(
e_{3},Ae_{3},A^{2}e_{3}\right)  $ by $\left(  w_{1},Aw_{1}\right)  $. So we
get the spanning set
\[
\left(  e_{1},\ \ e_{2},Ae_{2},\ \ w_{1},Aw_{1},\ \ e_{4},Ae_{4},A^{2}%
e_{4},\ \ e_{5},Ae_{5},A^{2}e_{5},\ \ e_{6},Ae_{6},A^{2}e_{6}\right)  .
\]
Keep making these steps. Eventually, there will be no more linear
dependencies, so we will have a basis.
\end{example}

\subsection{Powers and the Jordan canonical form}

Let $n\in\mathbb{N}$ and $A\in\mathbb{F}^{n\times n}$. Assume that we know the
JCF $J$ of $A$ and an invertible matrix $S$ such that
\[
A=SJS^{-1}.
\]
Then, it is fairly easy to compute all powers $A^{m}$ of $A$. Indeed, recall that

\begin{itemize}
\item $\left(  SJS^{-1}\right)  ^{m}=SJ^{m}S^{-1}$ for any $m\in\mathbb{N}$.

\item $\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right)  ^{m}=\left(
\begin{array}
[c]{cccc}%
A_{1}^{m} &  &  & \\
& A_{2}^{m} &  & \\
&  & \ddots & \\
&  &  & A_{k}^{m}%
\end{array}
\right)  $ for any $m\in\mathbb{N}$.
\end{itemize}

Thus, it suffices to compute the $m$-th power of any Jordan cell $J_{k}\left(
\lambda\right)  $.

So let us consider a Jordan cell
\[
C:=J_{5}\left(  \lambda\right)  =\left(
\begin{array}
[c]{ccccc}%
\lambda & 1 &  &  & \\
& \lambda & 1 &  & \\
&  & \lambda & 1 & \\
&  &  & \lambda & 1\\
&  &  &  & \lambda
\end{array}
\right)  .
\]
Then,%
\begin{align*}
C^{2}  &  =\left(
\begin{array}
[c]{ccccc}%
\lambda^{2} & 2\lambda & 1 &  & \\
& \lambda^{2} & 2\lambda & 1 & \\
&  & \lambda^{2} & 2\lambda & 1\\
&  &  & \lambda^{2} & 2\lambda\\
&  &  &  & \lambda^{2}%
\end{array}
\right)  ;\ \ \ \ \ \ \ \ \ \ C^{3}=\left(
\begin{array}
[c]{ccccc}%
\lambda^{3} & 3\lambda^{2} & 3\lambda & 1 & \\
& \lambda^{3} & 3\lambda^{2} & 3\lambda & 1\\
&  & \lambda^{3} & 3\lambda^{2} & 3\lambda\\
&  &  & \lambda^{3} & 3\lambda^{2}\\
&  &  &  & \lambda^{3}%
\end{array}
\right)  ;\\
C^{4}  &  =\left(
\begin{array}
[c]{ccccc}%
\lambda^{4} & 4\lambda^{3} & 6\lambda^{2} & 4\lambda & 1\\
& \lambda^{4} & 4\lambda^{3} & 6\lambda^{2} & 4\lambda\\
&  & \lambda^{4} & 4\lambda^{3} & 6\lambda^{2}\\
&  &  & \lambda^{4} & 4\lambda^{3}\\
&  &  &  & \lambda^{4}%
\end{array}
\right)  ;\ \ \ \ \ \ \ \ \ \ C^{5}=\left(
\begin{array}
[c]{ccccc}%
\lambda^{5} & 5\lambda^{4} & 10\lambda^{3} & 10\lambda^{2} & 5\lambda\\
& \lambda^{5} & 5\lambda^{4} & 10\lambda^{3} & 10\lambda^{2}\\
&  & \lambda^{5} & 5\lambda^{4} & 10\lambda^{3}\\
&  &  & \lambda^{5} & 5\lambda^{4}\\
&  &  &  & \lambda^{5}%
\end{array}
\right)  .
\end{align*}
In general:

\begin{theorem}
Let $k>0$ and $\lambda\in\mathbb{F}$. Let $C=J_{k}\left(  \lambda\right)  $.
Let $m\in\mathbb{N}$. Then, $C^{m}$ is the upper-triangular $k\times k$-matrix
whose $\left(  i,j\right)  $-th entry is $\dbinom{m}{j-i}\lambda^{m-j+i}$.
(Here, we follow the convention that $\dbinom{n}{\ell}:=0$ when $\ell
\notin\mathbb{N}$. Also, recall that $\dbinom{n}{\ell}=0$ when $n\in
\mathbb{N}$ and $\ell>n$.)
\end{theorem}

\begin{proof}
[First proof.]Induct on $m$ and use $C^{m}=CC^{m-1}$ as well as Pascal's
recursion%
\[
\dbinom{n}{\ell}=\dbinom{n-1}{\ell}+\dbinom{n-1}{\ell-1}.
\]

\end{proof}

\begin{proof}
[Second proof.]Set $B:=J_{k}\left(  0\right)  =\left(
\begin{array}
[c]{ccccc}
& 1 &  &  & \\
&  & 1 &  & \\
&  &  & 1 & \\
&  &  &  & 1\\
&  &  &  &
\end{array}
\right)  $. We know all powers of $B$ already: $B^{i}$ has $1$s $i$ steps
above the main diagonal, and $0$s everywhere else.

However, $C=B+\lambda I_{k}$. The matrices $\lambda I_{k}$ and $B$ commute
(i.e., we have $B\cdot\lambda I_{k}=\lambda I_{k}\cdot B$). It is a general
fact that if $X$ and $Y$ are two commuting $n\times n$-matrices, then the
binomial formula%
\[
\left(  X+Y\right)  ^{m}=\sum_{i=0}^{m}\dbinom{m}{i}X^{i}Y^{m-i}.
\]
(This can be proved in the same way as for numbers, because the commutativity
of $X$ and $Y$ lets you move any $X$es past any $Y$s.) Applying this formula
to $X=B$ and $Y=\lambda I_{k}$, we obtain%
\begin{align*}
\left(  B+\lambda I_{k}\right)  ^{m}  &  =\sum_{i=0}^{m}\dbinom{m}{i}%
B^{i}\underbrace{\left(  \lambda I_{k}\right)  ^{m-i}}_{=\lambda^{m-i}I_{k}%
}=\sum_{i=0}^{m}\dbinom{m}{i}\lambda^{m-i}B^{i}\\
&  =\left(
\begin{array}
[c]{ccccccc}%
\lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \dbinom{m}{2}\lambda^{m-2} & \cdots
& \cdots & \cdots & \cdots\\
& \lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \dbinom{m}{2}\lambda^{m-2} &
\cdots & \cdots & \cdots\\
&  & \lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \dbinom{m}{2}\lambda^{m-2} &
\cdots & \cdots\\
&  &  & \lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \cdots & \cdots\\
&  &  &  & \lambda^{m} & \cdots & \vdots\\
&  &  &  &  & \ddots & \vdots\\
&  &  &  &  &  & \lambda^{m}%
\end{array}
\right)  ,
\end{align*}
which is precisely the matrix claimed in the theorem.
\end{proof}

Now we know how to take powers of Jordan cells, and therefore how to take
powers of any matrix that we know how to bring to a Jordan canonical form.

\begin{corollary}
Let $A\in\mathbb{C}^{n\times n}$. Then, $\lim\limits_{m\rightarrow\infty}%
A^{m}=0$ if and only if all eigenvalues of $A$ have absolute value $<1$.
\end{corollary}

\begin{proof}
$\Longrightarrow:$ Suppose that $\lim\limits_{m\rightarrow\infty}A^{m}=0$, but
$A$ has an eigenvalue $\lambda$ of absolute value $\geq1$. We want a contradiction.

Consider a nonzero eigenvector $x$ for eigenvalue $\lambda$. Thus, $Ax=\lambda
x$. Then, $A^{2}x=\lambda^{2}x$ (since $A^{2}x=A\underbrace{Ax}_{=\lambda
x}=\lambda\underbrace{Ax}_{=\lambda x}=\lambda\lambda x=\lambda^{2}x$) and
similarly $A^{3}x=\lambda^{3}x$ and $A^{4}x=\lambda^{4}x$ and so on. Thus,
\[
A^{m}x=\lambda^{m}x\ \ \ \ \ \ \ \ \ \ \text{for each }m\in\mathbb{N}.
\]


Now, as $m\rightarrow\infty$, the vector $A^{m}x$ goes to $0$ (since
$A^{m}\rightarrow0$), but the vector $\lambda^{m}x$ does not (since $x\neq0$
and $\left\vert \lambda\right\vert \geq1$). Contradiction.

$\Longleftarrow:$ Suppose that all eigenvalues of $A$ have absolute value $<1$.

Let $A=SJS^{-1}$ be the Jordan canonical form of $A$. Write $J$ as $\left(
\begin{array}
[c]{ccc}%
J_{1} &  & \\
& \ddots & \\
&  & J_{p}%
\end{array}
\right)  $, where each $J_{i}$ is a Jordan cell.

It suffices to show that $\lim\limits_{n\rightarrow\infty}J_{i}^{m}=0$.

Write $J_{i}$ as $J_{k}\left(  \lambda\right)  $, with $\left\vert
\lambda\right\vert <1$. The preceding theorem then tells us that%
\[
J_{i}^{m}=\left(
\begin{array}
[c]{ccccccc}%
\lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \dbinom{m}{2}\lambda^{m-2} & \cdots
& \cdots & \cdots & \cdots\\
& \lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \dbinom{m}{2}\lambda^{m-2} &
\cdots & \cdots & \cdots\\
&  & \lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \dbinom{m}{2}\lambda^{m-2} &
\cdots & \cdots\\
&  &  & \lambda^{m} & \dbinom{m}{1}\lambda^{m-1} & \cdots & \cdots\\
&  &  &  & \lambda^{m} & \cdots & \vdots\\
&  &  &  &  & \ddots & \vdots\\
&  &  &  &  &  & \lambda^{m}%
\end{array}
\right)  .
\]
Look closely at this matrix. We need to show that for each $i,j\in\left[
k\right]  $, we have%
\[
\lim\limits_{m\rightarrow\infty}\dbinom{m}{j-i}\lambda^{m-j+i}=0.
\]
However, this is a standard asymptotics argument:%
\[
\underbrace{\dbinom{m}{j-i}}_{\substack{=\dfrac{m\left(  m-1\right)  \left(
m-2\right)  \cdots\left(  m-j+i\right)  }{\left(  j-i\right)  !}\\\text{(for
}i\leq j\text{; otherwise the claim is trivial)}}}\underbrace{\lambda^{m-j+i}%
}_{\substack{\text{exponential in }m\text{,}\\\text{with quotient }%
\lambda\text{ having absolute value }\left\vert \lambda\right\vert
<1}}\rightarrow0
\]
because exponential functions with a quotient of absolute value $<1$ converge
to $0$ faster than polynomials can go to $\infty$.
\end{proof}

\subsection{The minimal polynomial}

\textbf{Recall:} A polynomial $p\left(  t\right)  \in\mathbb{F}\left[
t\right]  $ (where $\mathbb{F}$ is any field, and $t$ is an indeterminate) is
said to be \textbf{monic} if its leading coefficient is $1$ -- that is, if it
can be written in the form%
\[
p\left(  t\right)  =t^{m}+p_{m-1}t^{m-1}+p_{m-2}t^{m-2}+\cdots+p_{0}%
t^{0}\ \ \ \ \ \ \ \ \ \ \text{for some }m\in\mathbb{N}\text{ and }p_{0}%
,p_{1},\ldots,p_{m-1}\in\mathbb{F}.
\]


\begin{definition}
Given a matrix $A\in\mathbb{F}^{n\times n}$ and a polynomial $p\left(
t\right)  \in\mathbb{F}\left[  t\right]  $, we saay that $p\left(  t\right)  $
\textbf{annihilates} $A$ if $p\left(  A\right)  =0$.
\end{definition}

The Cayley--Hamilton theorem says that the characteristic polynomial $p_{A}$
of a square matrix $A$ always annihilates $A$. However, often there are
matrices that are annihilated by other -- sometimes simpler -- polynomials.

\begin{example}
The identity matrix $I_{n}$ is annihilated by the polynomial $p\left(
t\right)  :=t-1$, because%
\[
p\left(  I_{n}\right)  =I_{n}-I_{n}=0.
\]

\end{example}

\begin{example}
The matrix $\left(
\begin{array}
[c]{cc}%
0 & 1\\
0 & 0
\end{array}
\right)  $ is annihilated by the polynomial $p\left(  t\right)  :=t^{2}$,
since its square is $0$.
\end{example}

\begin{example}
The matrix $\left(
\begin{array}
[c]{ccc}%
0 & 1 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{array}
\right)  $ is annihilated by the polynomial $p\left(  t\right)  :=t^{2}$,
since its square is $0$.
\end{example}

\begin{example}
The diagonal matrix $\left(
\begin{array}
[c]{ccc}%
2 &  & \\
& 2 & \\
&  & 3
\end{array}
\right)  $ is annihilated by the polynomial $p\left(  t\right)  :=\left(
t-2\right)  \left(  t-3\right)  $, since%
\begin{align*}
&  \left(  \left(
\begin{array}
[c]{ccc}%
2 &  & \\
& 2 & \\
&  & 3
\end{array}
\right)  -2I_{n}\right)  \left(  \left(
\begin{array}
[c]{ccc}%
2 &  & \\
& 2 & \\
&  & 3
\end{array}
\right)  -3I_{n}\right) \\
&  =\left(
\begin{array}
[c]{ccc}
&  & \\
&  & \\
&  & 1
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
-1 &  & \\
& -1 & \\
&  &
\end{array}
\right)  =0.
\end{align*}

\end{example}

\begin{theorem}
Let $\mathbb{F}$ be a field. Let $A\in\mathbb{F}^{n\times n}$ be an $n\times
n$-matrix. Then, there is a \textbf{unique} monic polynomial $q_{A}\left(
t\right)  $ of minimum degree that annihilates $A$.
\end{theorem}

\begin{proof}
The Cayley--Hamilton theorem shows that $p_{A}$ annihilates $A$. Since $p_{A}$
is monic, we thus conclude that there exists \textbf{some} monic polynomial
that annihilates $A$. Hence, there exists such a polynomial of minimum degree.

It remains to show that it is unique. To do so, we let $q_{A}$ and
$\widetilde{q}_{A}$ be two monic polynomials of minimum degree that annihilate
$A$. Our goal then is to show that $q_{A}=\widetilde{q}_{A}$.

Indeed, if $q_{A}\neq\widetilde{q}_{A}$, then the difference $q_{A}%
-\widetilde{q}_{A}$ is a polynomial of smaller degree that annihilates $A$
(indeed, it is of smaller degree because $q_{A}$ and $\widetilde{q}_{A}$ are
monic of the same degree and thus lose their leading terms upon subtraction;
it annihilates $A$ because $\left(  q_{A}-\widetilde{q}_{A}\right)  \left(
A\right)  =q_{A}\left(  A\right)  -\widetilde{q}_{A}\left(  A\right)
=0-0=0$). By scaling this difference appropriately, we can make it monic
(since it is nonzero), and then get a contradiction to the minimality of
$q_{A}$'s degree. This concludes the proof.
\end{proof}

\begin{definition}
Let $A\in\mathbb{F}^{n\times n}$ be an $n\times n$-matrix. The preceding
theorem shows that there is a \textbf{unique} monic polynomial $q_{A}\left(
t\right)  $ of minimum degree that annihilates $A$. This unique polynomial
will be denoted $q_{A}\left(  t\right)  $ and will be called the
\textbf{minimal polynomial} of $A$.
\end{definition}

\begin{example}
Let $\mathbb{F}=\mathbb{C}$. Let $A$ be the diagonal matrix $\left(
\begin{array}
[c]{ccc}%
2 &  & \\
& 2 & \\
&  & 3
\end{array}
\right)  $. Then,%
\[
q_{A}\left(  t\right)  =\left(  t-2\right)  \left(  t-3\right)  .
\]
Indeed, we already know that the monic polynomial $\left(  t-2\right)  \left(
t-3\right)  $ annihilates $A$. If there was any monic polynomial of smaller
degree that would annihilate $A$, then it would have the form $t-\lambda$ for
some $\lambda\in\mathbb{F}$, but $\lambda$ cannot be $2$ and $3$ at the same time.

For comparison: $p_{A}\left(  t\right)  =\left(  t-2\right)  ^{2}\left(
t-3\right)  $.
\end{example}

\begin{exercise}
\fbox{2} Find the minimal polynomial of a diagonal matrix whose
\textbf{distinct} diagonal entries are $\lambda_{1},\lambda_{2},\ldots
,\lambda_{k}$. (Each of these $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$ can
appear on the diagonal any positive number of times.)
\end{exercise}

\begin{exercise}
\fbox{3} Let $\mathbb{F}$ be a field. Let $n\geq2$ be an integer. Let
$x_{1},x_{2},\ldots,x_{n}\in\mathbb{F}$ and $y_{1},y_{2},\ldots,y_{n}%
\in\mathbb{F}$. Let $A$ be the $n\times n$-matrix $\left(
\begin{array}
[c]{cccc}%
x_{1}y_{1} & x_{1}y_{2} & \cdots & x_{1}y_{n}\\
x_{2}y_{1} & x_{2}y_{2} & \cdots & x_{2}y_{n}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n}y_{1} & x_{n}y_{2} & \cdots & x_{n}y_{n}%
\end{array}
\right)  \in\mathbb{F}^{n\times n}$. \medskip

\textbf{(a)} Find the minimal polynomial of $A$ under the assumption that
$x_{1},x_{2},\ldots,x_{n}$ and $y_{1},y_{2},\ldots,y_{n}$ are nonzero.
\medskip

\textbf{(b)} What changes if we drop this assumption? \medskip

[\textbf{Hint:} Compute $A^{2}$.]
\end{exercise}

\begin{theorem}
Let $A\in\mathbb{F}^{n\times n}$ be an $n\times n$-matrix. Let $f\left(
t\right)  \in\mathbb{F}\left[  t\right]  $ be any polynomial. Then, $f$
annihilates $A$ if and only if $f$ is a multiple of $q_{A}$ (that is,
$f\left(  t\right)  =q_{A}\left(  t\right)  \cdot g\left(  t\right)  $ for
some polynomial $g\left(  t\right)  \in\mathbb{F}\left[  t\right]  $).
\end{theorem}

\begin{proof}
$\Longrightarrow:$ Assume that $f$ annihilates $A$. Thus, $f\left(  A\right)
=0$. WLOG, assume that $f\neq0$. Thus, we can make $f$ monic by scaling it.
Thus, $\deg f\geq\deg q_{A}$ (since $q_{A}$ had minimum degree). Hence, we can
divide $f$ by $q_{A}$ with remainder, obtaining%
\[
f\left(  t\right)  =q_{A}\left(  t\right)  \cdot g\left(  t\right)  +r\left(
t\right)  ,
\]
where $g\left(  t\right)  $ and $r\left(  t\right)  $ are two polynomials with
$\deg r<\deg q_{A}$. Substituting $A$ for $t$ in this equality, we obtain%
\[
f\left(  A\right)  =\underbrace{q_{A}\left(  A\right)  }%
_{\substack{=0\\\text{(since }q_{A}\text{ annihilates }A\text{)}}}\cdot
g\left(  A\right)  +r\left(  A\right)  =r\left(  A\right)  ,
\]
so that $r\left(  A\right)  =f\left(  A\right)  =0$. In other words, $r$
annihilates $A$. Since $\deg r<\deg q_{A}$, this entails that $r=0$ (since
otherwise, scaling $r$ to make it monic would contradict the minimality of
$\deg q_{A}$). Thus,%
\[
f\left(  t\right)  =q_{A}\left(  t\right)  \cdot g\left(  t\right)
+\underbrace{r\left(  t\right)  }_{=0}=q_{A}\left(  t\right)  \cdot g\left(
t\right)  .
\]
Thus, $f$ is a multiple of $q_{A}$.

$\Longleftarrow:$ Easy and LTTR.
\end{proof}

\begin{corollary}
Let $\mathbb{F}$ be a field. Let $A\in\mathbb{F}^{n\times n}$ be a matrix.
Then, $q_{A}\left(  t\right)  \mid p_{A}\left(  t\right)  $.
\end{corollary}

\begin{proof}
Apply the previous theorem to $f=p_{A}$, recalling that $p_{A}$ annihilates
$A$.
\end{proof}

The corollary yields that any root of $q_{A}$ must be a root of $p_{A}$, that
is, an eigenvalue of $A$. Conversely, we can show that any eigenvalue of $A$
is a root of $q_{A}$ (but we don't know with which multiplicity):

\begin{proposition}
Let $A\in\mathbb{C}^{n\times n}$ be an $n\times n$-matrix. If $\lambda
\in\sigma\left(  A\right)  $, then $q_{A}\left(  \lambda\right)  =0$.
\end{proposition}

\begin{proof}
Let $\lambda\in\sigma\left(  A\right)  $. Thus, there exists a nonzero
eigenvector $x$ for $\lambda$.

Then, $Ax=\lambda x$. As we have seen above, this entails $A^{m}x=\lambda
^{m}x$ for each $m\in\mathbb{N}$. Therefore, $f\left(  A\right)  x=f\left(
\lambda\right)  x$ for each polynomial $f\left(  t\right)  \in\mathbb{C}%
\left[  t\right]  $ (because you can write $f\left(  t\right)  $ as
$f_{0}t^{0}+f_{1}t^{1}+\cdots+f_{p}t^{p}$, and then apply $A^{m}x=\lambda
^{m}x$ to each of $m=0,1,\ldots,p$). Hence, $q_{A}\left(  A\right)
x=q_{A}\left(  \lambda\right)  x$, so that%
\[
q_{A}\left(  \lambda\right)  x=\underbrace{q_{A}\left(  A\right)
}_{\substack{=0\\\text{(since }q_{A}\text{ annihilates }A\text{)}}}x=0.
\]
Since $x\neq0$, this entails $q_{A}\left(  \lambda\right)  =0$, qed.
\end{proof}

Combining the Corollary with the Proposition, we see that the roots of
$q_{A}\left(  t\right)  $ are precisely the eigenvalues of $A$ (when
$A\in\mathbb{C}^{n\times n}$); we just don't know yet with which
multiplicities they appear as roots. In other words, we have
\[
q_{A}\left(  t\right)  =\left(  t-\lambda_{1}\right)  ^{k_{1}}\left(
t-\lambda_{2}\right)  ^{k_{2}}\cdots\left(  t-\lambda_{p}\right)  ^{k_{p}},
\]
where $\lambda_{1},\lambda_{2},\ldots,\lambda_{p}$ are the distinct
eigenvalues of $A$, and the $k_{1},k_{2},\ldots,k_{p}$ are positive integers;
but we don't know these $k_{1},k_{2},\ldots,k_{p}$ yet. So let us find them.
We will use some lemmas for this.

\begin{lemma}
Let $\mathbb{F}$ be a field. Let $A$ and $B$ be two similar $n\times
n$-matrices in $\mathbb{F}^{n\times n}$. Then, $q_{A}\left(  t\right)
=q_{B}\left(  t\right)  $.
\end{lemma}

\begin{proof}
This is obvious from the viewpoint of endomorphisms. For a pedestrian proof,
you can just argue that a polynomial $f$ annihilates $A$ if and only if it
annihilates $B$. But this is easy: We have $A=SBS^{-1}$ for some invertible
$S$ (since $A$ and $B$ are similar), and therefore every polynomial $f$
satisfies
\[
f\left(  A\right)  =f\left(  SBS^{-1}\right)  =Sf\left(  B\right)  S^{-1}%
\]
and therefore $f\left(  A\right)  =0$ holds if and only if $f\left(  B\right)
=0$.
\end{proof}

We recall the notion of the lcm (= least common multiple) of several
polynomials. It is defined as one would expect: If $p_{1},p_{2},\ldots,p_{m}$
are $m$ nonzero polynomials (in a single indeterminate $t$), then
$\operatorname{lcm}\left(  p_{1},p_{2},\ldots,p_{m}\right)  $ is the monic
polynomial of smallest degree that is a common multiple of $p_{1},p_{2}%
,\ldots,p_{m}$. For example,%
\begin{align*}
\operatorname{lcm}\left(  t^{2}-1,\ t^{3}-1\right)   &  =\operatorname{lcm}%
\left(  \left(  t-1\right)  \left(  t+1\right)  ,\ \left(  t-1\right)  \left(
t^{2}+t+1\right)  \right) \\
&  =\allowbreak\left(  t-1\right)  \left(  t+1\right)  \left(  t+t^{2}%
+1\right)  =\allowbreak t^{4}+t^{3}-t-1.
\end{align*}
(Again, the lcm of several polynomials is unique. This can be shown in the
same way that we used to prove uniqueness of the minimal polynomial.)

\begin{lemma}
Let $A_{1},A_{2},\ldots,A_{m}$ be $m$ square matrices. Let%
\[
A=\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{m}%
\end{array}
\right)  .
\]
Then,%
\[
q_{A}=\operatorname{lcm}\left(  q_{A_{1}},\ q_{A_{2}},\ \ldots,\ q_{A_{m}%
}\right)  .
\]

\end{lemma}

\begin{proof}
For any polynomial $f\in\mathbb{C}\left[  t\right]  $, we have%
\[
f\left(  A\right)  =f\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{m}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cccc}%
f\left(  A_{1}\right)  &  &  & \\
& f\left(  A_{2}\right)  &  & \\
&  & \ddots & \\
&  &  & f\left(  A_{m}\right)
\end{array}
\right)
\]
(indeed, the last equality follows from%
\[
\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{m}%
\end{array}
\right)  ^{k}=\left(
\begin{array}
[c]{cccc}%
A_{1}^{k} &  &  & \\
& A_{2}^{k} &  & \\
&  & \ddots & \\
&  &  & A_{m}^{k}%
\end{array}
\right)
\]
and from the fact that a polynomial $f$ is just a $\mathbb{C}$-linear
combination of $t^{k}$s). Thus, $f\left(  A\right)  =0$ holds if and only if%
\[
f\left(  A_{1}\right)  =0\text{ and }f\left(  A_{2}\right)  =0\text{ and
}\cdots\text{ and }f\left(  A_{m}\right)  =0.
\]
However, $f\left(  A\right)  =0$ holds if and only if $f$ is a multiple of
$q_{A}$, whereas $f\left(  A_{i}\right)  =0$ holds if and only if $f$ is a
multiple of $q_{A_{i}}$. Thus, the previous sentence says that $f$ is a
multiple of $q_{A}$ if and only if $f$ is a multiple of all of the $q_{A_{i}}%
$s. In other words, the multiples of $q_{A}$ are precisely the common multiple
of all the $q_{A_{i}}$s. But this is the universal property of the lcm. So
$q_{A}$ is the lcm of the $q_{A_{i}}$s.
\end{proof}

\begin{lemma}
Let $k>0$ and $\lambda\in\mathbb{F}$. Let $A=J_{k}\left(  \lambda\right)  $.
Then,%
\[
q_{A}=\left(  t-\lambda\right)  ^{k}.
\]

\end{lemma}

\begin{proof}
It is easy to see that $q_{A}=q_{A-\lambda I_{k}}\left(  t-\lambda\right)  $,
because for a polynomial $f$ to annihilate $A-\lambda I_{k}$ is the same as
for the polynomial $f\left(  t-\lambda\right)  $ to annihilate $A$. So we need
to find $q_{A-\lambda I_{k}}$. Recall that
\[
A-\lambda I_{k}=J_{k}\left(  0\right)  =\left(
\begin{array}
[c]{ccccc}
& 1 &  &  & \\
&  & 1 &  & \\
&  &  &  & \\
&  &  & \ddots & \\
&  &  &  &
\end{array}
\right)  .
\]
Therefore, for any polynomial $f=f_{0}t^{0}+f_{1}t^{1}+f_{2}t^{2}+\cdots$, we
have%
\[
f\left(  A-\lambda I_{k}\right)  =\left(
\begin{array}
[c]{ccccc}%
f_{0} & f_{1} & f_{2} & \cdots & f_{k-1}\\
& f_{0} & f_{1} & \cdots & f_{k-2}\\
&  & f_{0} & \cdots & f_{k-3}\\
&  &  & \ddots & \vdots\\
&  &  &  & f_{0}%
\end{array}
\right)  .
\]
So $f\left(  A-\lambda I_{k}\right)  =0$ if and only if $f_{0}=f_{1}%
=\cdots=f_{k-1}=0$, i.e., if and only if the first $k$ coefficients of $f$ are
$0$. Now, the monic polynomial of smallest degree whose first $k$ coefficients
are $0$ is the polynomial $t^{k}$. So the monic polynomial $f$ of smallest
degree that satisfies $f\left(  A-\lambda I_{k}\right)  =0$ is $t^{k}$. In
other words, $q_{A-\lambda I_{k}}=t^{k}$.

Now, recall that $q_{A}=q_{A-\lambda I_{k}}\left(  t-\lambda\right)  =\left(
t-\lambda\right)  ^{k}$ (since $q_{A-\lambda I_{k}}=t^{k}$). Qed.
\end{proof}

\begin{theorem}
Let $A\in\mathbb{C}^{n\times n}$ be an $n\times n$-matrix. Let $J$ be the
Jordan canonical form of $A$. Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{p}$
be the distinct eigenvalues of $A$. Then,%
\[
q_{A}=\left(  t-\lambda_{1}\right)  ^{k_{1}}\left(  t-\lambda_{2}\right)
^{k_{2}}\cdots\left(  t-\lambda_{p}\right)  ^{k_{p}},
\]
where $k_{i}$ is the size of the largest Jordan cell at eigenvalue
$\lambda_{i}$ in $J$.
\end{theorem}

\begin{example}
Let $A$ have Jordan canonical form%
\[
J=\left(
\begin{array}
[c]{cccccccc}%
5 & 1 &  &  &  &  &  & \\
& 5 & 1 &  &  &  &  & \\
&  & 5 &  &  &  &  & \\
&  &  & 5 & 1 &  &  & \\
&  &  &  & 5 &  &  & \\
&  &  &  &  & 2 &  & \\
&  &  &  &  &  & 2 & 1\\
&  &  &  &  &  &  & 2
\end{array}
\right)  .
\]
Then,%
\[
q_{A}=\left(  t-5\right)  ^{3}\left(  t-2\right)  ^{2}.
\]

\end{example}

\begin{proof}
[Proof of the Theorem.]We have $A\sim J$, so that $q_{A}=q_{J}$ (by our first lemma).

Recall that $J$ is a Jordan matrix, i.e., a block-diagonal matrix whose
diagonal blocks are Jordan cells $J_{1},J_{2},\ldots,J_{m}$. Thus, by our
second lemma, we have%
\begin{align*}
q_{J}  &  =\operatorname{lcm}\left(  q_{J_{1}},\ q_{J_{2}},\ \ldots
,\ q_{J_{m}}\right) \\
&  =\operatorname{lcm}\left(  \left(  t-\lambda_{J_{1}}\right)  ^{k_{J_{1}}%
},\ \left(  t-\lambda_{J_{2}}\right)  ^{k_{J_{2}}},\ \ldots,\ \left(
t-\lambda_{J_{m}}\right)  ^{k_{J_{m}}}\right)  ,
\end{align*}
where each $J_{i}$ has eigenvalue $\lambda_{J_{i}}$ and size $k_{J_{i}}$ (by
our third lemma). This lcm must be divisible by each $t-\lambda$ at least as
often as each of the $\left(  t-\lambda_{J_{i}}\right)  ^{k_{J_{i}}}$s is;
i.e., it must be divisible by $\left(  t-\lambda\right)  ^{k}$, where $k$ is
the largest size of a Jordan cell of $J$ at eigenvalue $\lambda$. So the lcm
is the product of these $\left(  t-\lambda\right)  ^{k}$s. But this is
precisely our claim.
\end{proof}

\subsection{Application of functions to matrices}

Consider a square matrix $A\in\mathbb{C}^{n\times n}$. We have already defined
what it means to apply a polynomial $f$ to $A$: We just write $f$ as $\sum
_{i}f_{i}t^{i}$, and substitute $A$ for $t$.

Can we do the same with non-polynomial functions $f$ ? For example, can we
define $\exp A$ or $\sin A$ ?

One option to do so is to follow the same rule as for polynomials, but using
the Taylor series for $f$. For example, since $\exp$ has Taylor series $\exp
t=\sum_{i\in\mathbb{N}}\dfrac{t^{i}}{i!}$, we can set%
\[
\exp A=\sum_{i\in\mathbb{N}}\dfrac{A^{i}}{i!}.
\]
This indeed works for $\exp$ and for $\sin$, as the sums you get always
converge. But it doesn't generally work, e.g., for $f=\tan t$, since its
Taylor series only converges in a certain neighborhood of $0$. Is this the
best we can do?

There is a different approach that gives a more general definition.

\begin{lemma}
Let $k>0$ and $\lambda\in\mathbb{C}$. Let $A=J_{k}\left(  \lambda\right)  $.
Then, for any polynomial $f\in\mathbb{C}\left[  t\right]  $, we have%
\[
f\left(  A\right)  =\left(
\begin{array}
[c]{ccccc}%
\dfrac{f\left(  \lambda\right)  }{0!} & \dfrac{f^{\prime}\left(
\lambda\right)  }{1!} & \dfrac{f^{\prime\prime}\left(  \lambda\right)  }{2!} &
\cdots & \dfrac{f^{\left(  k-1\right)  }\left(  \lambda\right)  }{\left(
k-1\right)  !}\\
& \dfrac{f\left(  \lambda\right)  }{0!} & \dfrac{f^{\prime}\left(
\lambda\right)  }{1!} & \cdots & \dfrac{f^{\left(  k-2\right)  }\left(
\lambda\right)  }{\left(  k-2\right)  !}\\
&  & \dfrac{f\left(  \lambda\right)  }{0!} & \cdots & \dfrac{f^{\left(
k-3\right)  }\left(  \lambda\right)  }{\left(  k-3\right)  !}\\
&  &  & \ddots & \vdots\\
&  &  &  & \dfrac{f\left(  \lambda\right)  }{0!}%
\end{array}
\right)
\]

\end{lemma}

\begin{proof}
Exercise.
\end{proof}

\begin{exercise}
\fbox{2} Prove this.
\end{exercise}

Now, we aim to define $f\left(  A\right)  $ by the above formula, at least
when $A$ is a Jordan cell. This only requires $f$ to be $\left(  k-1\right)
$-times differentiable at $\lambda$.

\begin{definition}
Let $A\in\mathbb{C}^{n\times n}$ be an $n\times n$-matrix that has minimal
polynomial%
\[
q_{A}\left(  t\right)  =\left(  t-\lambda_{1}\right)  ^{k_{1}}\left(
t-\lambda_{2}\right)  ^{k_{2}}\cdots\left(  t-\lambda_{p}\right)  ^{k_{p}},
\]
where the $\lambda_{1},\lambda_{2},\ldots,\lambda_{p}$ are the distinct
eigenvalues of $A$.

Let $f$ be a function from $\mathbb{C}$ to $\mathbb{C}$ that is defined at
each of the numbers $\lambda_{1},\lambda_{2},\ldots,\lambda_{p}$ and is
holomorphic at each of them, or at least $\left(  k_{i}-1\right)  $-times
differentiable at each $\lambda_{i}$ if $\lambda_{i}$ is real. Then, we can
define an $n\times n$-matrix $f\left(  A\right)  \in\mathbb{C}^{n\times n}$ as
follows: Write $A=SJS^{-1}$, where $J$ is a Jordan matrix and $S$ is
invertible. Write $J$ as $\left(
\begin{array}
[c]{cccc}%
J_{1} &  &  & \\
& J_{2} &  & \\
&  & \ddots & \\
&  &  & J_{m}%
\end{array}
\right)  $, where the $J_{1},J_{2},\ldots,J_{m}$ are Jordan cells. Then, we
set%
\begin{align*}
f\left(  A\right)   &  :=Sf\left(  J\right)  S^{-1}%
,\ \ \ \ \ \ \ \ \ \ \text{where}\\
f\left(  J\right)   &  :=\left(
\begin{array}
[c]{cccc}%
f\left(  J_{1}\right)  &  &  & \\
& f\left(  J_{2}\right)  &  & \\
&  & \ddots & \\
&  &  & f\left(  J_{m}\right)
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ \text{where}\\
f\left(  J_{k}\left(  \lambda\right)  \right)   &  :=\left(
\begin{array}
[c]{ccccc}%
\dfrac{f\left(  \lambda\right)  }{0!} & \dfrac{f^{\prime}\left(
\lambda\right)  }{1!} & \dfrac{f^{\prime\prime}\left(  \lambda\right)  }{2!} &
\cdots & \dfrac{f^{\left(  k-1\right)  }\left(  \lambda\right)  }{\left(
k-1\right)  !}\\
& \dfrac{f\left(  \lambda\right)  }{0!} & \dfrac{f^{\prime}\left(
\lambda\right)  }{1!} & \cdots & \dfrac{f^{\left(  k-2\right)  }\left(
\lambda\right)  }{\left(  k-2\right)  !}\\
&  & \dfrac{f\left(  \lambda\right)  }{0!} & \cdots & \dfrac{f^{\left(
k-3\right)  }\left(  \lambda\right)  }{\left(  k-3\right)  !}\\
&  &  & \ddots & \vdots\\
&  &  &  & \dfrac{f\left(  \lambda\right)  }{0!}%
\end{array}
\right)  .
\end{align*}

\end{definition}

\begin{theorem}
This definition is actually well-defined. That is, the value $f\left(
A\right)  $ does not depend on the choice of $S$ and $J$.
\end{theorem}

\begin{exercise}
\fbox{5} Prove this.

[\textbf{Hint:} Use Hermite interpolation to find a polynomial $g\in
\mathbb{C}\left[  t\right]  $ such that $g^{\left(  m\right)  }\left(
\lambda\right)  =f^{\left(  m\right)  }\left(  \lambda\right)  $ for each
$\lambda\in\sigma\left(  A\right)  $ and each $m\in\left\{  0,1,\ldots
,m_{\lambda}-1\right\}  $, where $m_{\lambda}$ is the algebraic multiplicity
of $\lambda$ as an eigenvalue of $A$.]
\end{exercise}

\subsection{The companion matrix}

For each $n\times n$-matrix $A$, we have defined its characteristic polynomial
$p_{A}$ and its minimal polynomial $q_{A}$. What variety of polynomials do we
get this way? Do all characteristic polynomials share some property, or can
any monic polynomial be a characteristic polynomial?

The latter turns out to be true (and moreover, the same holds for the minimal
polynomial). We shall prove this by explicitly constructing a matrix with a
given polynomial as its characteristic polynomila.

\begin{definition}
Let $\mathbb{F}$ be a field, and let $n\in\mathbb{N}$.

Let $f\left(  t\right)  =t^{n}+f_{n-1}t^{n-1}+f_{n-2}t^{n-2}+\cdots+f_{1}%
t^{1}+f_{0}t^{0}$ be a monic polynomial of degree $n$ with coefficients in
$\mathbb{F}$. Then, the \textbf{companion matrix} of $f\left(  t\right)  $ is
defined to be the matrix%
\[
C_{f}:=\left(
\begin{array}
[c]{cccccc}%
0 &  &  & \cdots &  & -f_{0}\\
1 & 0 &  & \cdots &  & -f_{1}\\
& 1 & 0 & \cdots &  & -f_{2}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
&  &  & \cdots & 0 & -f_{n-2}\\
&  &  & \cdots & 1 & -f_{n-1}%
\end{array}
\right)  \in\mathbb{F}^{n\times n}.
\]
This is the $n\times n$-matrix whose first $n-1$ columns are the standard
basis vectors $e_{2},e_{3},\ldots,e_{n}$, and whose last column is $\left(
-f_{0},-f_{1},\ldots,-f_{n-1}\right)  ^{T}$.
\end{definition}

\begin{proposition}
For any monic polynomial $f\left(  t\right)  $, we have%
\[
p_{C_{f}}\left(  t\right)  =q_{C_{f}}\left(  t\right)  =f\left(  t\right)  .
\]

\end{proposition}

\begin{proof}
Let us first show that $p_{C_{f}}\left(  t\right)  =f\left(  t\right)  $. To
do so, we induct on $n$. Recall that%
\begin{align*}
p_{C_{f}}\left(  t\right)   &  =\det\left(  tI_{n}-C_{f}\right) \\
&  =\det\left(
\begin{array}
[c]{cccccc}%
t &  &  & \cdots &  & f_{0}\\
-1 & t &  & \cdots &  & f_{1}\\
& -1 & t & \cdots &  & f_{2}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
&  &  & \cdots & t & f_{n-2}\\
&  &  & \cdots & -1 & t+f_{n-1}%
\end{array}
\right)  .
\end{align*}
We compute this determinant by Laplace expansion along the first row:%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccccc}%
t &  &  & \cdots &  & f_{0}\\
-1 & t &  & \cdots &  & f_{1}\\
& -1 & t & \cdots &  & f_{2}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
&  &  & \cdots & t & f_{n-2}\\
&  &  & \cdots & -1 & t+f_{n-1}%
\end{array}
\right) \\
&  =t\ \underbrace{\det\left(
\begin{array}
[c]{ccccc}%
t &  & \cdots &  & f_{1}\\
-1 & t & \cdots &  & f_{2}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
&  & \cdots & t & f_{n-2}\\
&  & \cdots & -1 & t+f_{n-1}%
\end{array}
\right)  }_{\substack{=t^{n-1}+f_{n-1}t^{n-2}+\cdots+f_{2}t^{1}+f_{1}%
t^{0}\\\text{(by the induction hypothesis)}}}+\left(  -1\right)  ^{n+1}%
f_{0}\ \underbrace{\det\left(
\begin{array}
[c]{ccccc}%
-1 & t &  & \cdots & \\
& -1 & t & \cdots & \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
&  &  & \cdots & t\\
&  &  & \cdots & -1
\end{array}
\right)  }_{\substack{=\left(  -1\right)  ^{n-1}\\\text{(since this matrix is
upper-triangular}\\\text{of size }n-1\text{)}}}\\
&  =t\left(  t^{n-1}+f_{n-1}t^{n-2}+\cdots+f_{2}t^{1}+f_{1}t^{0}\right)
+\underbrace{\left(  -1\right)  ^{n+1}f_{0}\left(  -1\right)  ^{n-1}}_{=f_{0}%
}\\
&  =t\left(  t^{n-1}+f_{n-1}t^{n-2}+\cdots+f_{2}t^{1}+f_{1}t^{0}\right)
+f_{0}\\
&  =t^{n}+f_{n-1}t^{n-1}+f_{2}t^{2}+f_{1}t^{1}+f_{0}=f\left(  t\right)  .
\end{align*}


Thus, $p_{C_{f}}\left(  t\right)  =f\left(  t\right)  $ is proved.

Now, let us show that $q_{C_{f}}\left(  t\right)  =f\left(  t\right)  $.
Indeed, both $q_{C_{f}}\left(  t\right)  $ and $f\left(  t\right)  $ are monic
polynomials, and we know from last lecture that $q_{C_{f}}\left(  t\right)
\mid p_{C_{f}}\left(  t\right)  =f\left(  t\right)  $. Hence, if $q_{C_{f}%
}\left(  t\right)  \neq f\left(  t\right)  $, then $q_{C_{f}}\left(  t\right)
$ is a proper divisor of $f\left(  t\right)  $, thus has degree $<n$ (since
$f\left(  t\right)  $ has degree $n$). So we just need to rule out the
possibility that $q_{C_{f}}\left(  t\right)  $ has degree $<n$.

Indeed, assume (for the sake of contradiction) that $q_{C_{f}}\left(
t\right)  $ has degree $<n$. Thus, $q_{C_{f}}\left(  t\right)  =a_{k}%
t^{k}+a_{k-1}t^{k-1}+\cdots+a_{0}t^{0}$ with $k<n$ and $a_{k}=1$ (since
$q_{C_{f}}$ is monic of degree $<n$). However, the definition of $q_{C_{f}}$
yields $q_{C_{f}}\left(  C_{f}\right)  =0$. In other words,%
\[
a_{k}C_{f}^{k}+a_{k-1}C_{f}^{k-1}+\cdots+a_{0}C_{f}^{0}=0.
\]


However, let us look at what $C_{f}$ does to the standard basis vector
$e_{1}=\left(  1,0,0,0,\ldots,0\right)  ^{T}$. We have%
\begin{align*}
C_{f}^{0}e_{1}  &  =e_{1};\\
C_{f}^{1}e_{1}  &  =C_{f}e_{1}=e_{2};\\
C_{f}^{2}e_{1}  &  =C_{f}e_{2}=e_{3};\\
&  \ldots;\\
C_{f}^{n-1}e_{1}  &  =e_{n}.
\end{align*}
Thus, applying our equality%
\[
a_{k}C_{f}^{k}+a_{k-1}C_{f}^{k-1}+\cdots+a_{0}C_{f}^{0}=0
\]
to $e_{1}$, we obtain%
\[
a_{k}e_{k+1}+a_{k-1}e_{k}+\cdots+a_{0}e_{1}=0\ \ \ \ \ \ \ \ \ \ \left(
\text{since }k<n\right)  .
\]
But this is absurd, since $e_{1},e_{2},\ldots,e_{n}$ are linearly independent.
So we found a contradiction, and thus we conclude that $q_{C_{f}}\left(
t\right)  $ has degree $\geq n$. So, by the above, we obtain $q_{C_{f}}\left(
t\right)  =f\left(  t\right)  $.
\end{proof}

\begin{remark}
For algebraists: The companion matrix $C_{f}$ has a natural meaning. To wit,
consider the quotient ring $\mathbb{F}\left[  t\right]  /\left(  f\left(
t\right)  \right)  $ as an $n$-dimensional $\mathbb{F}$-vector space with
basis $\left(  \overline{t^{0}},\overline{t^{1}},\ldots,\overline{t^{n-1}%
}\right)  $. Then, the companion matrix $C_{f}$ represents the endomorphism
\textquotedblleft multiply by $t$\textquotedblright\ (that is, the
endomorphism that sends each $\overline{f\left(  t\right)  }$ to
$\overline{t\cdot f\left(  t\right)  }$) in this basis.
\end{remark}

\begin{exercise}
\fbox{3} Let $A\in\mathbb{C}^{n\times n}$ be an $n\times n$-matrix that has
$n$ \textbf{distinct} eigenvalues. Prove that $A\sim C_{p_{A}}$.
\end{exercise}

\begin{exercise}
\fbox{4} Let $\mathbb{F}$ be a field. Let $A\in\mathbb{F}^{n\times n}$ be an
$n\times n$-matrix. Prove that $A\sim C_{p_{A}}$ if and only if there exists a
vector $v\in\mathbb{F}^{n}$ such that
\[
\left(  v,\ Av,\ A^{2}v,\ \ldots,\ A^{n-1}v\right)  =\left(  A^{0}%
v,\ A^{1}v,\ \ldots,\ A^{n-1}v\right)
\]
is a basis of $\mathbb{F}^{n}$.
\end{exercise}

\subsection{The Jordan--Chevalley decomposition}

Recall that:

\begin{itemize}
\item A matrix $A\in\mathbb{C}^{n\times n}$ is said to be
\textbf{diagonalizable} if it is similar to a diagonal matrix.

\item A matrix $A\in\mathbb{C}^{n\times n}$ is said to be \textbf{nilpotent}
if some power of it is the zero matrix (i.e., if $A^{k}=0$ for some
$k\in\mathbb{N}$). Actually (this will be a HW problem), for an $n\times
n$-matrix $A$ to be nilpotent, it is necessary and sufficient that $A^{n}=0$.
\end{itemize}

\begin{theorem}
[Jordan--Chevalley decomposition]Let $A\in\mathbb{C}^{n\times n}$ be an
$n\times n$-matrix.

\textbf{(a)} Then, there exists a unique pair $\left(  D,N\right)  $
consisting of

\begin{itemize}
\item a diagonalizable matrix $D\in\mathbb{C}^{n\times n}$ and

\item a nilpotent matrix $N\in\mathbb{C}^{n\times n}$
\end{itemize}

such that $DN=ND$ and $A=D+N$.

\textbf{(b)} Both $D$ and $N$ in this pair can be written as polynomials in
$A$. In other words, there exist two polynomials $f,g\in\mathbb{C}\left[
t\right]  $ such that $D=f\left(  A\right)  $ and $N=g\left(  A\right)  $.
\end{theorem}

The pair $\left(  D,N\right)  $ in this theorem is known as the
\textbf{Jordan--Chevalley decomposition} (or the \textbf{Dunford
decomposition}) of $A$.

\begin{proof}
[Partial proof.]We will show the following two claims:

\begin{statement}
\textit{Claim 1:} There exists a pair $\left(  D,N\right)  $ as in part
\textbf{(a)} of the theorem.
\end{statement}

\begin{statement}
\textit{Claim 2:} The $D$ and $N$ in this particular pair can be written as
polynomials in $A$.
\end{statement}

To prove both Claims 1 and 2, we can WLOG assume that $A$ is a Jordan matrix.
Indeed, if $A=SJS^{-1}$ for some invertible $S$, and if $\left(  D^{\prime
},N^{\prime}\right)  $ is a Jordan--Chevalley decomposition of $J$, then
$\left(  SD^{\prime}S^{-1},SN^{\prime}S^{-1}\right)  $ is a Jordan--Chevalley
decomposition of $A$. Conjugation of matrices preserves all the properties we
need (such as being a polynomial, commuting, etc.), so we only need to prove
the claims for $J$.

So we WLOG assume that $A$ is a Jordan matrix. Thus,%
\[
A=\left(
\begin{array}
[c]{cccc}%
J_{k_{1}}\left(  \lambda_{1}\right)  &  &  & \\
& J_{k_{2}}\left(  \lambda_{2}\right)  &  & \\
&  & \ddots & \\
&  &  & J_{k_{p}}\left(  \lambda_{p}\right)
\end{array}
\right)
\]
for some $\lambda_{1},\lambda_{2},\ldots,\lambda_{p}$ and some $k_{1}%
,k_{2},\ldots,k_{p}$. Now, we want to decompose this $A$ as $D+N$ with $D$
diagonal and $N$ nilpotent and $DN=ND$. We do this by setting%
\[
D:=\left(
\begin{array}
[c]{cccc}%
\lambda_{1}I_{k_{1}} &  &  & \\
& \lambda_{2}I_{k_{2}} &  & \\
&  & \ddots & \\
&  &  & \lambda_{p}I_{k_{p}}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ N:=\left(
\begin{array}
[c]{cccc}%
J_{k_{1}}\left(  0\right)  &  &  & \\
& J_{k_{2}}\left(  0\right)  &  & \\
&  & \ddots & \\
&  &  & J_{k_{p}}\left(  0\right)
\end{array}
\right)  .
\]
It is easy to check that $A=D+N$ and $DN=ND$ (since block-diagonal matrices
can be multiplied block by block). Clearly, $D$ is diagonalizable (since $D$
is diagonal) and $N$ is nilpotent (since $N$ is strictly upper-triangular).
Thus, Claim 1 is proved.

Now, we need to prove Claim 2 -- i.e., we need to prove that $D$ and $N$ can
be written as polynomials in $A$. Since $A=D+N$, it suffices to show this for
$D$ (since $N=A-D$).

Our above construction of $D$ shows that $D$ is simply $A$ with its
non-diagonal entries removed. Let $\mu_{1},\mu_{2},\ldots,\mu_{m}$ be the
\textbf{distinct} eigenvalues (i.e., diagonal entries) of $A$. For each
$i\in\left[  m\right]  $, let $\ell_{i}$ be the size of the largest Jordan
block of $A$ at eigenvalue $\mu_{i}$. (Thus, the minimal polynomial of $A$ is
$\prod\limits_{i=1}^{m}\left(  t-\mu_{i}\right)  ^{\ell_{i}}$.)

Now, define the polynomial%
\[
f\left(  t\right)  :=\sum_{i=1}^{m}\mu_{i}\prod\limits_{j\neq i}\left(
\dfrac{t-\mu_{j}}{\mu_{i}-\mu_{j}}\right)  ^{\ell_{j}}\in\mathbb{C}\left[
t\right]  ,
\]
where the product sign $\prod\limits_{j\neq i}$ means a product over all
$j\in\left[  m\right]  $ except for $j=i$. We claim that $f\left(  A\right)
=D$. In other words, we claim that applying $f$ to $A$ has the effect of
cleaning out all off-diagonal entries (while the diagonal entries remain as
they are).

To prove this claim, we recall that%
\[
A=\left(
\begin{array}
[c]{cccc}%
J_{k_{1}}\left(  \lambda_{1}\right)  &  &  & \\
& J_{k_{2}}\left(  \lambda_{2}\right)  &  & \\
&  & \ddots & \\
&  &  & J_{k_{p}}\left(  \lambda_{p}\right)
\end{array}
\right)  .
\]
Hence,%
\[
f\left(  A\right)  =\left(
\begin{array}
[c]{cccc}%
f\left(  J_{k_{1}}\left(  \lambda_{1}\right)  \right)  &  &  & \\
& f\left(  J_{k_{2}}\left(  \lambda_{2}\right)  \right)  &  & \\
&  & \ddots & \\
&  &  & f\left(  J_{k_{p}}\left(  \lambda_{p}\right)  \right)
\end{array}
\right)  .
\]
So we need to show that%
\[
f\left(  J_{k_{u}}\left(  \lambda_{u}\right)  \right)  =\lambda_{u}I_{k_{u}%
}\ \ \ \ \ \ \ \ \ \ \text{for each }u\in\left[  p\right]  .
\]
To prove this, we fix a $u\in\left[  p\right]  $, and we set $B:=J_{k_{u}%
}\left(  \lambda_{u}\right)  $. Thus, $B=J_{k}\left(  \mu_{v}\right)  $ for
some $v\in\left[  p\right]  $ and some positive $k\leq\ell_{v}$.

Substituting $B$ for $t$ into%
\[
f\left(  t\right)  :=\sum_{i=1}^{m}\mu_{i}\prod\limits_{j\neq i}\left(
\dfrac{t-\mu_{j}}{\mu_{i}-\mu_{j}}\right)  ^{\ell_{j}},
\]
we obtain%
\[
f\left(  B\right)  =\sum_{i=1}^{m}\mu_{i}\prod\limits_{j\neq i}\left(
\dfrac{B-\mu_{j}I}{\mu_{i}-\mu_{j}}\right)  ^{\ell_{j}}.
\]
We take a closer look at the addends of the sum. For each $i\in\left[
m\right]  $ that is distinct from $v$, the product $\prod\limits_{j\neq
i}\left(  \dfrac{B-\mu_{j}I}{\mu_{i}-\mu_{j}}\right)  ^{\ell_{j}}$ contains a
factor $\left(  \dfrac{B-\mu_{v}I}{\mu_{i}-\mu_{v}}\right)  ^{\ell_{v}}=0$,
because the $k\times k$-matrix $\dfrac{B-\mu_{v}I}{\mu_{i}-\mu_{v}}$ is
strictly upper-triangular and $k\leq\ell_{v}$. So the entire addend $\mu
_{i}\prod\limits_{j\neq i}\left(  \dfrac{B-\mu_{j}I}{\mu_{i}-\mu_{j}}\right)
^{\ell_{j}}$ is $0$ whenever $i$ is distinct from $v$. Thus, all these addends
disappear except for the addend for $i=v$. So the above formula for $f\left(
B\right)  $ simplifies to%
\[
f\left(  B\right)  =\mu_{v}\prod\limits_{j\neq v}\left(  \dfrac{B-\mu_{j}%
I}{\mu_{v}-\mu_{j}}\right)  ^{\ell_{j}}.
\]
This should be $\mu_{v}I$.

I'm just seeing this isn't exactly the case. TODO: Fix this.

[Alternatively, use the Chinese Remainder Theorem for polynomials to find a
polynomial $f$ that satisfies $f\left(  J_{k_{u}}\left(  \lambda_{u}\right)
\right)  =\lambda_{u}I_{k_{u}}$. This polynomial $f$ should satisfy
$f\equiv\left(  t-\lambda_{u}\right)  ^{k_{u}}+\lambda_{u}$ for each $u$.]
\end{proof}

\subsection{The real Jordan canonical form}

Given a matrix $A\in\mathbb{R}^{n\times n}$ with real entries, its Jordan
canonical form doesn't necessarily have real entries. Indeed, the eigenvalues
of $A$ don't have to be real. Sometimes, we want to find a \textquotedblleft
simple\textquotedblright\ form for $A$ that does have real entries. What
follows is a way to tweak the Jordan canonical form to this use case.

We observe the following:

\begin{lemma}
Let $A\in\mathbb{R}^{n\times n}$ and $\lambda\in\mathbb{C}$. Then, the
\textquotedblleft Jordan structure of $A$ at $\lambda$\textquotedblright%
\ (meaning the multiset of the sizes of the Jordan blocks of $A$ at $\lambda$)
equals the Jordan structure of $A$ at $\overline{\lambda}$. In other words,
for each $p>0$, we have%
\begin{align*}
&  \left(  \text{the number of Jordan blocks of }A\text{ at }\lambda\text{
having size }p\right) \\
&  =\left(  \text{the number of Jordan blocks of }A\text{ at }\overline
{\lambda}\text{ having size }p\right)  .
\end{align*}


In other words, Jordan blocks at $\lambda$ and Jordan blocks at $\overline
{\lambda}$ come in pairs of equal sizes.
\end{lemma}

\begin{exercise}
\fbox{2} Prove this lemma.
\end{exercise}

So we can try to combine each Jordan block at $\lambda$ with an equally sized
Jordan block at $\overline{\lambda}$ and hope that something real comes out
somehow, in the same way as $\left(  t-\lambda\right)  \left(  t-\overline
{\lambda}\right)  =t^{2}-2\operatorname*{Re}\lambda+\left\vert \lambda
\right\vert ^{2}\in\mathbb{R}\left[  t\right]  $.

How to do this? For Jordan blocks of size $1$, this is easy:

\begin{lemma}
Let $\lambda\in\mathbb{C}$. Let $L$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
\lambda & \\
& \overline{\lambda}%
\end{array}
\right)  $. Let $a=\operatorname*{Re}\lambda$ and $b=\operatorname*{Im}%
\lambda$ (so that $\lambda=a+bi$). Then,%
\[
\left(
\begin{array}
[c]{cc}%
\lambda & \\
& \overline{\lambda}%
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cc}%
a & b\\
-b & a
\end{array}
\right)  .
\]

\end{lemma}

\begin{exercise}
\fbox{2} Prove this lemma.
\end{exercise}

Now, let us see how to combine a Jordan block at $\lambda$ with an equally
sized Jordan block at $\overline{\lambda}$ when the size is arbitrary. We can
WLOG assume that these two Jordan blocks are adjacent (since we can permute
the Jordan blocks at will). Thus, they look as follows together:%
\begin{align*}
&  \left(
\begin{array}
[c]{cccccccc}%
\lambda & 1 &  &  &  &  &  & \\
& \lambda & \ddots &  &  &  &  & \\
&  & \ddots & 1 &  &  &  & \\
&  &  & \lambda &  &  &  & \\
&  &  &  & \overline{\lambda} & 1 &  & \\
&  &  &  &  & \overline{\lambda} & \ddots & \\
&  &  &  &  &  & \ddots & 1\\
&  &  &  &  &  &  & \overline{\lambda}%
\end{array}
\right) \\
&  \sim\left(
\begin{array}
[c]{cccccccc}%
\lambda &  & 1 &  &  &  &  & \\
& \overline{\lambda} &  & 1 &  &  &  & \\
&  & \lambda &  & 1 &  &  & \\
&  &  & \overline{\lambda} &  & 1 &  & \\
&  &  &  & \ddots &  &  & \\
&  &  &  &  & \ddots &  & \\
&  &  &  &  &  & \lambda & \\
&  &  &  &  &  &  & \overline{\lambda}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cccc}%
L & I_{2} &  & \\
& L & I_{2} & \\
&  & \ddots & \\
&  &  & L
\end{array}
\right)  ,
\end{align*}
where $L$ is the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
\lambda & \\
& \overline{\lambda}%
\end{array}
\right)  $. However, our last lemma yields%
\[
\left(
\begin{array}
[c]{cc}%
\lambda & \\
& \overline{\lambda}%
\end{array}
\right)  \sim\left(
\begin{array}
[c]{cc}%
a & b\\
-b & a
\end{array}
\right)  ,
\]
where $a=\operatorname*{Re}\lambda$ and $b=\operatorname*{Im}\lambda$ (so that
$\lambda=a+bi$). So our matrix is similar to%
\[
\left(
\begin{array}
[c]{cccccccc}%
a & b & 1 &  &  &  &  & \\
-b & a &  & 1 &  &  &  & \\
&  & a & b & 1 &  &  & \\
&  & -b & a &  & 1 &  & \\
&  &  &  & \ddots &  &  & \\
&  &  &  &  & \ddots &  & \\
&  &  &  &  &  & a & b\\
&  &  &  &  &  & -b & a
\end{array}
\right)  .
\]


\subsection{The centralizer of a matrix}

Here is a fairly natural question: Which matrices commute with a given square
matrix $A$ ?

\begin{proposition}
Let $\mathbb{F}$ be a field. Let $A\in\mathbb{F}^{n\times n}$ be an $n\times
n$-matrix. Let $f$ and $g$ be two polynomials in a single variable $t$ over
$\mathbb{F}$. Then, $f\left(  A\right)  $ commutes with $g\left(  A\right)  $.
\end{proposition}

\begin{proof}
Write $f\left(  t\right)  $ as $f\left(  t\right)  =\sum_{i=0}^{n}f_{i}t^{i}$,
and write $g\left(  t\right)  $ as $g\left(  t\right)  =\sum_{j=0}^{m}%
g_{j}t^{j}$. Then,%
\[
f\left(  A\right)  =\sum_{i=0}^{n}f_{i}A^{i}\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ g\left(  A\right)  =\sum_{j=0}^{m}g_{j}A^{j}.
\]
Thus,%
\[
f\left(  A\right)  \cdot g\left(  A\right)  =\left(  \sum_{i=0}^{n}f_{i}%
A^{i}\right)  \cdot\left(  \sum_{j=0}^{m}g_{j}A^{j}\right)  =\sum_{i=0}%
^{n}\ \ \sum_{j=0}^{m}f_{i}g_{j}\underbrace{A^{i}A^{j}}_{=A^{i+j}}=\sum
_{i=0}^{n}\ \ \sum_{j=0}^{m}f_{i}g_{j}A^{i+j}.
\]
A similar computation shows that%
\[
g\left(  A\right)  \cdot f\left(  A\right)  =\sum_{i=0}^{n}\ \ \sum_{j=0}%
^{m}f_{i}g_{j}A^{i+j}.
\]
Comparing these two, we obtain $f\left(  A\right)  \cdot g\left(  A\right)
=g\left(  A\right)  \cdot f\left(  A\right)  $, qed.
\end{proof}

Thus, in particular, $f\left(  A\right)  $ commutes with $A$ for any
polynomial $f$ (because $A=g\left(  A\right)  $ for $g\left(  t\right)  =t$).

But are there other matrices that commute with $A$ ?

There certainly can be. For instance, if $A=\lambda I_{n}$ for some
$\lambda\in\mathbb{F}$, then \textbf{every} $n\times n$-matrix commutes with
$A$ (but very few matrices are of the form $f\left(  A\right)  $ for some
polynomial $f$). This is, in a sense, the \textquotedblleft best case
scentario\textquotedblright. Only for $A=\lambda I_{n}$ is it true that every
$n\times n$-matrix commutes with $A$.

Let us study the general case now.

\begin{definition}
Let $A\in\mathbb{F}^{n\times n}$ be an $n\times n$-matrix. The
\textbf{centralizer} of $A$ is defined to be the set of all $n\times
n$-matrices $B\in\mathbb{F}^{n\times n}$ such that $AB=BA$. We denote this set
by $\operatorname*{Cent}A$.
\end{definition}

We thus want to know what $\operatorname*{Cent}A$ is.

We begin with some general properties:

\begin{proposition}
Let $A\in\mathbb{F}^{n\times n}$ be an $n\times n$-matrix. Then,
$\operatorname*{Cent}A$ is a subset of $\mathbb{F}^{n\times n}$ that is closed
under addition, scaling and multiplication and contains $\lambda I_{n}$ for
all $\lambda\in\mathbb{F}$. In other words:

\textbf{(a)} For any $B,C\in\operatorname*{Cent}A$, we have $B+C\in
\operatorname*{Cent}A$.

\textbf{(b)} For any $B\in\operatorname*{Cent}A$ and $\lambda\in\mathbb{F}$,
we have $\lambda B\in\operatorname*{Cent}A$.

\textbf{(c)} For any $B,C\in\operatorname*{Cent}A$, we have $BC\in
\operatorname*{Cent}A$.

\textbf{(d)} For any $\lambda\in\mathbb{F}$, we have $\lambda I_{n}%
\in\operatorname*{Cent}A$.
\end{proposition}

This implies, in particular, that $\operatorname*{Cent}A$ is a vector subspace
of $\mathbb{F}^{n\times n}$. Furthermore, it shows that $\operatorname*{Cent}%
A$ is an $\mathbb{F}$-subalgebra of $\mathbb{F}^{n\times n}$ (in particular, a
subring of $\mathbb{F}^{n\times n}$).

\begin{proof}
[Proof of the Proposition.]Let me just show part \textbf{(c)}; the other parts
are even easier.

\textbf{(c)} Let $B,C\in\operatorname*{Cent}A$. Thus, $AB=BA$ and $AC=CA$.
Now,%
\[
\underbrace{AB}_{=BA}C=B\underbrace{AC}_{=CA}=BCA.
\]
This shows that $BC\in\operatorname*{Cent}A$. Thus, part \textbf{(c)} is proved.
\end{proof}

Now, as an example, let us compute $\operatorname*{Cent}A$ in the case when
$A$ is a single Jordan cell $J_{n}\left(  0\right)  $. So we fix an $n>0$, and
we set%
\[
A:=J_{n}\left(  0\right)  =\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  .
\]
Let $B\in\mathbb{F}^{n\times n}$ be arbitrary. We want to know when
$B\in\operatorname*{Cent}A$. In other words, we want to know when $AB=BA$.

We have
\begin{align*}
AB  &  =\left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  \left(
\begin{array}
[c]{ccccc}%
B_{1,1} & B_{1,2} & B_{1,3} & \cdots & B_{1,n}\\
B_{2,1} & B_{2,2} & B_{2,3} & \cdots & B_{2,n}\\
B_{3,1} & B_{3,2} & B_{3,3} & \cdots & B_{3,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
B_{n,1} & B_{n,2} & B_{n,3} & \cdots & B_{n,n}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{ccccc}%
B_{2,1} & B_{2,2} & B_{2,3} & \cdots & B_{2,n}\\
B_{3,1} & B_{3,2} & B_{3,3} & \cdots & B_{3,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
B_{n,1} & B_{n,2} & B_{n,3} & \cdots & B_{n,n}\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)
\end{align*}
and%
\begin{align*}
BA  &  =\left(
\begin{array}
[c]{ccccc}%
B_{1,1} & B_{1,2} & B_{1,3} & \cdots & B_{1,n}\\
B_{2,1} & B_{2,2} & B_{2,3} & \cdots & B_{2,n}\\
B_{3,1} & B_{3,2} & B_{3,3} & \cdots & B_{3,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
B_{n,1} & B_{n,2} & B_{n,3} & \cdots & B_{n,n}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccccc}%
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{ccccc}%
0 & B_{1,1} & B_{1,2} & \cdots & B_{1,n-1}\\
0 & B_{2,1} & B_{2,2} & \cdots & B_{2,n-1}\\
0 & B_{3,1} & B_{3,2} & \cdots & B_{3,n-1}\\
\vdots & \vdots & \vdots & \ddots & \ddots\\
0 & B_{n,1} & B_{n,2} & \cdots & B_{n,n-1}%
\end{array}
\right)  .
\end{align*}
Thus, $AB=BA$ holds if and only if
\[
\left(
\begin{array}
[c]{ccccc}%
B_{2,1} & B_{2,2} & B_{2,3} & \cdots & B_{2,n}\\
B_{3,1} & B_{3,2} & B_{3,3} & \cdots & B_{3,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
B_{n,1} & B_{n,2} & B_{n,3} & \cdots & B_{n,n}\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  =\left(
\begin{array}
[c]{ccccc}%
0 & B_{1,1} & B_{1,2} & \cdots & B_{1,n-1}\\
0 & B_{2,1} & B_{2,2} & \cdots & B_{2,n-1}\\
0 & B_{3,1} & B_{3,2} & \cdots & B_{3,n-1}\\
\vdots & \vdots & \vdots & \ddots & \ddots\\
0 & B_{n,1} & B_{n,2} & \cdots & B_{n,n-1}%
\end{array}
\right)  ,
\]
i.e., if%
\begin{align*}
B_{2,j}  &  =B_{1,j-1}\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  n\right]
\text{ (where }B_{1,0}:=0\text{);}\\
B_{3,j}  &  =B_{2,j-1}\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  n\right]
\text{ (where }B_{2,0}:=0\text{);}\\
B_{4,j}  &  =B_{3,j-1}\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  n\right]
\text{ (where }B_{3,0}:=0\text{);}\\
&  \ldots;\\
B_{n,j}  &  =B_{n-1,j-1}\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[
n\right]  \text{ (where }B_{n-1,0}:=0\text{);}\\
0  &  =B_{n,j}\ \ \ \ \ \ \ \ \ \ \text{for all }j\in\left[  n-1\right]  .
\end{align*}
The latter system of equations can be restated as follows:%
\begin{align*}
&  \ldots;\\
B_{n,n-2}  &  =B_{n-1,n-3}=B_{n-2,n-4}=\cdots=B_{3,1}=0;\\
B_{n,n-1}  &  =B_{n-1,n-2}=B_{n-2,n-3}=\cdots=B_{2,1}=0;\\
B_{n,n}  &  =B_{n-1,n-1}=B_{n-2,n-2}=\cdots=B_{1,1};\\
B_{n-1,n}  &  =B_{n-2,n-1}=B_{n-3,n-2}=\cdots=B_{1,2};\\
B_{n-2,n}  &  =B_{n-3,n-1}=B_{n-4,n-2}=\cdots=B_{1,3};\\
&  \ldots.
\end{align*}
In other words, it means that the matrix $B$ looks as follows:%
\[
B=\left(
\begin{array}
[c]{ccccc}%
b_{0} & b_{1} & b_{2} & \cdots & b_{n-1}\\
& b_{0} & b_{1} & \cdots & b_{n-2}\\
&  & b_{0} & \cdots & b_{n-3}\\
&  &  & \ddots & \vdots\\
&  &  &  & b_{0}%
\end{array}
\right)
\]
(where the empty cells have entries equal to $0$). This is called an
\textbf{upper-triangular Toeplitz matrix}. We can also rewrite it as%
\[
B=b_{0}I_{n}+b_{1}A+b_{2}A^{2}+\cdots+b_{n-1}A^{n-1}.
\]


So we have proved the following:

\begin{theorem}
Let $n>0$. Let $A=J_{n}\left(  0\right)  $. Then,
\begin{align*}
\operatorname*{Cent}A  &  =\left\{  \left(
\begin{array}
[c]{ccccc}%
b_{0} & b_{1} & b_{2} & \cdots & b_{n-1}\\
& b_{0} & b_{1} & \cdots & b_{n-2}\\
&  & b_{0} & \cdots & b_{n-3}\\
&  &  & \ddots & \vdots\\
&  &  &  & b_{0}%
\end{array}
\right)  \ \mid\ b_{0},b_{1},\ldots,b_{n-1}\in\mathbb{F}\right\} \\
&  =\left\{  b_{0}I_{n}+b_{1}A+b_{2}A^{2}+\cdots+b_{n-1}A^{n-1}\ \mid
\ b_{0},b_{1},\ldots,b_{n-1}\in\mathbb{F}\right\} \\
&  =\left\{  f\left(  A\right)  \ \mid\ f\in\mathbb{F}\left[  t\right]  \text{
is a polynomial of degree }\leq n-1\right\}  .
\end{align*}

\end{theorem}

So this is the worst-case scenario: The only matrices commuting with $A$ are
the matrices of the form $f\left(  A\right)  $ (which, as we recall, must
always commute with $A$).

What happens for an arbitrary $A$ ? Is the answer closer to the best-case
scenario or to the worst-case scenario? The answer is that the worst-case
scenario holds for a randomly chosen matrix, but we can actually answer the
question \textquotedblleft what is $\operatorname*{Cent}A$
exactly\textquotedblright\ if we know the Jordan canonical form of $A$.

We start with simple propositions:

\begin{proposition}
Let $A\in\mathbb{F}^{n\times n}$ and $\lambda\in\mathbb{F}$. Then,
$\operatorname*{Cent}\left(  A-\lambda I_{n}\right)  =\operatorname*{Cent}A$.
\end{proposition}

\begin{exercise}
\fbox{1} Prove this.
\end{exercise}

\begin{proposition}
Let $A$, $B$ and $S$ be three $n\times n$-matrices such that $S$ is
invertible. Then,
\[
\left(  B\in\operatorname*{Cent}A\right)  \ \Longleftrightarrow\ \left(
SBS^{-1}\in\operatorname*{Cent}\left(  SAS^{-1}\right)  \right)  .
\]

\end{proposition}

\begin{exercise}
\fbox{1} Prove this.
\end{exercise}

Thus, if $A$ is a matrix with complex entries, and if we want to compute
$\operatorname*{Cent}A$, it suffices to compute $\operatorname*{Cent}J$, where
$J$ is the JCF of $A$.

Therefore, we now focus on centralizers of Jordan matrices.

\begin{proposition}
Let $A_{1},A_{2},\ldots,A_{k}$ be square matrices with complex entries. Assume
that the spectra of these matrices are disjoint -- i.e., if $i\neq j$, then
$\sigma\left(  A_{i}\right)  \cap\sigma\left(  A_{j}\right)  =\varnothing$.

Then,%
\begin{align*}
&  \operatorname*{Cent}\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right) \\
&  =\left\{  \left(
\begin{array}
[c]{cccc}%
B_{1} &  &  & \\
& B_{2} &  & \\
&  & \ddots & \\
&  &  & B_{k}%
\end{array}
\right)  \ \mid\ B_{i}\in\operatorname*{Cent}\left(  A_{i}\right)  \text{ for
each }i\in\left[  k\right]  \right\}  .
\end{align*}

\end{proposition}

\begin{proof}
The $\supseteq$ inclusion is obvious. We thus need to prove the $\subseteq$
inclusion only.

Let $A_{i}$ be an $n_{i}\times n_{i}$-matrix for each $i\in\left[  k\right]  $.

Let $B\in\operatorname*{Cent}\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right)  $. We want to show that $B$ has the form $\left(
\begin{array}
[c]{cccc}%
B_{1} &  &  & \\
& B_{2} &  & \\
&  & \ddots & \\
&  &  & B_{k}%
\end{array}
\right)  $ where $B_{i}\in\operatorname*{Cent}\left(  A_{i}\right)  $ for each
$i\in\left[  k\right]  $.

Write $B$ as a block matrix%
\[
B=\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,k\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  & B\left(  k,2\right)  & \cdots & B\left(  k,k\right)
\end{array}
\right)  ,
\]
where each $B\left(  i,j\right)  $ is an $n_{i}\times n_{j}$-matrix. Then, by
the rule for multiplying block matrices, we have%
\begin{align*}
&  \left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,k\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  & B\left(  k,2\right)  & \cdots & B\left(  k,k\right)
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
A_{1}B\left(  1,1\right)  & A_{1}B\left(  1,2\right)  & \cdots & A_{1}B\left(
1,k\right) \\
A_{2}B\left(  2,1\right)  & A_{2}B\left(  2,2\right)  & \cdots & A_{2}B\left(
2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
A_{k}B\left(  k,1\right)  & A_{k}B\left(  k,2\right)  & \cdots & A_{k}B\left(
k,k\right)
\end{array}
\right)
\end{align*}
and%
\begin{align*}
&  \left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,k\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  & B\left(  k,2\right)  & \cdots & B\left(  k,k\right)
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  A_{1} & B\left(  1,2\right)  A_{2} & \cdots & B\left(
1,k\right)  A_{k}\\
B\left(  2,1\right)  A_{1} & B\left(  2,2\right)  A_{2} & \cdots & B\left(
2,k\right)  A_{k}\\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  A_{1} & B\left(  k,2\right)  A_{2} & \cdots & B\left(
k,k\right)  A_{k}%
\end{array}
\right)  .
\end{align*}
However, these two matrices must be equal, since $\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,k\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  & B\left(  k,2\right)  & \cdots & B\left(  k,k\right)
\end{array}
\right)  \in\operatorname*{Cent}\left(
\begin{array}
[c]{cccc}%
A_{1} &  &  & \\
& A_{2} &  & \\
&  & \ddots & \\
&  &  & A_{k}%
\end{array}
\right)  $. Thus, we have%
\[
\left(
\begin{array}
[c]{cccc}%
A_{1}B\left(  1,1\right)  & A_{1}B\left(  1,2\right)  & \cdots & A_{1}B\left(
1,k\right) \\
A_{2}B\left(  2,1\right)  & A_{2}B\left(  2,2\right)  & \cdots & A_{2}B\left(
2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
A_{k}B\left(  k,1\right)  & A_{k}B\left(  k,2\right)  & \cdots & A_{k}B\left(
k,k\right)
\end{array}
\right)  =\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  A_{1} & B\left(  1,2\right)  A_{2} & \cdots & B\left(
1,k\right)  A_{k}\\
B\left(  2,1\right)  A_{1} & B\left(  2,2\right)  A_{2} & \cdots & B\left(
2,k\right)  A_{k}\\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  A_{1} & B\left(  k,2\right)  A_{2} & \cdots & B\left(
k,k\right)  A_{k}%
\end{array}
\right)  .
\]
Comparing blocks, we can rewrite this as%
\[
A_{i}B\left(  i,j\right)  =B\left(  i,j\right)  A_{j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left[  k\right]  .
\]


Now, let $i,j\in\left[  k\right]  $ be distinct. Consider this equality
$A_{i}B\left(  i,j\right)  =B\left(  i,j\right)  A_{j}$. We can rewrite it as
$A_{i}B\left(  i,j\right)  -B\left(  i,j\right)  A_{j}=0$. Thus, $B\left(
i,j\right)  $ is an $n_{i}\times n_{j}$-matrix $X$ satisfying $A_{i}%
X-XA_{j}=0$. However, because $\sigma\left(  A_{i}\right)  \cap\sigma\left(
A_{j}\right)  =\varnothing$, a theorem we proved before (the Sylvester matrix
equation) tells us that there is a \textbf{unique} $n_{i}\times n_{j}$-matrix
$X$ satisfying $A_{i}X-XA_{j}=0$. Clearly, this unique matrix $X$ must be the
$0$ matrix (since the $0$ matrix satisfies $A_{i}0-0A_{j}=0$). So we conclude
that $B\left(  i,j\right)  $ is the $0$ matrix. In other words, $B\left(
i,j\right)  =0$.

So we have shown that $B\left(  i,j\right)  =0$ whenever $i$ and $j$ are
distinct. Thus,%
\[
B=\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,k\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  & B\left(  k,2\right)  & \cdots & B\left(  k,k\right)
\end{array}
\right)  =\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  &  &  & \\
& B\left(  2,2\right)  &  & \\
&  & \ddots & \\
&  &  & B\left(  k,k\right)
\end{array}
\right)  .
\]
This shows that $B$ is block-diagonal. Now, applying the equation%
\[
A_{i}B\left(  i,j\right)  =B\left(  i,j\right)  A_{j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\left[  k\right]
\]
to $j=i$, we obtain $A_{i}B\left(  i,i\right)  =B\left(  i,i\right)  A_{i}$,
which of course means that $B\left(  i,i\right)  \in\operatorname*{Cent}%
\left(  A_{i}\right)  $. Thus, $B$ has the form $\left(
\begin{array}
[c]{cccc}%
B_{1} &  &  & \\
& B_{2} &  & \\
&  & \ddots & \\
&  &  & B_{k}%
\end{array}
\right)  $ where $B_{i}\in\operatorname*{Cent}\left(  A_{i}\right)  $ for each
$i\in\left[  k\right]  $. Proof complete.
\end{proof}

So we only need to compute $\operatorname*{Cent}J$ when $J$ is a Jordan matrix
with only one eigenvalue.

We can WLOG assume that this eigenvalue is $0$, since we know that
$\operatorname*{Cent}\left(  A-\lambda I_{n}\right)  =\operatorname*{Cent}A$.

So we only need to compute $\operatorname*{Cent}J$ when $J$ is a Jordan matrix
with zeroes on its diagonal.

If $J$ is just a single Jordan cell, we already know the result (by the above
theorem which describes $\operatorname*{Cent}A$ for $A=J_{n}\left(  0\right)
$). In the general case, we have the following:

\begin{proposition}
Let $J$ be a Jordan matrix whose Jordan blocks are%
\[
J_{n_{1}}\left(  0\right)  ,\ \ J_{n_{2}}\left(  0\right)  ,\ \ \ldots
,\ \ J_{n_{k}}\left(  0\right)  .
\]
Let $B$ be an $n\times n$-matrix, written as a block matrix%
\[
B=\left(
\begin{array}
[c]{cccc}%
B\left(  1,1\right)  & B\left(  1,2\right)  & \cdots & B\left(  1,k\right) \\
B\left(  2,1\right)  & B\left(  2,2\right)  & \cdots & B\left(  2,k\right) \\
\vdots & \vdots & \ddots & \vdots\\
B\left(  k,1\right)  & B\left(  k,2\right)  & \cdots & B\left(  k,k\right)
\end{array}
\right)  ,
\]
where each $B\left(  i,j\right)  $ is an $n_{i}\times n_{j}$-matrix. Then,
$B\in\operatorname*{Cent}J$ if and only if each of the $k^{2}$ blocks
$B\left(  i,j\right)  $ is an \textbf{upper-triangular Toeplitz matrix in the
wide sense}.

Here, we say that a matrix is an \textbf{upper-triangular Toeplitz matrix in
the wide sense} if it

\begin{itemize}
\item has the form $\left(
\begin{array}
[c]{cc}%
0 & U
\end{array}
\right)  $, where $U$ is an upper-triangular Toeplitz (square) matrix and $0$
is a zero matrix, or

\item has the form $\left(
\begin{array}
[c]{c}%
U\\
0
\end{array}
\right)  $, where $U$ is an upper-triangular Toeplitz (square) matrix and $0$
is a zero matrix.
\end{itemize}

(The zero matrices are allowed to be empty.)
\end{proposition}

\begin{proof}
Essentially the same argument that we used to prove the theorem about
$J_{n}\left(  0\right)  $, just with a lot more bookkeeping involved.
\end{proof}

We can summarize our results into a single theorem:

\begin{theorem}
Let $A\in\mathbb{C}^{n\times n}$ be an $n\times n$-matrix with Jordan
canonical form $J$. Then, $\operatorname*{Cent}A$ is a vector subspace of
$\mathbb{C}^{n\times n}$ with dimension%
\[
\sum_{\lambda\in\sigma\left(  A\right)  }g_{\lambda}\left(  A\right)  .
\]
Here, for each eigenvalue $\lambda$ of $A$, the number $g_{\lambda}\left(
A\right)  $ is a nonnegative integer defined as follows: Let $n_{1}%
,n_{2},\ldots,n_{k}$ be the sizes of the Jordan blocks at eigenvalue $\lambda$
that appear in $J$; then, we set%
\[
g_{\lambda}\left(  A\right)  :=\sum_{i=1}^{k}\ \ \sum_{j=1}^{k}\min\left\{
n_{i},n_{j}\right\}  .
\]

\end{theorem}

\begin{proof}
Combine our above results and count the degrees of freedom.
\end{proof}

Now, let us return to the worst-case scenario: When is $\operatorname*{Cent}%
A=\left\{  f\left(  A\right)  \ \mid\ f\in\mathbb{C}\left[  t\right]
\right\}  $ ? We can answer this, too, although the proof takes longer.

\begin{definition}
An $n\times n$-matrix $A\in\mathbb{F}^{n\times n}$ is said to be
\textbf{nonderogatory} if $q_{A}=p_{A}$ (that is, the minimal polynomial of
$A$ equals the characteristic polynomial of $A$).
\end{definition}

\textquotedblleft Most\textquotedblright\ matrices are nonderogatory (in the
sense that a \textquotedblleft randomly chosen\textquotedblright\ matrix with
complex entries will be nonderogatory with probability $1$); but there are
exceptions. It is easy to see that if a matrix $A$ has $n$ distinct
eigenvalues, then $A$ is nonderogatory, but this is not an \textquotedblleft
if and only if\textquotedblright; a single Jordan cell is also nonderogatory.
Here is a necessary and sufficient criterion:

\begin{proposition}
An $n\times n$-matrix $A\in\mathbb{C}^{n\times n}$ is nonderogatory if and
only if its Jordan canonical form has exactly one Jordan block for each eigenvalue.
\end{proposition}

\begin{exercise}
\fbox{2} Prove this.
\end{exercise}

\begin{theorem}
Let $A\in\mathbb{C}^{n\times n}$ be an $n\times n$-matrix. Then,
\[
\operatorname*{Cent}A=\left\{  f\left(  A\right)  \ \mid\ f\in\mathbb{C}%
\left[  t\right]  \right\}
\]
if and only if $f$ is nonderogatory. Moreover, in this case,%
\[
\operatorname*{Cent}A=\left\{  f\left(  A\right)  \ \mid\ f\in\mathbb{C}%
\left[  t\right]  \text{ is a polynomial of degree }\leq n-1\right\}  .
\]

\end{theorem}

\begin{proof}
Later or exercises?
\end{proof}

[...]

\newpage

\begin{thebibliography}{99999999}                                                                                         %


\bibitem[AigZie14]{AigZie}%
\href{https://doi.org/10.1007/978-3-662-57265-8}{Martin Aigner, G\"{u}nter M.
Ziegler, \textit{Proofs from the Book}, 6th edition, Springer 2018.}

\bibitem[AndDos10]{AndDos}\href{https://bookstore.ams.org/xyz-13}{Titu
Andreescu, Gabriel Dospinescu, \textit{Problems from the Book}, 2nd edition,
XYZ Press 2010}.

\bibitem[AndDos12]{AndDosS}\href{https://bookstore.ams.org/xyz-6}{Titu
Andreescu, Gabriel Dospinescu, \textit{Straight from the Book}, XYZ Press
2012}.

\bibitem[Bartle14]{Bartle14}Padraic Bartlett, \textit{Math 108b: Advanced
Linear Algebra, Winter 2014}, 2014.\newline\url{http://web.math.ucsb.edu/~padraic/ucsb_2013_14/math108b_w2014/math108b_w2014.html}

\bibitem[Bourba74]{Bourba74}%
\href{http://libgen.rs/book/index.php?md5=3270565F6D0052635A1550883588204C}{Nicolas
Bourbaki, \textit{Algebra I: Chapters 1--3}, Addison-Wesley 1974}.

\bibitem[BoyDip12]{BoyDip12}William E. Boyce, Richard C. DiPrima,
\textit{Elementary Differential Equations}, 10th edition, Wiley 2012.

\bibitem[ChaSed97]{ChaSed97}%
\href{https://www.jstor.org/stable/10.4169/j.ctt19b9mbq}{Gengzhe Chang, Thomas
W. Sederberg, \textit{Over and Over Again}, Anneli Lax New Mathematical
Library \textbf{39}, The Mathematical Association of America 1997}.

\bibitem[Conrad]{Conrad}Keith Conrad, \textit{Expository notes
(\textquotedblleft blurbs\textquotedblright)}.\newline\url{https://kconrad.math.uconn.edu/blurbs/}

\bibitem[Edward05]{Edwards-Essays}%
\href{https://doi.org/10.1007/b138656}{Harold M. Edwards, \textit{Essays in
Constructive Mathematics}, Springer 2005}.\newline See
\url{https://www.math.nyu.edu/faculty/edwardsh/eserrata.pdf} for errata.

\bibitem[Edward95]{Edward95}%
\href{https://doi.org/10.1007/978-0-8176-4446-8}{Harold M. Edwards,
\textit{Linear Algebra}, Springer 1995}.

\bibitem[Elman20]{Elman20}Richard Elman, \textit{Lectures on Abstract
Algebra}, 28 September 2020.\newline\url{https://www.math.ucla.edu/~rse/algebra_book.pdf}

\bibitem[GalQua20]{GalQua20}Jean Gallier and Jocelyn Quaintance,
\textit{Algebra, Topology, Differential Calculus, and Optimization Theory For
Computer Science and Engineering}, 11 November 2020.\newline\url{https://www.cis.upenn.edu/~jean/gbooks/geomath.html}

\bibitem[Geck20]{Geck20}\href{https://doi.org/10.13001/ela.2020.5055}{Meinolf
Geck, \textit{On Jacob's construction of the rational canonical form of a
matrix}, Electronic Journal of Linear Algebra \textbf{36} (2020), pp.
177--182}.

\bibitem[GelAnd17]{GelAnd}%
\href{https://doi.org/10.1007/978-3-319-58988-6}{R\u{a}zvan Gelca, Titu
Andreescu, \textit{Putnam and Beyond}, 2nd edition, Springer 2017}.

\bibitem[Goodma15]{Goodman}Frederick M. Goodman, \textit{Algebra: Abstract and
Concrete}, edition 2.6, 1 May 2015.\newline%
\url{http://homepage.math.uiowa.edu/~goodman/algebrabook.dir/book.2.6.pdf} .

\bibitem[Grinbe15]{detnotes}Darij Grinberg, \textit{Notes on the combinatorial
fundamentals of algebra}, 10 January 2019.\newline%
\url{http://www.cip.ifi.lmu.de/~grinberg/primes2015/sols.pdf} \newline The
numbering of theorems and formulas in this link might shift when the project
gets updated; for a \textquotedblleft frozen\textquotedblright\ version whose
numbering is guaranteed to match that in the citations above, see
\url{https://github.com/darijgr/detnotes/releases/tag/2019-01-10} .

\bibitem[Grinbe19]{trach}Darij Grinberg, \textit{The trace Cayley-Hamilton
theorem}, 14 July 2019.\newline\url{https://www.cip.ifi.lmu.de/~grinberg/algebra/trach.pdf}

\bibitem[Grinbe21]{21s}Darij Grinberg, \textit{An Introduction to Algebraic
Combinatorics [Math 701, Spring 2021 lecture notes]}, 10 September
2021.\newline\url{https://www.cip.ifi.lmu.de/~grinberg/t/21s/lecs.pdf}

\bibitem[Heffer20]{Heffer20}Jim Hefferon, \textit{Linear Algebra}, 4th edition
2020.\newline\url{http://joshua.smcvt.edu/linearalgebra}

\bibitem[Ho14]{Ho-rear2}%
\href{https://www.math.hkust.edu.hk/excalibur/v19_n3.pdf}{Law Ka Ho,
\textit{Variations and Generalisations to the Rearrangement Inequality},
Mathematical Excalibur \textbf{19}, Number 3, pp. 1--2, 4}.

\bibitem[HorJoh13]{HorJoh13}%
\href{http://www.cse.zju.edu.cn/eclass/attachments/2015-10/01-1446086008-145421.pdf}{Roger
A. Horn, Charles R. Johnson, \textit{Matrix analysis}, Cambridge University
Press, 2nd edition 2013}.

\bibitem[Hung07]{Hung07}%
\href{http://refkol.ro/matek/mathbooks/!Books!/Secrets in Inequalities (volume 1) Pham Kim Hung.pdf}{Pham
Kim Hung, \textit{Secrets in Inequalities, volume 1}, GIL 2007}.

\bibitem[Ivanov08]{Ivanov08}Nikolai V. Ivanov, \textit{Linear Recurrences}, 17
January 2008.\newline\url{https://nikolaivivanov.files.wordpress.com/2014/02/ivanov2008arecurrence.pdf}

\bibitem[Knapp16]{Knapp1}Anthony W. Knapp, \textit{Basic Algebra}, digital
second edition 2016.\newline\url{http://www.math.stonybrook.edu/~aknapp/download.html}

\bibitem[Korner20]{Korner20}T. W. K\"{o}rner, \textit{Where Do Numbers Come
From?}, Cambridge University Press 2020.\newline See
\url{https://web.archive.org/web/20190813160507/https://www.dpmms.cam.ac.uk/~twk/Number.pdf}
for a preprint.\newline See \url{https://www.dpmms.cam.ac.uk/~twk/} for errata
and solutions.

\bibitem[LaNaSc16]{LaNaSc16}Isaiah Lankham, Bruno Nachtergaele, Anne
Schilling, \textit{Linear Algebra As an Introduction to Abstract Mathematics},
2016.\newline\url{https://www.math.ucdavis.edu/~anne/linear_algebra/mat67_course_notes.pdf}

\bibitem[Li99]{Li-rear1}%
\href{https://www.math.hkust.edu.hk/excalibur/v4_n3.pdf}{Kin-Yin Li,
\textit{Rearrangement Inequality}, Mathematical Excalibur \textbf{4}, Number
3, pp. 1--2, 4}.

\bibitem[Loehr14]{Loehr14}%
\href{https://elblogdecontar.files.wordpress.com/2017/01/ebookdaraz-advanced-linear-algebra.pdf}{Nicholas
Loehr, \textit{Advanced Linear Algebra}, CRC Press 2014}.

\bibitem[Markus83]{Markus83}%
\href{https://archive.org/details/recursion-sequences}{Aleksei Ivanovich
Markushevich, \textit{Recursion sequences}, Mir Publishers, Moscow, 2nd
printing 1983}.

\bibitem[Mate16]{Mate16}Attila M\'{a}t\'{e}, \textit{The Cayley-Hamilton
Theorem}, version 28 March 2016.\newline\url{http://www.sci.brooklyn.cuny.edu/~mate/misc/cayley_hamilton.pdf}

\bibitem[Melian01]{Melian01}Mar\'{\i}a Victoria Meli\'{a}n, \textit{Linear
recurrence relations with constant coefficients}, 9 April 2001.\newline\url{http://matematicas.uam.es/~mavi.melian/CURSO_15_16/web_Discreta/recurrence.pdf}

\bibitem[Nathan21]{Nathan21}\href{https://arxiv.org/abs/2109.01746v1}{Melvyn
B. Nathanson, \textit{The Muirhead-Rado inequality, 1 Vector majorization and
the permutohedron}, arXiv:2109.01746v1}.

\bibitem[PolSze78]{PolSze78}%
\href{https://doi.org/10.1007/978-3-642-61983-0}{George P\'{o}lya, Gabor
Szeg\H{o}, \textit{Problems and Theorems in Analysis I}, Springer 1978
(reprinted 1998)}.

\bibitem[Prasol94]{Prasolov}Viktor V. Prasolov,
\textit{\href{http://www2.math.su.se/~mleites/books/prasolov-1994-problems.pdf}{\textit{Problems
and Theorems in Linear Algebra}}}, Translations of Mathematical Monographs,
vol. \#134, AMS 1994.

\bibitem[Shurma15]{Shurma15}Jerry Shurman, \textit{The Cayley-Hamilton theorem
via multilinear algebra}, \url{http://people.reed.edu/~jerry/332/28ch.pdf} .
Part of the collection \textit{Course Materials for Mathematics 332: Algebra},
available at \url{http://people.reed.edu/~jerry/332/mat.html}

\bibitem[Silves00]{Silvest}%
\href{https://web.archive.org/web/20140505161153/http://www.mth.kcl.ac.uk/~jrs/gazette/blocks.pdf}{John
R. Silvester, \textit{Determinants of Block Matrices}, The Mathematical
Gazette, Vol. 84, No. 501 (Nov., 2000), pp. 460--467.}

\bibitem[Steele04]{Steele04}%
\href{http://www.ma.huji.ac.il/~ehudf/courses/Ineq09/The Cauchy-Schwarz Master Class .pdf}{J.
Michael Steele, \textit{The Cauchy--Schwarz Master Class: An Introduction to
the Art of Mathematical Inequalities}, Cambridge University Press
2004}.\newline See
\url{http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/CSMC_Errata.pdf}
for errata.

\bibitem[Steinb06]{Steinb06}Mark Steinberger, \textit{Algebra}, 31 August
2006.\newline\url{https://web.archive.org/web/20180821125315/https://www.albany.edu/~mark/algebra.pdf}

\bibitem[Straub83]{Straub83}Howard Straubing, \textit{A combinatorial proof of
the Cayley-Hamilton theorem}, Discrete Mathematics, Volume 43, Issues 2--3,
1983, pp. 273--279.\newline\url{https://doi.org/10.1016/0012-365X(83)90164-4}

\bibitem[Strick20]{Strick20}Neil Strickland, \textit{Linear mathematics for
applications}, 11 February 2020.\newline\url{https://neilstrickland.github.io/linear_maths/notes/linear_maths.pdf}

\bibitem[Swanso20]{Swanso20}Irene Swanson, \textit{Introduction with Analysis
with Complex Numbers}, 2020.\newline\url{https://web.archive.org/web/20201012174324/https://people.reed.edu/~iswanson/analysisconstructR.pdf}

\bibitem[Tao07]{Tao07}Terence Tao, \textit{The Jordan normal form and the
Euclidean algorithm}, 12 October 2007.\newline\url{https://terrytao.wordpress.com/2007/10/12/the-jordan-normal-form-and-the-euclidean-algorithm/}

\bibitem[Taylor20]{Taylor20}%
\href{https://bookstore.ams.org/amstext-45/}{Michael Taylor, \textit{Linear
Algebra}, AMS 2020}.\newline See
\url{https://mtaylor.web.unc.edu/wp-content/uploads/sites/16915/2018/04/linalg.pdf}
for a preprint.

\bibitem[Treil15]{Treil15}Sergei Treil, \textit{Linear Algebra Done Wrong},
2017.\newline\url{https://www.math.brown.edu/~treil/papers/LADW/LADW.html}

\bibitem[Walker87]{Walker87}%
\href{https://web.archive.org/web/20170809055317/https://www.math.nmsu.edu/~elbert/AbsAlgeb.pdf}{Elbert
A. Walker, \textit{Introduction to Abstract Algebra}, Random House/Birkhauser,
New York, 1987.}

\bibitem[Woerde16]{Woerde16}Hugo J. Woerdeman, \textit{Advanced Linear
Algebra}, CRC Press 2016.

\bibitem[Zeilbe85]{Zeilbe}%
\href{http://www.math.rutgers.edu/~zeilberg/mamarimY/DM85.pdf}{Doron
Zeilberger, \textit{A combinatorial approach to matrix algebra}, Discrete
Mathematics 56 (1985), pp. 61--72.}

\bibitem[Zill17]{Zill17}Dennis G. Zill, \textit{A First Course in Differential
Equations with Modeling Applications}, Cengage 2017.
\end{thebibliography}


\end{document}